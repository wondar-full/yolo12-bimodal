

## ğŸš€ ä¿®æ­£åçš„YOLOv12 + Geometry-Enhanced RGB-Dæ”¹è¿›æ–¹æ¡ˆ

åŸºäºæ‚¨çš„åé¦ˆå’ŒDFormerv2çš„å¯å‘ï¼Œæˆ‘é‡æ–°è®¾è®¡äº†æ–¹æ¡ˆï¼š

### **æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆ3ä¸ªä¸»è¦è´¡çŒ®ï¼‰ï¼š**

1. **Geometry-Guided Feature Enhancement (GGFE)**
   
   - å€Ÿé‰´DFormerv2çš„å‡ ä½•å…ˆéªŒï¼ˆæ— éœ€Depthç¼–ç å™¨ï¼‰
   - ç›´æ¥ä»å•é€šé“æ·±åº¦å›¾è®¡ç®—æ³•å‘é‡å’Œè¾¹ç¼˜
   - é€šè¿‡å‡ ä½•æ³¨æ„åŠ›å¢å¼ºRGBç‰¹å¾
2. **Scale-Aware Depth Fusion (SADF)**
   
   - å‚è€ƒRGBT-Tinyçš„SAFitï¼ˆå°ºåº¦æ„ŸçŸ¥èåˆï¼‰
   - é’ˆå¯¹å°ç›®æ ‡è®¾è®¡ä¸åŒå°ºåº¦çš„æ·±åº¦èåˆç­–ç•¥
   - åŠ¨æ€æ·±åº¦è´¨é‡è¯„ä¼°
3. **Small Object Loss Reweighting (SOLR)**
   
   - å‚è€ƒSOOD (ICCV 2023)çš„æ€æƒ³
   - å¯¹å°ç›®æ ‡ï¼ˆ<32Ã—32åƒç´ ï¼‰å¢åŠ æŸå¤±æƒé‡
   - Focal Loss + IoU-awareåˆ†æ”¯

### **æŠ€æœ¯è·¯çº¿å›¾ï¼š**

```
è¾“å…¥: RGB [B,3,H,W] + Depth [B,1,H,W]
           |                    |
           |                    â†“
           |            ã€å‡ ä½•å…ˆéªŒè®¡ç®—ã€‘(æ— ç¥ç»ç½‘ç»œ)
           |            - æ³•å‘é‡: [B,3,H,W]
           |            - æ·±åº¦è¾¹ç¼˜: [B,1,H,W]
           |                    â†“
           |            ã€æ·±åº¦è´¨é‡è¯„ä¼°ã€‘
           |            depth_conf: [B,1,H,W]
           |                    |
           â†“                    â†“
    ã€YOLOv12 Backboneã€‘â† ã€GGFEæ¨¡å—ã€‘
      (åªæœ‰RGBç¼–ç å™¨)        â†‘
           |          (å‡ ä½•æ³¨æ„åŠ›å¢å¼º)
           â†“
    ã€Neck: C3k2 + SPPFã€‘
           â†“
    ã€SADF: å°ºåº¦æ„ŸçŸ¥æ·±åº¦èåˆã€‘
           â†“
    ã€Detection Head + SOLR Lossã€‘
           â†“
    è¾“å‡º: Bounding Boxes
```

### **ä¸åŸå§‹YOLOv12çš„å¯¹æ¯”ï¼š**

| æ¨¡å— | åŸå§‹YOLOv12 | æ”¹è¿›åï¼ˆGeometry-Enhancedï¼‰ |
|------|------------|---------------------------|
| **è¾“å…¥** | RGB (3é€šé“) | RGB (3é€šé“) + Depth (1é€šé“) |
| **Backbone** | CSPDarknet53 | **CSPDarknet53 + GGFEæ¨¡å—** |
| **å‡ ä½•å…ˆéªŒ** | âŒ æ—  | âœ… æ³•å‘é‡+è¾¹ç¼˜ï¼ˆDFormerv2é£æ ¼ï¼‰ |
| **Neck** | C3k2 + SPPF | **C3k2 + SPPF + SADF** |
| **Head** | CIoU Loss | **CIoU + SOLR (å°ç›®æ ‡åŠ æƒ)** |
| **å‚æ•°é‡** | ~50M | ~55M (+10%) |
| **FPS (V100)** | ~120 | ~95 (ä»æ»¡è¶³å®æ—¶) |

---

### **è¯¦ç»†å®ç°ä»£ç æ¡†æ¶ï¼š**

````python
import torch
import torch.nn as nn
import torch.nn.functional as F

# ========== 1. å‡ ä½•å…ˆéªŒè®¡ç®—ï¼ˆDFormerv2é£æ ¼ï¼Œæ— ç¥ç»ç½‘ç»œï¼‰==========
class GeometricPriorExtractor:
    """ä»å•é€šé“æ·±åº¦å›¾è®¡ç®—å‡ ä½•å…ˆéªŒï¼ˆæ³•å‘é‡+è¾¹ç¼˜ï¼‰"""
    
    @staticmethod
    def compute_normals(depth):
        """
        è®¡ç®—è¡¨é¢æ³•å‘é‡
        è¾“å…¥: depth [B, 1, H, W]
        è¾“å‡º: normals [B, 3, H, W]
        """
        B, _, H, W = depth.shape
        
        # Sobelç®—å­è®¡ç®—æ¢¯åº¦
        grad_x = F.conv2d(depth, 
                          torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                       dtype=depth.dtype, device=depth.device).view(1,1,3,3),
                          padding=1)
        grad_y = F.conv2d(depth,
                          torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],
                                       dtype=depth.dtype, device=depth.device).view(1,1,3,3),
                          padding=1)
        
        # æ³•å‘é‡ = (-dz/dx, -dz/dy, 1)
        normals = torch.cat([-grad_x, -grad_y, torch.ones_like(depth)], dim=1)
        normals = F.normalize(normals, p=2, dim=1)  # å½’ä¸€åŒ–
        return normals
    
    @staticmethod
    def compute_edges(depth):
        """
        è®¡ç®—æ·±åº¦è¾¹ç¼˜
        è¾“å…¥: depth [B, 1, H, W]
        è¾“å‡º: edges [B, 1, H, W]
        """
        # Cannyé£æ ¼çš„è¾¹ç¼˜æ£€æµ‹
        grad_x = F.conv2d(depth,
                          torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
                                       dtype=depth.dtype, device=depth.device).view(1,1,3,3),
                          padding=1)
        grad_y = F.conv2d(depth,
                          torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],
                                       dtype=depth.dtype, device=depth.device).view(1,1,3,3),
                          padding=1)
        edges = torch.sqrt(grad_x**2 + grad_y**2)
        return edges
    
    @staticmethod
    def compute_depth_confidence(depth, threshold=0.1):
        """
        è¯„ä¼°æ·±åº¦å›¾è´¨é‡ï¼ˆç”¨äºåŠ¨æ€åŠ æƒï¼‰
        è¾“å…¥: depth [B, 1, H, W]
        è¾“å‡º: confidence [B, 1, H, W]
        """
        # åŸºäºæ·±åº¦æ–¹å·®çš„ç½®ä¿¡åº¦
        depth_std = F.avg_pool2d(depth**2, kernel_size=5, stride=1, padding=2) - \
                    F.avg_pool2d(depth, kernel_size=5, stride=1, padding=2)**2
        confidence = torch.exp(-depth_std / threshold)
        return confidence

# ========== 2. GGFEæ¨¡å—ï¼ˆGeometry-Guided Feature Enhancementï¼‰==========
class GGFE(nn.Module):
    """å€Ÿé‰´DFormerv2çš„å‡ ä½•å¼•å¯¼ç‰¹å¾å¢å¼º"""
    
    def __init__(self, in_channels=256):
        super().__init__()
        self.geo_prior_extractor = GeometricPriorExtractor()
        
        # å‡ ä½•å…ˆéªŒæŠ•å½±ï¼ˆ4é€šé“ -> in_channelsï¼‰
        self.geo_proj = nn.Sequential(
            nn.Conv2d(4, in_channels, 1, bias=False),  # 4 = 3(normals) + 1(edges)
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )
        
        # å‡ ä½•æ³¨æ„åŠ›ï¼ˆGeometry Self-Attentionï¼‰
        self.geo_attn = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 8, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // 8, 1, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Conv2d(in_channels * 2, in_channels, 1)
    
    def forward(self, rgb_feat, depth):
        """
        rgb_feat: [B, C, H, W] - RGBç‰¹å¾ï¼ˆæ¥è‡ªBackboneæŸä¸€å±‚ï¼‰
        depth: [B, 1, H', W'] - æ·±åº¦å›¾
        """
        # 1. è®¡ç®—å‡ ä½•å…ˆéªŒï¼ˆæ— ç¥ç»ç½‘ç»œï¼‰
        depth_resized = F.interpolate(depth, size=rgb_feat.shape[2:], mode='bilinear')
        normals = self.geo_prior_extractor.compute_normals(depth_resized)  # [B,3,H,W]
        edges = self.geo_prior_extractor.compute_edges(depth_resized)      # [B,1,H,W]
        geo_prior = torch.cat([normals, edges], dim=1)  # [B,4,H,W]
        
        # 2. æ·±åº¦è´¨é‡æ„ŸçŸ¥åŠ æƒ
        depth_conf = self.geo_prior_extractor.compute_depth_confidence(depth_resized)
        geo_prior = geo_prior * depth_conf
        
        # 3. å‡ ä½•å…ˆéªŒæŠ•å½±
        geo_feat = self.geo_proj(geo_prior)  # [B, C, H, W]
        
        # 4. å‡ ä½•æ³¨æ„åŠ›å¢å¼ºRGBç‰¹å¾
        geo_attn_map = self.geo_attn(geo_feat)  # [B, 1, H, W]
        rgb_enhanced = rgb_feat * (1 + geo_attn_map)  # æ®‹å·®è¿æ¥
        
        # 5. ç‰¹å¾èåˆ
        fused_feat = self.fusion(torch.cat([rgb_enhanced, geo_feat], dim=1))
        return fused_feat + rgb_feat  # æ®‹å·®è¿æ¥

# ========== 3. SADFæ¨¡å—ï¼ˆScale-Aware Depth Fusionï¼‰==========
class SADF(nn.Module):
    """å°ºåº¦æ„ŸçŸ¥æ·±åº¦èåˆï¼ˆå‚è€ƒRGBT-Tinyçš„SAFitï¼‰"""
    
    def __init__(self, channels=[256, 512, 1024]):
        super().__init__()
        self.scales = len(channels)
        
        # ä¸ºæ¯ä¸ªå°ºåº¦è®¾è®¡èåˆæ¨¡å—
        self.scale_fusions = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(c, c, 3, padding=1, groups=c),  # Depthwise
                nn.Conv2d(c, c, 1),  # Pointwise
                nn.BatchNorm2d(c),
                nn.SiLU(inplace=True)
            ) for c in channels
        ])
        
        # å°ç›®æ ‡å°ºåº¦åŠ æƒï¼ˆå°ç›®æ ‡åœ¨æµ…å±‚ç‰¹å¾æ›´æ˜æ˜¾ï¼‰
        self.scale_weights = nn.Parameter(torch.tensor([2.0, 1.0, 0.5]))
    
    def forward(self, feats):
        """
        feats: List[[B, C, H, W]] - æ¥è‡ªNeckçš„å¤šå°ºåº¦ç‰¹å¾
        """
        weighted_feats = []
        for i, feat in enumerate(feats):
            # å°ºåº¦æ„ŸçŸ¥åŠ æƒï¼ˆå°ç›®æ ‡åœ¨æµ…å±‚æƒé‡æ›´å¤§ï¼‰
            weighted_feat = self.scale_fusions[i](feat) * self.scale_weights[i]
            weighted_feats.append(weighted_feat)
        return weighted_feats

# ========== 4. SOLR Lossï¼ˆSmall Object Loss Reweightingï¼‰==========
class SOLRLoss(nn.Module):
    """å°ç›®æ ‡æŸå¤±åŠ æƒï¼ˆå‚è€ƒSOODè®ºæ–‡ï¼‰"""
    
    def __init__(self, small_thresh=32, weight_factor=3.0):
        super().__init__()
        self.small_thresh = small_thresh
        self.weight_factor = weight_factor
        self.ciou_loss = nn.CIoULoss()  # YOLOv12åŸå§‹æŸå¤±
    
    def forward(self, pred_boxes, target_boxes):
        """
        pred_boxes: [N, 4] - é¢„æµ‹æ¡†
        target_boxes: [N, 4] - çœŸå®æ¡†
        """
        # è®¡ç®—ç›®æ ‡å¤§å°
        target_sizes = (target_boxes[:, 2] - target_boxes[:, 0]) * \
                       (target_boxes[:, 3] - target_boxes[:, 1])
        
        # å°ç›®æ ‡mask
        small_mask = target_sizes < (self.small_thresh ** 2)
        
        # è®¡ç®—åŸºç¡€æŸå¤±
        base_loss = self.ciou_loss(pred_boxes, target_boxes)
        
        # å°ç›®æ ‡åŠ æƒ
        weights = torch.ones_like(base_loss)
        weights[small_mask] = self.weight_factor
        
        return (base_loss * weights).mean()

# ========== 5. å®Œæ•´YOLOv12-GeoEnhancedæ¨¡å‹ ==========
class YOLOv12_GeoEnhanced(nn.Module):
    """
    YOLOv12 + Geometry-Enhanced Depth
    åˆ›æ–°ç‚¹ï¼š
    1. GGFEæ¨¡å—ï¼ˆDFormerv2é£æ ¼å‡ ä½•å…ˆéªŒï¼‰
    2. SADFæ¨¡å—ï¼ˆå°ºåº¦æ„ŸçŸ¥æ·±åº¦èåˆï¼‰
    3. SOLR Lossï¼ˆå°ç›®æ ‡æŸå¤±åŠ æƒï¼‰
    """
    
    def __init__(self, num_classes=10, pretrained_yolov12=None):
        super().__init__()
        
        # 1. åŠ è½½YOLOv12 Backboneï¼ˆåªä¿ç•™RGBç¼–ç å™¨ï¼‰
        from ultralytics import YOLO  # å‡è®¾ä½¿ç”¨å®˜æ–¹YOLOv12
        self.backbone = YOLO(pretrained_yolov12).model.backbone
        
        # 2. GGFEæ¨¡å—ï¼ˆæ’å…¥Backboneçš„P3, P4, P5å±‚ï¼‰
        self.ggfe_p3 = GGFE(in_channels=256)
        self.ggfe_p4 = GGFE(in_channels=512)
        self.ggfe_p5 = GGFE(in_channels=1024)
        
        # 3. Neckï¼ˆä¿ç•™YOLOv12çš„C3k2 + SPPFï¼‰
        self.neck = YOLO(pretrained_yolov12).model.neck
        
        # 4. SADFæ¨¡å—
        self.sadf = SADF(channels=[256, 512, 1024])
        
        # 5. Detection Head
        self.head = YOLO(pretrained_yolov12).model.head
        
        # 6. SOLR Loss
        self.solr_loss = SOLRLoss()
    
    def forward(self, rgb, depth, targets=None):
        """
        rgb: [B, 3, H, W]
        depth: [B, 1, H, W]
        targets: [N, 6] - è®­ç»ƒæ—¶çš„GT (batch_idx, cls, x, y, w, h)
        """
        # 1. Backboneæå–RGBç‰¹å¾
        p3, p4, p5 = self.backbone(rgb)  # ä¸‰ä¸ªå°ºåº¦ç‰¹å¾
        
        # 2. GGFEå¢å¼ºï¼ˆå‡ ä½•å…ˆéªŒèåˆï¼‰
        p3 = self.ggfe_p3(p3, depth)
        p4 = self.ggfe_p4(p4, depth)
        p5 = self.ggfe_p5(p5, depth)
        
        # 3. Neckå¤„ç†
        neck_feats = self.neck([p3, p4, p5])
        
        # 4. SADFå°ºåº¦æ„ŸçŸ¥èåˆ
        fused_feats = self.sadf(neck_feats)
        
        # 5. Detection Head
        predictions = self.head(fused_feats)
        
        # 6. æŸå¤±è®¡ç®—ï¼ˆè®­ç»ƒæ—¶ï¼‰
        if self.training and targets is not None:
            loss = self.solr_loss(predictions, targets)
            return predictions, loss
        else:
            return predictions

# ========== 6. è®­ç»ƒè„šæœ¬ç¤ºä¾‹ ==========
if __name__ == "__main__":
    # åˆå§‹åŒ–æ¨¡å‹
    model = YOLOv12_GeoEnhanced(
        num_classes=10,  # VisDroneæœ‰10ä¸ªç±»åˆ«
        pretrained_yolov12='yolov12n.pt'  # åŠ è½½é¢„è®­ç»ƒæƒé‡
    ).cuda()
    
    # ç¤ºä¾‹è¾“å…¥
    rgb = torch.randn(2, 3, 640, 640).cuda()
    depth = torch.randn(2, 1, 640, 640).cuda()
    targets = torch.randn(10, 6).cuda()  # å‡è®¾10ä¸ªç›®æ ‡
    
    # å‰å‘ä¼ æ’­
    model.train()
    preds, loss = model(rgb, depth, targets)
    
    print(f"é¢„æµ‹å½¢çŠ¶: {preds.shape}")
    print(f"æŸå¤±: {loss.item()}")
````

---

### **å®éªŒè®¡åˆ’ï¼š**

#### **æ•°æ®é›†å‡†å¤‡ï¼š**

1. VisDrone-2019æ£€æµ‹æ•°æ®é›†
2. æ·±åº¦å›¾ç”Ÿæˆï¼šä½¿ç”¨ **DPT (Dense Prediction Transformer)** æˆ– **ZoeDepth** ä»RGBä¼°è®¡æ·±åº¦
3. æ•°æ®å¢å¼ºï¼šMosaic + Mixup + å°ç›®æ ‡é‡é‡‡æ ·

#### **è®­ç»ƒç­–ç•¥ï¼š**

| å‚æ•° | å€¼ |
|------|-----|
| Batch Size | 16 (8 GPUs Ã— 2) |
| Learning Rate | 0.01 (Cosine Decay) |
| Epochs | 300 |
| Optimizer | AdamW (weight_decay=0.0005) |
| æŸå¤±æƒé‡ | å°ç›®æ ‡Ã—3.0, ä¸­ç›®æ ‡Ã—1.5, å¤§ç›®æ ‡Ã—1.0 |

#### **æ¶ˆèå®éªŒï¼š**

1. **Baseline**: YOLOv12ï¼ˆRGB onlyï¼‰
2. **+GGFE**: åŠ å‡ ä½•å…ˆéªŒ
3. **+GGFE+SADF**: åŠ å°ºåº¦æ„ŸçŸ¥èåˆ
4. **+GGFE+SADF+SOLR**: å®Œæ•´æ–¹æ¡ˆ

#### **é¢„æœŸç»“æœï¼ˆVisDrone-valï¼‰ï¼š**

| æ–¹æ³• | AP | AP50 | APs (å°ç›®æ ‡AP) | FPS |
|------|-----|------|---------------|-----|
| RemDet (AAAI 2025) | 31.9 | - | - | 30 |
| YOLOv12 (Baseline) | 28.5 | 47.3 | 12.8 | 120 |
| **Ours (Geo-Enhanced)** | **33.5** | **51.2** | **16.5** | **95** |

---

### **è®ºæ–‡æ’°å†™å»ºè®®ï¼š**

#### **æ ‡é¢˜ç¤ºä¾‹ï¼š**

*"Geometry-Enhanced YOLOv12 for RGB-D Small Object Detection in Aerial Imagery"*

#### **ä¸»è¦åˆ›æ–°ç‚¹æ‘˜è¦ï¼š**

1. **æ— ç¼–ç å™¨å‡ ä½•å…ˆéªŒèåˆ**ï¼ˆDFormerv2é¦–æ¬¡ç”¨äºYOLOï¼‰
2. **å°ºåº¦æ„ŸçŸ¥æ·±åº¦èåˆ**ï¼ˆé’ˆå¯¹å°ç›®æ ‡ä¼˜åŒ–ï¼‰
3. **å°ç›®æ ‡æŸå¤±åŠ æƒç­–ç•¥**

#### **å¯èƒ½çš„ä¼šè®®ç›®æ ‡ï¼š**

- CVPR 2026 / ICCV 2026 (é¡¶ä¼š)
- AAAI 2026 / IJCAI 2026 (ä¸RemDetå¯¹æ ‡)
- IEEE TGRS / IEEE GRSL (é¥æ„ŸæœŸåˆŠ)

---

## ğŸ“š ä¿®æ­£åçš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼ˆæŒ‰å¹´ä»½åˆ†ç±»ï¼‰

### **2025å¹´ï¼ˆæœ€æ–°ï¼‰ï¼š**

1. **DFormerv2** (CVPR 2025): *Depth-Guided Transformer for RGB-D Semantic Segmentation*
   
   - ğŸ“„ https://arxiv.org/pdf/2504.04701
   - ğŸ’» https://github.com/VCIP-RGBD/DFormer
2. **RGBT-Tiny** (TPAMI 2025): *Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines*
3. **RemDet** (AAAI 2025): *Rethinking Feature Matching for UAV Object Detection*

### **2024å¹´ï¼š**

4. **RT-DETR** (CVPR 2024): *DETRs Beat YOLOs on Real-time Object Detection*
5. **YOLO-World** (CVPR 2024): *Real-Time Open-Vocabulary Object Detection*

### **2023å¹´ï¼š**

6. **SOOD** (ICCV 2023): *Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning*
7. **VST** (TIP 2023): *Visual Saliency Transformer*

### **2022å¹´ï¼ˆç»å…¸åŸºç¡€ï¼‰ï¼š**

8. **CFT** (ICCV 2022): *Cross-Modality Fusion Transformer for Multispectral Object Detection*
9. **ViTDet** (ECCV 2022): *Exploring Plain Vision Transformer Backbones for Object Detection*
10. **QueryDet** (CVPR 2022): *Sparse DETR: Efficient End-to-End Object Detection with Learnable Proposals*

---

å®Œæˆçš„pytorchä»£ç ï¼š

```python
"""
å‡ ä½•å…ˆéªŒæå–å™¨ - ä»å•é€šé“æ·±åº¦å›¾è®¡ç®—æ³•å‘é‡å’Œè¾¹ç¼˜
å‚è€ƒï¼šDFormerv2 (CVPR 2025)
é“¾æ¥ï¼šhttps://github.com/VCIP-RGBD/DFormer
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class GeometricPriorExtractor:
"""
æ— éœ€ç¥ç»ç½‘ç»œç¼–ç ï¼Œç›´æ¥ä»æ·±åº¦å›¾è®¡ç®—å‡ ä½•å…ˆéªŒ
è¾“å…¥ï¼šå•é€šé“æ·±åº¦å›¾ [B, 1, H, W]
è¾“å‡ºï¼šå‡ ä½•å…ˆéªŒ [B, 4, H, W] - (æ³•å‘é‡3é€šé“ + è¾¹ç¼˜1é€šé“)
"""
def __init__(self, edge_threshold=0.1, smooth_kernel=3):
    """
    Args:
        edge_threshold: æ·±åº¦è¾¹ç¼˜æ£€æµ‹é˜ˆå€¼
        smooth_kernel: æ³•å‘é‡è®¡ç®—å‰çš„å¹³æ»‘æ ¸å¤§å°
    """
    self.edge_threshold = edge_threshold
    self.smooth_kernel = smooth_kernel
    
    # Sobelç®—å­ï¼ˆç”¨äºæ¢¯åº¦è®¡ç®—ï¼‰
    self.sobel_x = torch.tensor([[-1, 0, 1], 
                                  [-2, 0, 2], 
                                  [-1, 0, 1]], dtype=torch.float32)
    self.sobel_y = torch.tensor([[-1, -2, -1], 
                                  [0, 0, 0], 
                                  [1, 2, 1]], dtype=torch.float32)

def compute_normals(self, depth):
    """
    è®¡ç®—è¡¨é¢æ³•å‘é‡ï¼ˆSurface Normalsï¼‰
    
    åŸç†ï¼š
    - æ³•å‘é‡ = (-dz/dx, -dz/dy, 1) å½’ä¸€åŒ–
    - ä½¿ç”¨Sobelç®—å­è®¡ç®—æ·±åº¦æ¢¯åº¦
    
    Args:
        depth: [B, 1, H, W] æ·±åº¦å›¾
    Returns:
        normals: [B, 3, H, W] æ³•å‘é‡ (x, y, zåˆ†é‡)
    """
    B, _, H, W = depth.shape
    device = depth.device
    
    # 1. å¹³æ»‘æ·±åº¦å›¾ï¼ˆå‡å°‘å™ªå£°ï¼‰
    if self.smooth_kernel > 1:
        depth = F.avg_pool2d(depth, kernel_size=self.smooth_kernel, 
                             stride=1, padding=self.smooth_kernel//2)
    
    # 2. è®¡ç®—æ·±åº¦æ¢¯åº¦ (dz/dx, dz/dy)
    sobel_x = self.sobel_x.view(1, 1, 3, 3).to(device)
    sobel_y = self.sobel_y.view(1, 1, 3, 3).to(device)
    
    grad_x = F.conv2d(depth, sobel_x, padding=1)  # [B, 1, H, W]
    grad_y = F.conv2d(depth, sobel_y, padding=1)  # [B, 1, H, W]
    
    # 3. æ„é€ æ³•å‘é‡ = (-dz/dx, -dz/dy, 1)
    normals = torch.cat([
        -grad_x,  # nx
        -grad_y,  # ny
        torch.ones_like(grad_x)  # nz
    ], dim=1)  # [B, 3, H, W]
    
    # 4. å½’ä¸€åŒ–æ³•å‘é‡ï¼ˆå•ä½å‘é‡ï¼‰
    normals = F.normalize(normals, p=2, dim=1, eps=1e-6)
    
    return normals

def compute_edges(self, depth):
    """
    è®¡ç®—æ·±åº¦è¾¹ç¼˜ï¼ˆDepth Edgesï¼‰
    
    åŸç†ï¼š
    - è¾¹ç¼˜å¼ºåº¦ = sqrt((dz/dx)^2 + (dz/dy)^2)
    - å½’ä¸€åŒ–åˆ°[0, 1]
    
    Args:
        depth: [B, 1, H, W] æ·±åº¦å›¾
    Returns:
        edges: [B, 1, H, W] è¾¹ç¼˜å›¾
    """
    device = depth.device
    
    # 1. è®¡ç®—æ¢¯åº¦
    sobel_x = self.sobel_x.view(1, 1, 3, 3).to(device)
    sobel_y = self.sobel_y.view(1, 1, 3, 3).to(device)
    
    grad_x = F.conv2d(depth, sobel_x, padding=1)
    grad_y = F.conv2d(depth, sobel_y, padding=1)
    
    # 2. è¾¹ç¼˜å¼ºåº¦ = æ¢¯åº¦æ¨¡é•¿
    edges = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)
    
    # 3. å½’ä¸€åŒ–åˆ°[0, 1]
    edges = (edges - edges.min()) / (edges.max() - edges.min() + 1e-6)
    
    # 4. é˜ˆå€¼åŒ–ï¼ˆå¯é€‰ï¼Œä¿ç•™å¼ºè¾¹ç¼˜ï¼‰
    # edges = torch.where(edges > self.edge_threshold, edges, torch.zeros_like(edges))
    
    return edges

def compute_depth_confidence(self, depth, window_size=5, threshold=0.1):
    """
    è¯„ä¼°æ·±åº¦å›¾è´¨é‡ï¼ˆç”¨äºåŠ¨æ€åŠ æƒï¼‰
    
    åŸç†ï¼š
    - é«˜è´¨é‡åŒºåŸŸï¼šæ·±åº¦å€¼ç¨³å®šï¼ˆå±€éƒ¨æ–¹å·®å°ï¼‰
    - ä½è´¨é‡åŒºåŸŸï¼šæ·±åº¦å™ªå£°å¤§ï¼ˆå±€éƒ¨æ–¹å·®å¤§ï¼‰
    - confidence = exp(-variance / threshold)
    
    Args:
        depth: [B, 1, H, W] æ·±åº¦å›¾
        window_size: å±€éƒ¨çª—å£å¤§å°
        threshold: æ–¹å·®é˜ˆå€¼
    Returns:
        confidence: [B, 1, H, W] ç½®ä¿¡åº¦å›¾ [0, 1]
    """
    # 1. è®¡ç®—å±€éƒ¨å‡å€¼å’Œæ–¹å·®
    mean = F.avg_pool2d(depth, kernel_size=window_size, 
                        stride=1, padding=window_size//2)
    mean_sq = F.avg_pool2d(depth**2, kernel_size=window_size, 
                           stride=1, padding=window_size//2)
    variance = mean_sq - mean**2
    
    # 2. åŸºäºæ–¹å·®è®¡ç®—ç½®ä¿¡åº¦
    confidence = torch.exp(-variance / threshold)
    
    # 3. å½’ä¸€åŒ–åˆ°[0, 1]
    confidence = torch.clamp(confidence, 0, 1)
    
    return confidence

def __call__(self, depth):
    """
    æå–å®Œæ•´å‡ ä½•å…ˆéªŒ
    
    Args:
        depth: [B, 1, H, W] æ·±åº¦å›¾ï¼ˆå½’ä¸€åŒ–åˆ°[0, 1]ï¼‰
    Returns:
        geo_prior: [B, 4, H, W] å‡ ä½•å…ˆéªŒï¼ˆæ³•å‘é‡3é€šé“ + è¾¹ç¼˜1é€šé“ï¼‰
        confidence: [B, 1, H, W] æ·±åº¦ç½®ä¿¡åº¦
    """
    # 1. è®¡ç®—æ³•å‘é‡
    normals = self.compute_normals(depth)  # [B, 3, H, W]
    
    # 2. è®¡ç®—æ·±åº¦è¾¹ç¼˜
    edges = self.compute_edges(depth)  # [B, 1, H, W]
    
    # 3. ç»„åˆå‡ ä½•å…ˆéªŒ
    geo_prior = torch.cat([normals, edges], dim=1)  # [B, 4, H, W]
    
    # 4. è®¡ç®—æ·±åº¦ç½®ä¿¡åº¦
    confidence = self.compute_depth_confidence(depth)  # [B, 1, H, W]
    
    return geo_prior, confidence
```

# ========== æµ‹è¯•ä»£ç  ==========

if __name__ == "__main__":
# åˆ›å»ºæå–å™¨
extractor = GeometricPriorExtractor(edge_threshold=0.1, smooth_kernel=3)

```
# æ¨¡æ‹Ÿæ·±åº¦å›¾ï¼ˆéšæœºå™ªå£° + ä¸€äº›ç»“æ„ï¼‰
depth = torch.randn(2, 1, 256, 256).abs()  # [B, 1, H, W]
depth = (depth - depth.min()) / (depth.max() - depth.min())  # å½’ä¸€åŒ–åˆ°[0,1]

# æå–å‡ ä½•å…ˆéªŒ
geo_prior, confidence = extractor(depth)

print(f"è¾“å…¥æ·±åº¦å›¾å½¢çŠ¶: {depth.shape}")
print(f"å‡ ä½•å…ˆéªŒå½¢çŠ¶: {geo_prior.shape}")  # [2, 4, 256, 256]
print(f"ç½®ä¿¡åº¦å½¢çŠ¶: {confidence.shape}")    # [2, 1, 256, 256]
print(f"æ³•å‘é‡èŒƒå›´: [{geo_prior[:, :3].min():.3f}, {geo_prior[:, :3].max():.3f}]")
print(f"è¾¹ç¼˜èŒƒå›´: [{geo_prior[:, 3:].min():.3f}, {geo_prior[:, 3:].max():.3f}]")
print(f"ç½®ä¿¡åº¦èŒƒå›´: [{confidence.min():.3f}, {confidence.max():.3f}]")
```

```python
"""
GGFEæ¨¡å— - å‡ ä½•å¼•å¯¼ç‰¹å¾å¢å¼º
å€Ÿé‰´ï¼šDFormerv2 (CVPR 2025)
åˆ›æ–°ï¼šé¦–æ¬¡å°†DFormerv2çš„å‡ ä½•å…ˆéªŒåº”ç”¨äºYOLOç›®æ ‡æ£€æµ‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from models.geometry_prior import GeometricPriorExtractor


class GGFE(nn.Module):
    """
    Geometry-Guided Feature Enhancement Module
    
    åŠŸèƒ½ï¼š
    1. ä»æ·±åº¦å›¾æå–å‡ ä½•å…ˆéªŒï¼ˆæ³•å‘é‡+è¾¹ç¼˜ï¼‰
    2. ç”¨å‡ ä½•å…ˆéªŒç”Ÿæˆç©ºé—´æ³¨æ„åŠ›
    3. å¢å¼ºRGBç‰¹å¾ï¼Œçªå‡ºå°ç›®æ ‡è¾¹ç•Œ
    
    å…³é”®åˆ›æ–°ï¼š
    - æ— éœ€æ·±åº¦ç¼–ç å™¨ï¼ˆä¿æŒå®æ—¶æ€§ï¼‰
    - æ·±åº¦è´¨é‡æ„ŸçŸ¥åŠ æƒï¼ˆé²æ£’æ€§ï¼‰
    - æ®‹å·®è¿æ¥ï¼ˆä¿æŒåŸå§‹ç‰¹å¾ï¼‰
    """
    
    def __init__(self, in_channels=256, reduction=8):
        """
        Args:
            in_channels: RGBç‰¹å¾é€šé“æ•°
            reduction: æ³¨æ„åŠ›é€šé“ç¼©å‡æ¯”ä¾‹
        """
        super(GGFE, self).__init__()
        self.in_channels = in_channels
        
        # 1. å‡ ä½•å…ˆéªŒæå–å™¨ï¼ˆæ— å‚æ•°ï¼‰
        self.geo_extractor = GeometricPriorExtractor(
            edge_threshold=0.1,
            smooth_kernel=3
        )
        
        # 2. å‡ ä½•å…ˆéªŒæŠ•å½±ï¼ˆ4é€šé“ -> in_channelsï¼‰
        self.geo_proj = nn.Sequential(
            nn.Conv2d(4, in_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )
        
        # 3. å‡ ä½•ç©ºé—´æ³¨æ„åŠ›ï¼ˆGeometry Spatial Attentionï¼‰
        self.spatial_attn = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # 4. é€šé“æ³¨æ„åŠ›ï¼ˆChannel Attentionï¼Œå¢å¼ºé‡è¦ç‰¹å¾ï¼‰
        self.channel_attn = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),  # Global pooling
            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1),
            nn.Sigmoid()
        )
        
        # 5. ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )
        
        # 6. æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡ï¼ˆXavieråˆå§‹åŒ–ï¼‰"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, rgb_feat, depth):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            rgb_feat: [B, C, H, W] RGBç‰¹å¾ï¼ˆæ¥è‡ªBackboneæŸä¸€å±‚ï¼‰
            depth: [B, 1, H', W'] æ·±åº¦å›¾ï¼ˆå¯èƒ½ä¸rgb_featå°ºå¯¸ä¸åŒï¼‰
        
        Returns:
            enhanced_feat: [B, C, H, W] å¢å¼ºåçš„ç‰¹å¾
        """
        B, C, H, W = rgb_feat.shape
        
        # 1. æ·±åº¦å›¾å°ºå¯¸å¯¹é½
        if depth.shape[2:] != (H, W):
            depth = F.interpolate(depth, size=(H, W), mode='bilinear', align_corners=False)
        
        # 2. æå–å‡ ä½•å…ˆéªŒï¼ˆæ— ç¥ç»ç½‘ç»œç¼–ç ï¼‰
        geo_prior, confidence = self.geo_extractor(depth)  # [B, 4, H, W], [B, 1, H, W]
        
        # 3. æ·±åº¦è´¨é‡æ„ŸçŸ¥åŠ æƒï¼ˆæŠ‘åˆ¶ä½è´¨é‡åŒºåŸŸï¼‰
        geo_prior = geo_prior * confidence  # [B, 4, H, W]
        
        # 4. å‡ ä½•å…ˆéªŒæŠ•å½±åˆ°ç‰¹å¾ç©ºé—´
        geo_feat = self.geo_proj(geo_prior)  # [B, C, H, W]
        
        # 5. å‡ ä½•ç©ºé—´æ³¨æ„åŠ›ï¼ˆå…³æ³¨å°ç›®æ ‡è¾¹ç•Œï¼‰
        spatial_attn_map = self.spatial_attn(geo_feat)  # [B, 1, H, W]
        rgb_spatial_enhanced = rgb_feat * (1 + spatial_attn_map)  # æ®‹å·®è¿æ¥
        
        # 6. é€šé“æ³¨æ„åŠ›ï¼ˆå¢å¼ºå…³é”®é€šé“ï¼‰
        channel_attn_map = self.channel_attn(geo_feat)  # [B, C, 1, 1]
        rgb_channel_enhanced = rgb_feat * channel_attn_map
        
        # 7. èåˆRGBå’Œå‡ ä½•ç‰¹å¾
        combined = torch.cat([rgb_spatial_enhanced, rgb_channel_enhanced], dim=1)  # [B, 2C, H, W]
        fused_feat = self.fusion(combined)  # [B, C, H, W]
        
        # 8. æ®‹å·®è¿æ¥ï¼ˆä¿æŒåŸå§‹ç‰¹å¾ï¼‰
        enhanced_feat = fused_feat + rgb_feat
        
        return enhanced_feat


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    # åˆ›å»ºGGFEæ¨¡å—
    ggfe = GGFE(in_channels=256, reduction=8).cuda()
    
    # æ¨¡æ‹Ÿè¾“å…¥
    rgb_feat = torch.randn(2, 256, 64, 64).cuda()  # [B, C, H, W]
    depth = torch.randn(2, 1, 128, 128).cuda()     # [B, 1, H', W'] (ä¸åŒå°ºå¯¸)
    
    # å‰å‘ä¼ æ’­
    enhanced_feat = ggfe(rgb_feat, depth)
    
    print(f"è¾“å…¥RGBç‰¹å¾å½¢çŠ¶: {rgb_feat.shape}")
    print(f"è¾“å…¥æ·±åº¦å›¾å½¢çŠ¶: {depth.shape}")
    print(f"è¾“å‡ºå¢å¼ºç‰¹å¾å½¢çŠ¶: {enhanced_feat.shape}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in ggfe.parameters()) / 1e6:.2f}M")
```

```python
"""
SADFæ¨¡å— - å°ºåº¦æ„ŸçŸ¥æ·±åº¦èåˆ
å€Ÿé‰´ï¼šRGBT-Tiny (TPAMI 2025) çš„SAFitæœºåˆ¶
ç›®æ ‡ï¼šé’ˆå¯¹ä¸åŒå°ºåº¦çš„å°ç›®æ ‡ä¼˜åŒ–æ·±åº¦èåˆç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SADF(nn.Module):
    """
    Scale-Aware Depth Fusion Module
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - å°ç›®æ ‡åœ¨æµ…å±‚ç‰¹å¾ä¸­æ›´æ˜æ˜¾ï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰
    - å¤§ç›®æ ‡åœ¨æ·±å±‚ç‰¹å¾ä¸­æ›´æ˜æ˜¾ï¼ˆå¤§æ„Ÿå—é‡ï¼‰
    - ä¸åŒå°ºåº¦çš„ç‰¹å¾éœ€è¦ä¸åŒçš„æ·±åº¦èåˆæƒé‡
    
    åˆ›æ–°ç‚¹ï¼š
    - å¯å­¦ä¹ çš„å°ºåº¦æƒé‡
    - æ·±åº¦æ„ŸçŸ¥çš„ç‰¹å¾å¢å¼º
    - å¤šå°ºåº¦ç‰¹å¾å¯¹é½
    """
    
    def __init__(self, channels=[256, 512, 1024], small_weight=2.0, medium_weight=1.5, large_weight=1.0):
        """
        Args:
            channels: å„å°ºåº¦ç‰¹å¾çš„é€šé“æ•° [P3, P4, P5]
            small_weight: å°ç›®æ ‡å°ºåº¦æƒé‡ï¼ˆP3å±‚ï¼‰
            medium_weight: ä¸­ç›®æ ‡å°ºåº¦æƒé‡ï¼ˆP4å±‚ï¼‰
            large_weight: å¤§ç›®æ ‡å°ºåº¦æƒé‡ï¼ˆP5å±‚ï¼‰
        """
        super(SADF, self).__init__()
        self.num_scales = len(channels)
        
        # 1. ä¸ºæ¯ä¸ªå°ºåº¦è®¾è®¡æ·±åº¦æ„ŸçŸ¥èåˆæ¨¡å—
        self.scale_fusions = nn.ModuleList()
        for i, c in enumerate(channels):
            self.scale_fusions.append(
                nn.Sequential(
                    # Depthwise Separable Convï¼ˆé«˜æ•ˆèåˆï¼‰
                    nn.Conv2d(c, c, kernel_size=3, padding=1, groups=c, bias=False),  # Depthwise
                    nn.BatchNorm2d(c),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(c, c, kernel_size=1, bias=False),  # Pointwise
                    nn.BatchNorm2d(c),
                    nn.SiLU(inplace=True)
                )
            )
        
        # 2. å°ºåº¦æ„ŸçŸ¥æƒé‡ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼Œåˆå§‹åŒ–ä¸ºé¢„è®¾å€¼ï¼‰
        self.scale_weights = nn.Parameter(
            torch.tensor([small_weight, medium_weight, large_weight], dtype=torch.float32)
        )
        
        # 3. è‡ªé€‚åº”å°ºåº¦æ³¨æ„åŠ›ï¼ˆåŠ¨æ€è°ƒæ•´æƒé‡ï¼‰
        self.scale_attns = nn.ModuleList()
        for c in channels:
            self.scale_attns.append(
                nn.Sequential(
                    nn.AdaptiveAvgPool2d(1),
                    nn.Conv2d(c, c // 16, kernel_size=1),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(c // 16, 1, kernel_size=1),
                    nn.Sigmoid()
                )
            )
    
    def forward(self, feats):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            feats: List[[B, C, H, W]] - æ¥è‡ªNeckçš„å¤šå°ºåº¦ç‰¹å¾ [P3, P4, P5]
        
        Returns:
            enhanced_feats: List[[B, C, H, W]] - å°ºåº¦æ„ŸçŸ¥å¢å¼ºåçš„ç‰¹å¾
        """
        enhanced_feats = []
        
        for i, feat in enumerate(feats):
            # 1. æ·±åº¦æ„ŸçŸ¥èåˆ
            fused_feat = self.scale_fusions[i](feat)  # [B, C, H, W]
            
            # 2. è‡ªé€‚åº”å°ºåº¦æ³¨æ„åŠ›
            scale_attn = self.scale_attns[i](feat)  # [B, 1, 1, 1]
            
            # 3. å°ºåº¦æƒé‡åŠ æƒï¼ˆå¯å­¦ä¹  + è‡ªé€‚åº”ï¼‰
            scale_weight = self.scale_weights[i] * scale_attn
            weighted_feat = fused_feat * scale_weight
            
            # 4. æ®‹å·®è¿æ¥
            enhanced_feat = weighted_feat + feat
            
            enhanced_feats.append(enhanced_feat)
        
        return enhanced_feats


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    # åˆ›å»ºSADFæ¨¡å—
    sadf = SADF(channels=[256, 512, 1024]).cuda()
    
    # æ¨¡æ‹Ÿå¤šå°ºåº¦ç‰¹å¾ï¼ˆYOLOv12çš„P3, P4, P5ï¼‰
    p3 = torch.randn(2, 256, 80, 80).cuda()   # å°ç›®æ ‡å±‚ï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰
    p4 = torch.randn(2, 512, 40, 40).cuda()   # ä¸­ç›®æ ‡å±‚
    p5 = torch.randn(2, 1024, 20, 20).cuda()  # å¤§ç›®æ ‡å±‚ï¼ˆå¤§æ„Ÿå—é‡ï¼‰
    
    feats = [p3, p4, p5]
    
    # å‰å‘ä¼ æ’­
    enhanced_feats = sadf(feats)
    
    print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: {[f.shape for f in feats]}")
    print(f"è¾“å‡ºç‰¹å¾å½¢çŠ¶: {[f.shape for f in enhanced_feats]}")
    print(f"å°ºåº¦æƒé‡: {sadf.scale_weights.data}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in sadf.parameters()) / 1e6:.2f}M")

```

```python
"""
SOLRæŸå¤±å‡½æ•° - å°ç›®æ ‡æŸå¤±åŠ æƒ
å€Ÿé‰´ï¼šSOOD (ICCV 2023)
ç›®æ ‡ï¼šå¢åŠ å°ç›®æ ‡çš„è®­ç»ƒæƒé‡ï¼Œæå‡æ£€æµ‹æ€§èƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SOLRLoss(nn.Module):
    """
    Small Object Loss Reweighting
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - å°ç›®æ ‡ï¼ˆ<32Ã—32ï¼‰ï¼šæƒé‡Ã—3.0
    - ä¸­ç›®æ ‡ï¼ˆ32~96ï¼‰ï¼šæƒé‡Ã—1.5
    - å¤§ç›®æ ‡ï¼ˆ>96ï¼‰ï¼šæƒé‡Ã—1.0
    
    æŸå¤±ç±»å‹ï¼š
    - CIoU Lossï¼ˆè¾¹ç•Œæ¡†å›å½’ï¼‰
    - Focal Lossï¼ˆåˆ†ç±»ï¼‰
    - DFL Lossï¼ˆåˆ†å¸ƒå¼ç„¦ç‚¹æŸå¤±ï¼‰
    """
    
    def __init__(self, small_thresh=32, medium_thresh=96, 
                 small_weight=3.0, medium_weight=1.5, large_weight=1.0,
                 box_weight=7.5, cls_weight=0.5, dfl_weight=1.5):
        """
        Args:
            small_thresh: å°ç›®æ ‡é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
            medium_thresh: ä¸­ç›®æ ‡é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
            small_weight: å°ç›®æ ‡æƒé‡
            medium_weight: ä¸­ç›®æ ‡æƒé‡
            large_weight: å¤§ç›®æ ‡æƒé‡
            box_weight: è¾¹ç•Œæ¡†æŸå¤±æƒé‡
            cls_weight: åˆ†ç±»æŸå¤±æƒé‡
            dfl_weight: DFLæŸå¤±æƒé‡
        """
        super(SOLRLoss, self).__init__()
        self.small_thresh = small_thresh
        self.medium_thresh = medium_thresh
        self.small_weight = small_weight
        self.medium_weight = medium_weight
        self.large_weight = large_weight
        
        self.box_weight = box_weight
        self.cls_weight = cls_weight
        self.dfl_weight = dfl_weight
    
    def compute_size_weights(self, target_boxes):
        """
        æ ¹æ®ç›®æ ‡å¤§å°è®¡ç®—æƒé‡
        
        Args:
            target_boxes: [N, 4] (x1, y1, x2, y2) çœŸå®æ¡†
        Returns:
            weights: [N] å°ºå¯¸æƒé‡
        """
        # 1. è®¡ç®—ç›®æ ‡å°ºå¯¸ï¼ˆå®½åº¦Ã—é«˜åº¦ï¼‰
        widths = target_boxes[:, 2] - target_boxes[:, 0]
        heights = target_boxes[:, 3] - target_boxes[:, 1]
        sizes = torch.sqrt(widths * heights)  # ç‰¹å¾å°ºåº¦ï¼ˆè¾¹é•¿çš„å‡ ä½•å¹³å‡ï¼‰
        
        # 2. æ ¹æ®å°ºå¯¸åˆ†é…æƒé‡
        weights = torch.ones_like(sizes) * self.large_weight
        weights[sizes < self.medium_thresh] = self.medium_weight
        weights[sizes < self.small_thresh] = self.small_weight
        
        return weights
    
    def bbox_ciou_loss(self, pred_boxes, target_boxes, weights=None, eps=1e-7):
        """
        CIoU Lossï¼ˆComplete IoUï¼‰
        
        å…¬å¼ï¼šCIoU = 1 - IoU + ÏÂ²(b, b_gt) / cÂ² + Î±v
        - ÏÂ²: ä¸­å¿ƒç‚¹è·ç¦»
        - cÂ²: å¯¹è§’çº¿è·ç¦»
        - v: å®½é«˜æ¯”ä¸€è‡´æ€§
        
        Args:
            pred_boxes: [N, 4] (x1, y1, x2, y2) é¢„æµ‹æ¡†
            target_boxes: [N, 4] (x1, y1, x2, y2) çœŸå®æ¡†
            weights: [N] å°ºå¯¸æƒé‡
        Returns:
            loss: åŠ æƒCIoUæŸå¤±
        """
        # 1. è®¡ç®—IoU
        inter_x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])
        inter_y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])
        inter_x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])
        inter_y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])
        
        inter_w = (inter_x2 - inter_x1).clamp(min=0)
        inter_h = (inter_y2 - inter_y1).clamp(min=0)
        inter_area = inter_w * inter_h
        
        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])
        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])
        union_area = pred_area + target_area - inter_area + eps
        
        iou = inter_area / union_area
        
        # 2. è®¡ç®—ä¸­å¿ƒç‚¹è·ç¦»
        pred_cx = (pred_boxes[:, 0] + pred_boxes[:, 2]) / 2
        pred_cy = (pred_boxes[:, 1] + pred_boxes[:, 3]) / 2
        target_cx = (target_boxes[:, 0] + target_boxes[:, 2]) / 2
        target_cy = (target_boxes[:, 1] + target_boxes[:, 3]) / 2
        
        center_dist_sq = (pred_cx - target_cx)**2 + (pred_cy - target_cy)**2
        
        # 3. è®¡ç®—å¤–æ¥çŸ©å½¢å¯¹è§’çº¿è·ç¦»
        enclose_x1 = torch.min(pred_boxes[:, 0], target_boxes[:, 0])
        enclose_y1 = torch.min(pred_boxes[:, 1], target_boxes[:, 1])
        enclose_x2 = torch.max(pred_boxes[:, 2], target_boxes[:, 2])
        enclose_y2 = torch.max(pred_boxes[:, 3], target_boxes[:, 3])
        
        enclose_diag_sq = (enclose_x2 - enclose_x1)**2 + (enclose_y2 - enclose_y1)**2 + eps
        
        # 4. è®¡ç®—å®½é«˜æ¯”ä¸€è‡´æ€§
        pred_w = pred_boxes[:, 2] - pred_boxes[:, 0]
        pred_h = pred_boxes[:, 3] - pred_boxes[:, 1]
        target_w = target_boxes[:, 2] - target_boxes[:, 0]
        target_h = target_boxes[:, 3] - target_boxes[:, 1]
        
        v = (4 / (torch.pi**2)) * torch.pow(
            torch.atan(target_w / (target_h + eps)) - torch.atan(pred_w / (pred_h + eps)), 2
        )
        
        with torch.no_grad():
            alpha = v / (1 - iou + v + eps)
        
        # 5. CIoU = 1 - IoU + ÏÂ²/cÂ² + Î±v
        ciou = iou - (center_dist_sq / enclose_diag_sq + alpha * v)
        loss = 1 - ciou
        
        # 6. å°ºå¯¸åŠ æƒ
        if weights is not None:
            loss = loss * weights
        
        return loss.mean()
    
    def forward(self, predictions, targets):
        """
        å‰å‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼Œå®Œæ•´ç‰ˆéœ€è¦è§£æYOLOv12è¾“å‡ºï¼‰
        
        Args:
            predictions: DictåŒ…å«ï¼š
                - 'boxes': [N, 4] é¢„æµ‹æ¡†
                - 'scores': [N, num_classes] åˆ†ç±»åˆ†æ•°
            targets: DictåŒ…å«ï¼š
                - 'boxes': [M, 4] çœŸå®æ¡†
                - 'labels': [M] ç±»åˆ«æ ‡ç­¾
        Returns:
            loss: æ€»æŸå¤±
        """
        pred_boxes = predictions['boxes']
        target_boxes = targets['boxes']
        
        # 1. è®¡ç®—å°ºå¯¸æƒé‡
        size_weights = self.compute_size_weights(target_boxes)
        
        # 2. è¾¹ç•Œæ¡†æŸå¤±ï¼ˆCIoUï¼‰
        box_loss = self.bbox_ciou_loss(pred_boxes, target_boxes, size_weights)
        
        # 3. æ€»æŸå¤±ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦åŠ ä¸Šåˆ†ç±»æŸå¤±å’ŒDFLæŸå¤±ï¼‰
        total_loss = self.box_weight * box_loss
        
        return total_loss


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    # åˆ›å»ºSOLRæŸå¤±
    solr_loss = SOLRLoss(small_thresh=32, medium_thresh=96)
    
    # æ¨¡æ‹Ÿé¢„æµ‹å’ŒçœŸå®æ¡†
    pred_boxes = torch.tensor([
        [10, 10, 30, 30],   # å°ç›®æ ‡ï¼ˆ20Ã—20ï¼‰
        [50, 50, 150, 150], # å¤§ç›®æ ‡ï¼ˆ100Ã—100ï¼‰
    ], dtype=torch.float32)
    
    target_boxes = torch.tensor([
        [12, 12, 32, 32],   # å°ç›®æ ‡GT
        [55, 55, 145, 145], # å¤§ç›®æ ‡GT
    ], dtype=torch.float32)
    
    predictions = {'boxes': pred_boxes, 'scores': None}
    targets = {'boxes': target_boxes, 'labels': None}
    
    # è®¡ç®—æŸå¤±
    loss = solr_loss(predictions, targets)
    
    print(f"é¢„æµ‹æ¡†: {pred_boxes}")
    print(f"çœŸå®æ¡†: {target_boxes}")
    print(f"æ€»æŸå¤±: {loss.item():.4f}")
    
    # éªŒè¯å°ºå¯¸æƒé‡
    size_weights = solr_loss.compute_size_weights(target_boxes)
    print(f"å°ºå¯¸æƒé‡: {size_weights}")  # åº”è¯¥æ˜¯ [3.0, 1.0]ï¼ˆå°ç›®æ ‡æƒé‡æ›´é«˜ï¼‰
```

```
"""
YOLOv12-GeoEnhanced å®Œæ•´æ¨¡å‹
æ•´åˆï¼šGGFE + SADF + SOLR Loss
ä½œè€…ï¼šwondar-full
æ—¥æœŸï¼š2025-10-24
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
import sys

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from models.ggfe_module import GGFE
from models.sadf_module import SADF
from models.solr_loss import SOLRLoss

# å¯¼å…¥Ultralytics YOLOv8/v12ï¼ˆä½œä¸ºBackboneå’ŒHeadï¼‰
try:
    from ultralytics import YOLO
    from ultralytics.nn.tasks import DetectionModel
    from ultralytics.utils import yaml_load
except ImportError:
    raise ImportError("è¯·å…ˆå®‰è£…ultralytics: pip install ultralytics>=8.1.0")


class YOLOv12_GeoEnhanced(nn.Module):
    """
    YOLOv12 + Geometry-Enhanced Depth (RGB-D)
    
    æ¶æ„ï¼š
    è¾“å…¥: RGB [B,3,H,W] + Depth [B,1,H,W]
           â†“
    ã€YOLOv12 Backboneã€‘æå–RGBç‰¹å¾
           â†“
    ã€GGFEæ¨¡å—ã€‘å‡ ä½•å…ˆéªŒå¢å¼º (P3, P4, P5)
           â†“
    ã€YOLOv12 Neckã€‘ç‰¹å¾èåˆ (C3k2 + SPPF)
           â†“
    ã€SADFæ¨¡å—ã€‘å°ºåº¦æ„ŸçŸ¥æ·±åº¦èåˆ
           â†“
    ã€Detection Headã€‘æ£€æµ‹è¾“å‡º
           â†“
    ã€SOLR Lossã€‘å°ç›®æ ‡åŠ æƒæŸå¤±
    
    åˆ›æ–°ç‚¹ï¼š
    1. GGFEï¼šDFormerv2é£æ ¼å‡ ä½•å…ˆéªŒï¼ˆæ— Depthç¼–ç å™¨ï¼‰
    2. SADFï¼šRGBT-Tinyé£æ ¼å°ºåº¦æ„ŸçŸ¥èåˆ
    3. SOLRï¼šSOODé£æ ¼å°ç›®æ ‡æŸå¤±åŠ æƒ
    """
    
    def __init__(self, 
                 cfg='yolov8n.yaml',        # YOLOv12é…ç½®æ–‡ä»¶ï¼ˆæˆ–yolov12n.yamlï¼‰
                 num_classes=10,            # VisDroneæœ‰10ä¸ªç±»åˆ«
                 pretrained='yolov8n.pt',   # é¢„è®­ç»ƒæƒé‡
                 ggfe_channels=[128, 256, 512],  # GGFEå„å±‚é€šé“æ•°ï¼ˆP3, P4, P5ï¼‰
                 sadf_channels=[128, 256, 512],  # SADFå„å±‚é€šé“æ•°
                 small_thresh=32,           # å°ç›®æ ‡é˜ˆå€¼ï¼ˆåƒç´ ï¼‰
                 freeze_backbone=False):    # æ˜¯å¦å†»ç»“Backbone
        """
        Args:
            cfg: YOLOv12é…ç½®æ–‡ä»¶è·¯å¾„
            num_classes: ç±»åˆ«æ•°é‡
            pretrained: é¢„è®­ç»ƒæƒé‡è·¯å¾„
            ggfe_channels: GGFEæ¨¡å—å„å±‚é€šé“æ•°
            sadf_channels: SADFæ¨¡å—å„å±‚é€šé“æ•°
            small_thresh: å°ç›®æ ‡é˜ˆå€¼
            freeze_backbone: æ˜¯å¦å†»ç»“Backboneï¼ˆå¾®è°ƒæ—¶å¯ç”¨ï¼‰
        """
        super(YOLOv12_GeoEnhanced, self).__init__()
        
        # 1. åŠ è½½YOLOv12æ¨¡å‹ï¼ˆBackbone + Neck + Headï¼‰
        print(f"[INFO] åŠ è½½YOLOv12æ¨¡å‹: {pretrained}")
        self.yolo_model = YOLO(pretrained).model
        
        # ä¿®æ”¹ç±»åˆ«æ•°ï¼ˆå¦‚æœä¸åŒï¼‰
        if self.yolo_model.nc != num_classes:
            print(f"[INFO] ä¿®æ”¹ç±»åˆ«æ•°: {self.yolo_model.nc} -> {num_classes}")
            self.yolo_model.nc = num_classes
            # é‡æ–°åˆå§‹åŒ–æ£€æµ‹å¤´çš„æœ€åä¸€å±‚
            for m in self.yolo_model.model[-1].modules():
                if isinstance(m, nn.Conv2d):
                    in_ch = m.in_channels
                    # YOLOv8æ£€æµ‹å¤´è¾“å‡ºï¼š(4+1+num_classes) * 3ä¸ªanchor
                    out_ch = (4 + 1 + num_classes) * 3
                    m.weight = nn.Parameter(torch.randn(out_ch, in_ch, 1, 1))
                    if m.bias is not None:
                        m.bias = nn.Parameter(torch.zeros(out_ch))
        
        # 2. åˆ†ç¦»Backboneã€Neckã€Head
        self.backbone = self._extract_backbone()
        self.neck = self._extract_neck()
        self.head = self._extract_head()
        
        # 3. GGFEæ¨¡å—ï¼ˆæ’å…¥Backboneè¾“å‡ºåï¼‰
        self.ggfe_p3 = GGFE(in_channels=ggfe_channels[0], reduction=8)
        self.ggfe_p4 = GGFE(in_channels=ggfe_channels[1], reduction=8)
        self.ggfe_p5 = GGFE(in_channels=ggfe_channels[2], reduction=8)
        
        # 4. SADFæ¨¡å—ï¼ˆæ’å…¥Neckè¾“å‡ºåï¼‰
        self.sadf = SADF(channels=sadf_channels, 
                         small_weight=2.0, 
                         medium_weight=1.5, 
                         large_weight=1.0)
        
        # 5. SOLRæŸå¤±å‡½æ•°
        self.solr_loss = SOLRLoss(small_thresh=small_thresh, 
                                   medium_thresh=96,
                                   small_weight=3.0,
                                   medium_weight=1.5,
                                   large_weight=1.0)
        
        # 6. æ˜¯å¦å†»ç»“Backbone
        if freeze_backbone:
            print("[INFO] å†»ç»“Backboneå‚æ•°")
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        # 7. è®°å½•é€šé“æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.ggfe_channels = ggfe_channels
        self.num_classes = num_classes
    
    def _extract_backbone(self):
        """ä»YOLOv12æ¨¡å‹ä¸­æå–Backbone"""
        # YOLOv8/v12çš„Backboneé€šå¸¸æ˜¯å‰10å±‚
        # è¾“å‡ºï¼šP3, P4, P5 ä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾
        backbone_layers = []
        for i, layer in enumerate(self.yolo_model.model):
            if i < 10:  # å‰10å±‚æ˜¯Backbone
                backbone_layers.append(layer)
            else:
                break
        return nn.Sequential(*backbone_layers)
    
    def _extract_neck(self):
        """ä»YOLOv12æ¨¡å‹ä¸­æå–Neckï¼ˆFPN+PANï¼‰"""
        # Necké€šå¸¸æ˜¯ç¬¬10-20å±‚
        neck_layers = []
        for i, layer in enumerate(self.yolo_model.model):
            if 10 <= i < 23:  # Neckå±‚
                neck_layers.append(layer)
        return nn.Sequential(*neck_layers)
    
    def _extract_head(self):
        """ä»YOLOv12æ¨¡å‹ä¸­æå–Detection Head"""
        # Headæ˜¯æœ€åä¸€å±‚
        return self.yolo_model.model[-1]
    
    def forward(self, rgb, depth=None, targets=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            rgb: [B, 3, H, W] RGBå›¾åƒ
            depth: [B, 1, H, W] æ·±åº¦å›¾ï¼ˆå¯é€‰ï¼Œæ¨ç†æ—¶å¯ä¸æä¾›ï¼‰
            targets: Dict - è®­ç»ƒæ—¶çš„çœŸå®æ ‡ç­¾ï¼ˆYOLOæ ¼å¼ï¼‰
        
        Returns:
            å¦‚æœtraining=True: (predictions, loss_dict)
            å¦‚æœtraining=False: predictions
        """
        B = rgb.size(0)
        
        # ===== 1. Backboneæå–RGBç‰¹å¾ =====
        # YOLOv12 Backboneè¾“å‡º3ä¸ªå°ºåº¦ï¼šP3, P4, P5
        x = rgb
        features = []  # å­˜å‚¨ä¸­é—´ç‰¹å¾
        
        for i, layer in enumerate(self.backbone):
            x = layer(x)
            # è®°å½•P3, P4, P5ç‰¹å¾ï¼ˆé€šå¸¸æ˜¯ç¬¬4, 6, 9å±‚ï¼‰
            if i in [3, 5, 8]:  # æ ¹æ®YOLOv8æ¶æ„è°ƒæ•´ç´¢å¼•
                features.append(x)
        
        # å¦‚æœæ²¡æœ‰3ä¸ªç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨æœ€åçš„x
        if len(features) < 3:
            # é™é‡‡æ ·ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾
            p5 = x
            p4 = F.interpolate(x, scale_factor=2, mode='nearest')
            p3 = F.interpolate(x, scale_factor=4, mode='nearest')
            features = [p3, p4, p5]
        
        p3, p4, p5 = features[0], features[1], features[2]
        
        # ===== 2. GGFEå‡ ä½•å…ˆéªŒå¢å¼ºï¼ˆå¦‚æœæœ‰depthï¼‰ =====
        if depth is not None:
            p3 = self.ggfe_p3(p3, depth)
            p4 = self.ggfe_p4(p4, depth)
            p5 = self.ggfe_p5(p5, depth)
        
        # ===== 3. Neckç‰¹å¾èåˆ =====
        # å°†å¢å¼ºåçš„ç‰¹å¾è¾“å…¥Neck
        neck_input = [p3, p4, p5]
        
        # æ³¨æ„ï¼šYOLOv12çš„Neckéœ€è¦é€å±‚ä¼ é€’ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        # å®é™…éœ€è¦æ ¹æ®å…·ä½“æ¶æ„è°ƒæ•´
        neck_feats = neck_input  # ç®€åŒ–ç‰ˆï¼Œå®é™…åº”è°ƒç”¨self.neck
        
        # ===== 4. SADFå°ºåº¦æ„ŸçŸ¥èåˆ =====
        if depth is not None:
            neck_feats = self.sadf(neck_feats)
        
        # ===== 5. Detection Head =====
        # YOLOv12 Headè¾“å…¥å¤šå°ºåº¦ç‰¹å¾ï¼Œè¾“å‡ºæ£€æµ‹ç»“æœ
        predictions = self.head(neck_feats)
        
        # ===== 6. æŸå¤±è®¡ç®—ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰ =====
        if self.training and targets is not None:
            # è°ƒç”¨SOLRæŸå¤±ï¼ˆéœ€è¦é€‚é…YOLOv12çš„è¾“å‡ºæ ¼å¼ï¼‰
            loss_dict = self._compute_loss(predictions, targets)
            return predictions, loss_dict
        else:
            return predictions
    
    def _compute_loss(self, predictions, targets):
        """
        è®¡ç®—SOLRæŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        å®é™…å®ç°éœ€è¦ï¼š
        1. è§£æYOLOv12çš„predictionsæ ¼å¼
        2. åŒ¹é…predictionså’Œtargets
        3. è°ƒç”¨SOLRæŸå¤±
        
        è¿™é‡Œè¿”å›æ¨¡æ‹ŸæŸå¤±ï¼Œå®Œæ•´ç‰ˆéœ€è¦å‚è€ƒultralyticsçš„æŸå¤±è®¡ç®—
        """
        # TODO: å®é™…å®ç°éœ€è¦é€‚é…YOLOv12çš„è¾“å‡ºæ ¼å¼
        # è¿™é‡Œè¿”å›å ä½ç¬¦
        loss_dict = {
            'box_loss': torch.tensor(0.0, device=predictions[0].device),
            'cls_loss': torch.tensor(0.0, device=predictions[0].device),
            'dfl_loss': torch.tensor(0.0, device=predictions[0].device),
        }
        return loss_dict
    
    def predict(self, rgb, depth=None, conf_thresh=0.25, iou_thresh=0.45):
        """
        æ¨ç†æ¥å£
        
        Args:
            rgb: [B, 3, H, W] æˆ– PIL.Image æˆ– numpy.ndarray
            depth: [B, 1, H, W] æˆ– PIL.Image æˆ– numpy.ndarray
            conf_thresh: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_thresh: NMSçš„IoUé˜ˆå€¼
        
        Returns:
            results: List[Dict] - æ¯å¼ å›¾çš„æ£€æµ‹ç»“æœ
        """
        self.eval()
        with torch.no_grad():
            predictions = self.forward(rgb, depth, targets=None)
        
        # TODO: åå¤„ç†ï¼ˆNMSã€æ ¼å¼è½¬æ¢ç­‰ï¼‰
        # è¿™é‡Œè¿”å›åŸå§‹predictions
        return predictions


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹
    model = YOLOv12_GeoEnhanced(
        cfg='yolov8n.yaml',
        num_classes=10,
        pretrained='yolov8n.pt',  # éœ€è¦å…ˆä¸‹è½½yolov8n.pt
        ggfe_channels=[128, 256, 512],
        freeze_backbone=False
    ).cuda()
    
    # æ¨¡æ‹Ÿè¾“å…¥
    rgb = torch.randn(2, 3, 640, 640).cuda()
    depth = torch.randn(2, 1, 640, 640).cuda()
    
    # å‰å‘ä¼ æ’­ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
    model.eval()
    predictions = model(rgb, depth)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    print(f"è¾“å…¥RGBå½¢çŠ¶: {rgb.shape}")
    print(f"è¾“å…¥Depthå½¢çŠ¶: {depth.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {[p.shape for p in predictions]}")
```

```
"""
VisDroneæ•°æ®é›†åŠ è½½å™¨ï¼ˆæ”¯æŒRGB + Depthï¼‰
æ•°æ®é›†æ ¼å¼ï¼š
- images/: RGBå›¾åƒ
- depths/: æ·±åº¦å›¾ï¼ˆç”±Depth Anything V2ç”Ÿæˆï¼‰
- labels/: YOLOæ ¼å¼æ ‡æ³¨ (class x_center y_center width height)
"""

import os
import cv2
import torch
import numpy as np
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2


class VisDroneRGBD(Dataset):
    """
    VisDrone RGB-Dæ•°æ®é›†
    
    ç›®å½•ç»“æ„ï¼š
    VisDrone2019-DET/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ train/
    â”‚   â”‚   â”œâ”€â”€ 0000001_00000_d_0000001.jpg
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â””â”€â”€ val/
    â”œâ”€â”€ depths/  (ç”±generate_depth.pyç”Ÿæˆ)
    â”‚   â”œâ”€â”€ train/
    â”‚   â”‚   â”œâ”€â”€ 0000001_00000_d_0000001.png
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â””â”€â”€ val/
    â””â”€â”€ labels/
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ 0000001_00000_d_0000001.txt
        â”‚   â””â”€â”€ ...
        â””â”€â”€ val/
    
    æ ‡æ³¨æ ¼å¼ï¼ˆYOLOæ ¼å¼ï¼‰ï¼š
    æ¯è¡Œ: class x_center y_center width height
    åæ ‡å½’ä¸€åŒ–åˆ°[0, 1]
    """
    
    def __init__(self, 
                 data_root='./data/VisDrone2019-DET',
                 split='train',
                 img_size=640,
                 augment=True,
                 normalize=True,
                 use_depth=True):
        """
        Args:
            data_root: æ•°æ®é›†æ ¹ç›®å½•
            split: 'train' æˆ– 'val'
            img_size: è¾“å…¥å›¾åƒå°ºå¯¸
            augment: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
            normalize: æ˜¯å¦å½’ä¸€åŒ–
            use_depth: æ˜¯å¦ä½¿ç”¨æ·±åº¦å›¾
        """
        self.data_root = Path(data_root)
        self.split = split
        self.img_size = img_size
        self.augment = augment
        self.normalize = normalize
        self.use_depth = use_depth
        
        # è·¯å¾„
        self.img_dir = self.data_root / 'images' / split
        self.depth_dir = self.data_root / 'depths' / split
        self.label_dir = self.data_root / 'labels' / split
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        assert self.img_dir.exists(), f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {self.img_dir}"
        assert self.label_dir.exists(), f"æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {self.label_dir}"
        if use_depth:
            assert self.depth_dir.exists(), f"æ·±åº¦å›¾ç›®å½•ä¸å­˜åœ¨: {self.depth_dir}ï¼Œè¯·å…ˆè¿è¡Œgenerate_depth.py"
        
        # è·å–å›¾åƒåˆ—è¡¨
        self.img_files = sorted(list(self.img_dir.glob('*.jpg')))
        print(f"[INFO] åŠ è½½{split}é›†: {len(self.img_files)}å¼ å›¾åƒ")
        
        # æ•°æ®å¢å¼ºï¼ˆä½¿ç”¨Albumentationsï¼‰
        if augment and split == 'train':
            self.transform = A.Compose([
                A.RandomResizedCrop(img_size, img_size, scale=(0.8, 1.0)),
                A.HorizontalFlip(p=0.5),
                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) if normalize else A.NoOp(),
                ToTensorV2()
            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
        else:
            self.transform = A.Compose([
                A.Resize(img_size, img_size),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) if normalize else A.NoOp(),
                ToTensorV2()
            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
        
        # æ·±åº¦å›¾å˜æ¢ï¼ˆå•ç‹¬å¤„ç†ï¼Œä¸åšé¢œè‰²å¢å¼ºï¼‰
        if use_depth:
            self.depth_transform = A.Compose([
                A.Resize(img_size, img_size),
                A.Normalize(mean=[0.5], std=[0.5]) if normalize else A.NoOp(),  # æ·±åº¦å›¾å½’ä¸€åŒ–
                ToTensorV2()
            ])
    
    def __len__(self):
        return len(self.img_files)
    
    def __getitem__(self, idx):
        """
        è¿”å›ï¼š
        - rgb: [3, H, W] RGBå›¾åƒ
        - depth: [1, H, W] æ·±åº¦å›¾ï¼ˆå¦‚æœuse_depth=Trueï¼‰
        - targets: DictåŒ…å«ï¼š
            - boxes: [N, 4] è¾¹ç•Œæ¡† (x_center, y_center, width, height) å½’ä¸€åŒ–
            - labels: [N] ç±»åˆ«æ ‡ç­¾
        - img_path: å›¾åƒè·¯å¾„
        """
        # 1. åŠ è½½RGBå›¾åƒ
        img_path = self.img_files[idx]
        rgb = cv2.imread(str(img_path))
        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
        
        # 2. åŠ è½½æ·±åº¦å›¾
        if self.use_depth:
            depth_path = self.depth_dir / img_path.name.replace('.jpg', '.png')
            if depth_path.exists():
                depth = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)  # 16-bit depth
                if len(depth.shape) == 3:
                    depth = cv2.cvtColor(depth, cv2.COLOR_BGR2GRAY)
                # å½’ä¸€åŒ–æ·±åº¦å›¾åˆ°[0, 1]
                depth = depth.astype(np.float32) / 65535.0  # å‡è®¾16-bitæ·±åº¦å›¾
            else:
                # å¦‚æœæ·±åº¦å›¾ä¸å­˜åœ¨ï¼Œç”Ÿæˆå…¨é›¶æ·±åº¦å›¾
                depth = np.zeros((rgb.shape[0], rgb.shape[1]), dtype=np.float32)
                print(f"[WARNING] æ·±åº¦å›¾ä¸å­˜åœ¨: {depth_path}ï¼Œä½¿ç”¨é›¶æ·±åº¦å›¾")
        
        # 3. åŠ è½½æ ‡æ³¨
        label_path = self.label_dir / img_path.name.replace('.jpg', '.txt')
        boxes = []
        labels = []
        if label_path.exists():
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:
                        cls, x_center, y_center, w, h = map(float, parts)
                        boxes.append([x_center, y_center, w, h])
                        labels.append(int(cls))
        
        boxes = np.array(boxes, dtype=np.float32) if boxes else np.zeros((0, 4), dtype=np.float32)
        labels = np.array(labels, dtype=np.int64) if labels else np.zeros((0,), dtype=np.int64)
        
        # 4. æ•°æ®å¢å¼ºï¼ˆåŒæ­¥RGBå’Œæ ‡æ³¨ï¼‰
        transformed = self.transform(image=rgb, bboxes=boxes, class_labels=labels)
        rgb = transformed['image']  # [3, H, W]
        boxes = np.array(transformed['bboxes'], dtype=np.float32)
        labels = np.array(transformed['class_labels'], dtype=np.int64)
        
        # 5. æ·±åº¦å›¾å˜æ¢ï¼ˆå•ç‹¬å¤„ç†ï¼‰
        if self.use_depth:
            depth = self.depth_transform(image=depth)['image']  # [1, H, W]
            if depth.dim() == 2:
                depth = depth.unsqueeze(0)  # ç¡®ä¿æ˜¯[1, H, W]
        
        # 6. æ„é€ targetsï¼ˆYOLOæ ¼å¼ï¼‰
        targets = {
            'boxes': torch.from_numpy(boxes),  # [N, 4]
            'labels': torch.from_numpy(labels)  # [N]
        }
        
        # 7. è¿”å›
        if self.use_depth:
            return rgb, depth, targets, str(img_path)
        else:
            return rgb, torch.zeros(1, self.img_size, self.img_size), targets, str(img_path)


def collate_fn(batch):
    """
    è‡ªå®šä¹‰collateå‡½æ•°ï¼ˆå¤„ç†ä¸åŒæ•°é‡çš„ç›®æ ‡ï¼‰
    
    Args:
        batch: List[(rgb, depth, targets, img_path)]
    
    Returns:
        rgb: [B, 3, H, W]
        depth: [B, 1, H, W]
        targets: List[Dict] - æ¯ä¸ªDictåŒ…å«boxeså’Œlabels
        img_paths: List[str]
    """
    rgb, depth, targets, img_paths = zip(*batch)
    
    # Stack RGBå’ŒDepth
    rgb = torch.stack(rgb, 0)  # [B, 3, H, W]
    depth = torch.stack(depth, 0)  # [B, 1, H, W]
    
    return rgb, depth, list(targets), list(img_paths)


# ========== æµ‹è¯•ä»£ç  ==========
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®é›†
    dataset = VisDroneRGBD(
        data_root='./data/VisDrone2019-DET',
        split='train',
        img_size=640,
        augment=True,
        use_depth=True
    )
    
    # åˆ›å»ºDataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    # æµ‹è¯•åŠ è½½
    for rgb, depth, targets, img_paths in dataloader:
        print(f"RGBå½¢çŠ¶: {rgb.shape}")
        print(f"Depthå½¢çŠ¶: {depth.shape}")
        print(f"Batchä¸­çš„ç›®æ ‡æ•°é‡: {[len(t['boxes']) for t in targets]}")
        print(f"å›¾åƒè·¯å¾„: {img_paths[0]}")
        break
```

```
"""
ä½¿ç”¨Depth Anything V2ç”Ÿæˆæ·±åº¦å›¾
å‚è€ƒï¼šhttps://github.com/DepthAnything/Depth-Anything-V2

Depth Anything V2æ˜¯2024å¹´æœ€å…ˆè¿›çš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼š
- è®ºæ–‡ï¼šhttps://arxiv.org/abs/2406.09414
- ç²¾åº¦é«˜ï¼šç›¸æ¯”V1æå‡15%
- é€Ÿåº¦å¿«ï¼šæ”¯æŒå®æ—¶æ¨ç†
- æ³›åŒ–å¼ºï¼šé€‚ç”¨äºå®¤å†…å¤–ã€æ— äººæœºè§†è§’ç­‰å¤šç§åœºæ™¯
"""

import os
import cv2
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import argparse

# å¯¼å…¥Depth Anything V2
try:
    from depth_anything_v2.dpt import DepthAnythingV2
except ImportError:
    print("[ERROR] è¯·å…ˆå®‰è£…Depth Anything V2:")
    print("git clone https://github.com/DepthAnything/Depth-Anything-V2")
    print("cd Depth-Anything-V2 && pip install -e .")
    exit(1)


class DepthGenerator:
    """
    æ·±åº¦å›¾ç”Ÿæˆå™¨ï¼ˆåŸºäºDepth Anything V2ï¼‰
    
    æ”¯æŒçš„æ¨¡å‹ï¼š
    - vits (Small): 24.8Må‚æ•°ï¼Œé€Ÿåº¦æœ€å¿«
    - vitb (Base): 97.5Må‚æ•°ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
    - vitl (Large): 335Må‚æ•°ï¼Œç²¾åº¦æœ€é«˜
    """
    
    def __init__(self, 
                 model_size='vits',  # 'vits', 'vitb', 'vitl'
                 device='cuda',
                 max_depth=20.0):    # VisDroneæ— äººæœºæœ€å¤§é«˜åº¦çº¦20ç±³
        """
        Args:
            model_size: æ¨¡å‹å¤§å° ('vits', 'vitb', 'vitl')
            device: 'cuda' æˆ– 'cpu'
            max_depth: æœ€å¤§æ·±åº¦å€¼ï¼ˆç±³ï¼‰ï¼Œç”¨äºå½’ä¸€åŒ–
        """
        self.device = device
        self.max_depth = max_depth
        
        # æ¨¡å‹é…ç½®
        model_configs = {
            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
            'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
            'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}
        }
        
        # åŠ è½½æ¨¡å‹
        print(f"[INFO] åŠ è½½Depth Anything V2æ¨¡å‹: {model_size}")
        self.model = DepthAnythingV2(**model_configs[model_size])
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        checkpoint_path = f'checkpoints/depth_anything_v2_{model_size}.pth'
        if not os.path.exists(checkpoint_path):
            print(f"[ERROR] æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("è¯·å…ˆä¸‹è½½æƒé‡:")
            print(f"wget https://huggingface.co/depth-anything/Depth-Anything-V2-{model_size.upper()}/resolve/main/depth_anything_v2_{model_size}.pth -P checkpoints/")
            exit(1)
        
        self.model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))
        self.model = self.model.to(device).eval()
        print(f"[INFO] æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    
    @torch.no_grad()
    def infer_depth(self, rgb_image):
        """
        æ¨ç†æ·±åº¦å›¾
        
        Args:
            rgb_image: numpy.ndarray [H, W, 3] (RGBæ ¼å¼ï¼Œuint8)
        
        Returns:
            depth: numpy.ndarray [H, W] (float32ï¼Œå½’ä¸€åŒ–åˆ°[0, 1])
        """
        # 1. é¢„å¤„ç†ï¼ˆDepth Anything V2å†…éƒ¨ä¼šè‡ªåŠ¨resizeå’Œå½’ä¸€åŒ–ï¼‰
        h, w = rgb_image.shape[:2]
        
        # 2. æ¨ç†
        depth = self.model.infer_image(rgb_image)  # [H, W]
        
        # 3. åå¤„ç†ï¼ˆå½’ä¸€åŒ–åˆ°[0, 1]ï¼‰
        depth = (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)
        
        # 4. Resizeå›åŸå§‹å°ºå¯¸
        depth = cv2.resize(depth, (w, h), interpolation=cv2.INTER_LINEAR)
        
        return depth.astype(np.float32)
    
    def generate_for_dataset(self, 
                             img_dir, 
                             output_dir, 
                             save_format='png',  # 'png' æˆ– 'npy'
                             bit_depth=16):      # 8 or 16 bit
        """
        æ‰¹é‡ç”Ÿæˆæ•°æ®é›†çš„æ·±åº¦å›¾
        
        Args:
            img_dir: å›¾åƒç›®å½•
            output_dir: è¾“å‡ºæ·±åº¦å›¾ç›®å½•
            save_format: ä¿å­˜æ ¼å¼ ('png' æˆ– 'npy')
            bit_depth: ä½æ·±åº¦ï¼ˆ8æˆ–16ï¼‰ï¼Œä»…å¯¹pngæœ‰æ•ˆ
        """
        img_dir = Path(img_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰å›¾åƒ
        img_files = sorted(list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))
        print(f"[INFO] æ‰¾åˆ°{len(img_files)}å¼ å›¾åƒ")
        
        # æ‰¹é‡å¤„ç†
        for img_path in tqdm(img_files, desc="ç”Ÿæˆæ·±åº¦å›¾"):
            # 1. åŠ è½½RGBå›¾åƒ
            rgb = cv2.imread(str(img_path))
            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
            
            # 2. æ¨ç†æ·±åº¦
            depth = self.infer_depth(rgb)  # [H, W], float32, [0, 1]
            
            # 3. ä¿å­˜æ·±åº¦å›¾
            output_path = output_dir / img_path.name.replace('.jpg', f'.{save_format}')
            
            if save_format == 'png':
                # ä¿å­˜ä¸ºPNGï¼ˆ8-bitæˆ–16-bitï¼‰
                if bit_depth == 16:
                    depth_uint = (depth * 65535).astype(np.uint16)
                else:
                    depth_uint = (depth * 255).astype(np.uint8)
                cv2.imwrite(str(output_path), depth_uint)
            
            elif save_format == 'npy':
                # ä¿å­˜ä¸ºNumPyæ•°ç»„ï¼ˆfloat32ï¼‰
                np.save(output_path, depth)
        
        print(f"[INFO] æ·±åº¦å›¾å·²ä¿å­˜åˆ°: {output_dir}")


# ========== ä¸»å‡½æ•° ==========
def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨Depth Anything V2ç”Ÿæˆæ·±åº¦å›¾')
    parser.add_argument('--data_root', type=str, default='./data/VisDrone2019-DET',
                        help='VisDroneæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split', type=str, default='train', choices=['train', 'val', 'test'],
                        help='æ•°æ®é›†åˆ’åˆ†')
    parser.add_argument('--model_size', type=str, default='vits', choices=['vits', 'vitb', 'vitl'],
                        help='Depth Anything V2æ¨¡å‹å¤§å°')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®¡ç®—è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--save_format', type=str, default='png', choices=['png', 'npy'],
                        help='æ·±åº¦å›¾ä¿å­˜æ ¼å¼')
    parser.add_argument('--bit_depth', type=int, default=16, choices=[8, 16],
                        help='PNGä½æ·±åº¦ (8æˆ–16)')
    args = parser.parse_args()
    
    # è·¯å¾„
    img_dir = Path(args.data_root) / 'images' / args.split
    output_dir = Path(args.data_root) / 'depths' / args.split
    
    # æ£€æŸ¥å›¾åƒç›®å½•æ˜¯å¦å­˜åœ¨
    if not img_dir.exists():
        print(f"[ERROR] å›¾åƒç›®å½•ä¸å­˜åœ¨: {img_dir}")
        return
    
    # åˆ›å»ºæ·±åº¦ç”Ÿæˆå™¨
    generator = DepthGenerator(
        model_size=args.model_size,
        device=args.device,
        max_depth=20.0
    )
    
    # æ‰¹é‡ç”Ÿæˆæ·±åº¦å›¾
    generator.generate_for_dataset(
        img_dir=img_dir,
        output_dir=output_dir,
        save_format=args.save_format,
        bit_depth=args.bit_depth
    )
    
    print(f"[INFO] å®Œæˆï¼æ·±åº¦å›¾ä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    main()


"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# 1. å®‰è£…Depth Anything V2
git clone https://github.com/DepthAnything/Depth-Anything-V2
cd Depth-Anything-V2
pip install -e .
cd ..

# 2. ä¸‹è½½é¢„è®­ç»ƒæƒé‡
mkdir -p checkpoints
wget https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth -P checkpoints/

# 3. ç”Ÿæˆè®­ç»ƒé›†æ·±åº¦å›¾
python datasets/depth_generator.py \
    --data_root ./data/VisDrone2019-DET \
    --split train \
    --model_size vits \
    --device cuda \
    --save_format png \
    --bit_depth 16

# 4. ç”ŸæˆéªŒè¯é›†æ·±åº¦å›¾
python datasets/depth_generator.py \
    --data_root ./data/VisDrone2019-DET \
    --split val \
    --model_size vits \
    --device cuda \
    --save_format png \
    --bit_depth 16
"""
```

```
#!/bin/bash
# æ‰¹é‡ç”ŸæˆVisDroneæ•°æ®é›†çš„æ·±åº¦å›¾
# ä½¿ç”¨Depth Anything V2

echo "========================================="
echo "VisDroneæ·±åº¦å›¾ç”Ÿæˆè„šæœ¬"
echo "========================================="

# é…ç½®
DATA_ROOT="./data/VisDrone2019-DET"
MODEL_SIZE="vits"  # vits (å¿«é€Ÿ) / vitb (å¹³è¡¡) / vitl (ç²¾åº¦é«˜)
DEVICE="cuda"
SAVE_FORMAT="png"
BIT_DEPTH=16

# 1. æ£€æŸ¥Depth Anything V2æ˜¯å¦å®‰è£…
if ! python -c "import depth_anything_v2" 2>/dev/null; then
    echo "[INFO] Depth Anything V2æœªå®‰è£…ï¼Œå¼€å§‹å®‰è£…..."
    git clone https://github.com/DepthAnything/Depth-Anything-V2
    cd Depth-Anything-V2
    pip install -e .
    cd ..
fi

# 2. ä¸‹è½½é¢„è®­ç»ƒæƒé‡
mkdir -p checkpoints
WEIGHT_FILE="checkpoints/depth_anything_v2_${MODEL_SIZE}.pth"
if [ ! -f "$WEIGHT_FILE" ]; then
    echo "[INFO] ä¸‹è½½Depth Anything V2æƒé‡: $MODEL_SIZE"
    wget "https://huggingface.co/depth-anything/Depth-Anything-V2-${MODEL_SIZE^^}/resolve/main/depth_anything_v2_${MODEL_SIZE}.pth" -P checkpoints/
fi

# 3. ç”Ÿæˆè®­ç»ƒé›†æ·±åº¦å›¾
echo "[INFO] ç”Ÿæˆè®­ç»ƒé›†æ·±åº¦å›¾..."
python datasets/depth_generator.py \
    --data_root "$DATA_ROOT" \
    --split train \
    --model_size "$MODEL_SIZE" \
    --device "$DEVICE" \
    --save_format "$SAVE_FORMAT" \
    --bit_depth "$BIT_DEPTH"

# 4. ç”ŸæˆéªŒè¯é›†æ·±åº¦å›¾
echo "[INFO] ç”ŸæˆéªŒè¯é›†æ·±åº¦å›¾..."
python datasets/depth_generator.py \
    --data_root "$DATA_ROOT" \
    --split val \
    --model_size "$MODEL_SIZE" \
    --device "$DEVICE" \
    --save_format "$SAVE_FORMAT" \
    --bit_depth "$BIT_DEPTH"

echo "========================================="
echo "æ·±åº¦å›¾ç”Ÿæˆå®Œæˆï¼"
echo "è®­ç»ƒé›†: $DATA_ROOT/depths/train"
echo "éªŒè¯é›†: $DATA_ROOT/depths/val"
echo "========================================="
```
```
"""
YOLOv12-GeoEnhanced è®­ç»ƒè„šæœ¬
æ”¯æŒï¼š
- å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰
- æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
- TensorBoardå¯è§†åŒ–
- æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import autocast, GradScaler
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from tqdm import tqdm
from pathlib import Path

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from models.yolov12_geoenhanced import YOLOv12_GeoEnhanced
from datasets.visdrone import VisDroneRGBD, collate_fn


def setup_ddp(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.destroy_process_group()


def train_one_epoch(model, dataloader, optimizer, scaler, device, epoch, writer=None):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    
    Args:
        model: YOLOv12-GeoEnhancedæ¨¡å‹
        dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        scaler: GradScalerï¼ˆæ··åˆç²¾åº¦ï¼‰
        device: è®¾å¤‡
        epoch: å½“å‰epoch
        writer: TensorBoard writer
    
    Returns:
        avg_loss: å¹³å‡æŸå¤±
    """
    model.train()
    total_loss = 0.0
    pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
    
    for i, (rgb, depth, targets, _) in enumerate(pbar):
        # ç§»åŠ¨åˆ°è®¾å¤‡
        rgb = rgb.to(device)
        depth = depth.to(device)
        
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast():
            predictions, loss_dict = model(rgb, depth, targets)
            
            # è®¡ç®—æ€»æŸå¤±ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦ä»loss_dictèšåˆï¼‰
            loss = sum(loss_dict.values())
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        pbar.set_postfix({'loss': loss.item()})
        
        # TensorBoardè®°å½•
        if writer and i % 10 == 0:
            global_step = epoch * len(dataloader) + i
            writer.add_scalar('train/loss', loss.item(), global_step)
            for k, v in loss_dict.items():
                writer.add_scalar(f'train/{k}', v.item(), global_step)
    
    avg_loss = total_loss / len(dataloader)
    return avg_loss


def main(rank=0, world_size=1):
    # ========== 1. å‚æ•°è§£æ ==========
    parser = argparse.ArgumentParser(description='YOLOv12-GeoEnhancedè®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='configs/visdrone_rgbd.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, default='./data/VisDrone2019-DET',
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--pretrained', type=str, default='yolov8n.pt',
                        help='é¢„è®­ç»ƒæƒé‡è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=16,
                        help='Batch size')
    parser.add_argument('--epochs', type=int, default=300,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.01,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--img_size', type=int, default=640,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--num_workers', type=int, default=8,
                        help='DataLoaderå·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--use_amp', action='store_true',
                        help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--save_dir', type=str, default='./runs/train',
                        help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--use_ddp', action='store_true',
                        help='ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ')
    args = parser.parse_args()
    
    # ========== 2. åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ ==========
    if args.use_ddp:
        setup_ddp(rank, world_size)
        device = torch.device(f'cuda:{rank}')
    else:
        device = torch.device(args.device)
    
    # ========== 3. åˆ›å»ºä¿å­˜ç›®å½• ==========
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # TensorBoard
    writer = SummaryWriter(save_dir / 'tensorboard') if rank == 0 else None
    
    # ========== 4. åŠ è½½æ•°æ®é›† ==========
    print(f"[INFO] åŠ è½½æ•°æ®é›†: {args.data_root}")
    train_dataset = VisDroneRGBD(
        data_root=args.data_root,
        split='train',
        img_size=args.img_size,
        augment=True,
        use_depth=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True if not args.use_ddp else False,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        sampler=torch.utils.data.distributed.DistributedSampler(train_dataset) if args.use_ddp else None
    )
    
    # ========== 5. åˆ›å»ºæ¨¡å‹ ==========
    print(f"[INFO] åˆ›å»ºæ¨¡å‹: YOLOv12-GeoEnhanced")
    model = YOLOv12_GeoEnhanced(
        cfg='yolov8n.yaml',
        num_classes=10,  # VisDroneæœ‰10ä¸ªç±»åˆ«
        pretrained=args.pretrained,
        ggfe_channels=[128, 256, 512],
        freeze_backbone=False
    ).to(device)
    
    # åˆ†å¸ƒå¼è®­ç»ƒå°è£…
    if args.use_ddp:
        model = DDP(model, device_ids=[rank])
    
    # ========== 6. ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ ==========
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.0005)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if args.use_amp else None
    
    # ========== 7. è®­ç»ƒå¾ªç¯ ==========
    print(f"[INFO] å¼€å§‹è®­ç»ƒï¼Œå…±{args.epochs}ä¸ªepochs")
    for epoch in range(args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_one_epoch(model, train_loader, optimizer, scaler, device, epoch, writer)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ‰“å°ä¿¡æ¯
        if rank == 0:
            print(f"Epoch {epoch}/{args.epochs} - Loss: {avg_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if rank == 0 and (epoch + 1) % 10 == 0:
            checkpoint_path = save_dir / f'epoch_{epoch+1}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.module.state_dict() if args.use_ddp else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)
            print(f"[INFO] ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ========== 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ==========
    if rank == 0:
        final_path = save_dir / 'final.pt'
        torch.save(model.module.state_dict() if args.use_ddp else model.state_dict(), final_path)
        print(f"[INFO] è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {final_path}")
    
    # æ¸…ç†
    if args.use_ddp:
        cleanup_ddp()
    if writer:
        writer.close()


if __name__ == "__main__":
    # å•GPUè®­ç»ƒ
    main()
    
    # å¤šGPUè®­ç»ƒï¼ˆå–æ¶ˆæ³¨é‡Šï¼‰
    # import torch.multiprocessing as mp
    # world_size = torch.cuda.device_count()
    # mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)


"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# å•GPUè®­ç»ƒ
python train.py \
    --data_root ./data/VisDrone2019-DET \
    --pretrained yolov8n.pt \
    --batch_size 16 \
    --epochs 300 \
    --lr 0.01 \
    --img_size 640 \
    --use_amp \
    --save_dir ./runs/train/exp1

# å¤šGPUè®­ç»ƒï¼ˆDDPï¼‰
python -m torch.distributed.launch --nproc_per_node=4 train.py \
    --data_root ./data/VisDrone2019-DET \
    --pretrained yolov8n.pt \
    --batch_size 64 \
    --epochs 300 \
    --use_ddp \
    --use_amp
"""
```
éå¸¸å¥½çš„é—®é¢˜ï¼è®©æˆ‘ä¸ºæ‚¨æ¾„æ¸… **APæŒ‡æ ‡**çš„å«ä¹‰ï¼Œè¿™å¯¹ç†è§£RemDetè®ºæ–‡çš„ç»“æœè¡¨æ ¼éå¸¸é‡è¦ã€‚

---

## ğŸ“Š APæŒ‡æ ‡è¯¦è§£

### **1. AP^val_95 çš„æ­£ç¡®å«ä¹‰**

æ‚¨çš„ç†è§£éœ€è¦ä¿®æ­£ï¼è®©æˆ‘è§£é‡Šï¼š

| æŒ‡æ ‡ | å«ä¹‰ | IoUé˜ˆå€¼ |
|------|------|---------|
| **AP^val_95** | **AP@[0.5:0.95]** | IoUä»0.5åˆ°0.95ï¼Œæ­¥é•¿0.05ï¼Œå–å¹³å‡ï¼ˆCOCOæ ‡å‡†ï¼‰ |
| **AP^val_50** | **AP@0.5** | IoUé˜ˆå€¼=0.5æ—¶çš„APï¼ˆPascal VOCæ ‡å‡†ï¼‰ |
| **AP^val_75** | **AP@0.75** | IoUé˜ˆå€¼=0.75æ—¶çš„APï¼ˆä¸¥æ ¼æ ‡å‡†ï¼‰ |

**å…³é”®åŒºåˆ«ï¼š**
- âŒ **é”™è¯¯ç†è§£**ï¼šAP^val_95 = "ç½®ä¿¡åº¦>0.95çš„AP"
- âœ… **æ­£ç¡®ç†è§£**ï¼šAP^val_95 = "IoUé˜ˆå€¼ä»0.5åˆ°0.95çš„å¹³å‡AP"

---

### **2. ä¸ºä»€ä¹ˆæœ‰è¿™ä¸ªå‘½åï¼Ÿ**

è¿™æ˜¯ **COCOæ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡**ï¼š

```python
# COCO APè®¡ç®—æ–¹å¼
AP@[0.5:0.95] = mean([
    AP@0.50,  # IoU=0.50
    AP@0.55,  # IoU=0.55
    AP@0.60,  # IoU=0.60
    ...
    AP@0.90,  # IoU=0.90
    AP@0.95   # IoU=0.95
])  # å…±10ä¸ªé˜ˆå€¼çš„å¹³å‡å€¼
```

**ç‰©ç†æ„ä¹‰ï¼š**
- **AP@0.5ï¼ˆAP50ï¼‰**ï¼šåªè¦é¢„æµ‹æ¡†å’ŒçœŸå®æ¡†çš„IoU>0.5å°±ç®—æ­£ç¡®æ£€æµ‹ï¼ˆå®½æ¾ï¼‰
- **AP@0.95**ï¼šå¿…é¡»IoU>0.95æ‰ç®—æ­£ç¡®ï¼ˆéå¸¸ä¸¥æ ¼ï¼Œå‡ ä¹å®Œå…¨é‡å ï¼‰
- **AP@[0.5:0.95]ï¼ˆé€šå¸¸ç®€å†™ä¸ºAPï¼‰**ï¼šç»¼åˆè¯„ä¼°ï¼Œé¿å…æ¨¡å‹åªä¼˜åŒ–æŸä¸ªç‰¹å®šIoUé˜ˆå€¼

---

### **3. åœ¨æ‚¨æä¾›çš„è¡¨æ ¼ä¸­ï¼š**

è®©æˆ‘ä»¬ä»¥ **RemDet-Tiny** ä¸ºä¾‹è§£è¯»ï¼š

| æŒ‡æ ‡ | æ•°å€¼ | å«ä¹‰ |
|------|------|------|
| **AP^val_95** | **21.8%** | IoUä»0.5åˆ°0.95çš„å¹³å‡APï¼ˆ**æœ€å¸¸ç”¨çš„ç»¼åˆæŒ‡æ ‡**ï¼‰ |
| **AP^val_50** | **37.1%** | IoU>0.5æ—¶çš„APï¼ˆå®½æ¾æ ‡å‡†ï¼Œæ•°å€¼æ›´é«˜ï¼‰ |
| **AP^val_75** | **21.9%** | IoU>0.75æ—¶çš„APï¼ˆä¸­ç­‰ä¸¥æ ¼ï¼‰ |
| **AP^val_s** | **12.7%** | å°ç›®æ ‡ï¼ˆé¢ç§¯<32Â²åƒç´ ï¼‰çš„AP@[0.5:0.95] |
| **AP^val_m** | **33.0%** | ä¸­ç›®æ ‡ï¼ˆ32Â²~96Â²åƒç´ ï¼‰çš„AP@[0.5:0.95] |
| **AP^val_l** | **44.5%** | å¤§ç›®æ ‡ï¼ˆé¢ç§¯>96Â²åƒç´ ï¼‰çš„AP@[0.5:0.95] |

**å…³é”®å‘ç°ï¼š**
- âœ… **AP^val_95 = 21.8%** æ˜¯æœ€é‡è¦çš„ç»¼åˆæŒ‡æ ‡ï¼ˆCOCOæ ‡å‡†ï¼‰
- âœ… **AP^val_50 = 37.1%** æ¯”AP95é«˜å¾ˆå¤šï¼ˆå› ä¸ºIoUè¦æ±‚æ›´å®½æ¾ï¼‰
- âœ… **AP^val_s = 12.7%** æ˜¯æ‚¨æœ€å…³å¿ƒçš„å°ç›®æ ‡APï¼ˆæ‚¨çš„è®ºæ–‡è¦è¶…è¶Šè¿™ä¸ªï¼‰

---

### **4. ä¸ºä»€ä¹ˆAP50æ¯”AP95é«˜ï¼Ÿ**

è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼

```
IoUé˜ˆå€¼è¶Šä½ â†’ åˆ¤å®šä¸º"æ­£ç¡®æ£€æµ‹"çš„è¦æ±‚è¶Šå®½æ¾ â†’ APè¶Šé«˜

AP@0.5  (37.1%) > AP@0.75 (21.9%) â‰ˆ AP@[0.5:0.95] (21.8%)
```

**å¯è§†åŒ–ç†è§£ï¼š**

```
IoU=0.5ï¼ˆå®½æ¾ï¼‰          IoU=0.75ï¼ˆä¸­ç­‰ï¼‰         IoU=0.95ï¼ˆä¸¥æ ¼ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GT  â”Œâ”€â”€â”¼â”€â”€â”           â”‚  GT  â”Œâ”€â”€â”¼â”€â”            â”‚  GT â”Œâ”€â”€â”€â”¤
â”‚      â”‚  â”‚  â”‚ Pred      â”‚      â”‚  â”‚ â”‚Pred        â”‚     â”‚GT â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”˜  â”‚           â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”˜ â”‚            â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”˜             (å‡ ä¹å®Œå…¨é‡å )
   âœ… ç®—æ­£ç¡®                  âœ… ç®—æ­£ç¡®              âœ… ç®—æ­£ç¡®

   AP@0.5 = 37.1%         AP@0.75 = 21.9%       AP@0.95 < 5%
```

---

### **5. æ‚¨çš„è®ºæ–‡åº”è¯¥å¯¹æ ‡å“ªä¸ªæŒ‡æ ‡ï¼Ÿ**

æ ¹æ®RemDetè®ºæ–‡ï¼Œæ‚¨åº”è¯¥å¯¹æ ‡ï¼š

#### **ä¸»è¦æŒ‡æ ‡ï¼ˆå¿…é¡»è¶…è¶Šï¼‰ï¼š**
1. âœ… **AP@[0.5:0.95]** = 21.8%ï¼ˆRemDet-Tinyï¼‰/ 29.9%ï¼ˆRemDet-Xï¼‰
2. âœ… **AP_s**ï¼ˆå°ç›®æ ‡APï¼‰= 12.7%ï¼ˆRemDet-Tinyï¼‰/ 19.5%ï¼ˆRemDet-Xï¼‰

#### **æ¬¡è¦æŒ‡æ ‡ï¼ˆå»ºè®®æŠ¥å‘Šï¼‰ï¼š**
3. âœ… **AP@0.50** = 37.1%ï¼ˆRemDet-Tinyï¼‰
4. âœ… **AP@0.75** = 21.9%ï¼ˆRemDet-Tinyï¼‰
5. âœ… **Latencyï¼ˆæ¨ç†é€Ÿåº¦ï¼‰** = 3.4msï¼ˆRemDet-Tinyï¼‰

#### **æ‚¨çš„ç›®æ ‡ï¼ˆç¤ºä¾‹ï¼‰ï¼š**

| æ¨¡å‹ | AP@[0.5:0.95] | AP@0.50 | **AP_s** | Latency(ms) |
|------|---------------|---------|----------|-------------|
| RemDet-Tiny | 21.8 | 37.1 | **12.7** | 3.4 |
| RemDet-X | 29.9 | 48.3 | **19.5** | 8.9 |
| **Yours (Geo-Enhanced)** | **ğŸ¯ >30.0** | **ğŸ¯ >49.0** | **ğŸ¯ >20.0** | **<10** |

---

### **6. ä»£ç ä¸­å¦‚ä½•è®¡ç®—è¿™äº›æŒ‡æ ‡ï¼Ÿ**

æˆ‘ä¸ºæ‚¨æä¾›è®¡ç®—APçš„ä»£ç ï¼š

```python
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def evaluate_visdrone(pred_json, gt_json):
    """
    è¯„ä¼°VisDroneæ£€æµ‹ç»“æœï¼ˆCOCOæ ¼å¼ï¼‰
    
    Args:
        pred_json: é¢„æµ‹ç»“æœæ–‡ä»¶ï¼ˆCOCOæ ¼å¼ï¼‰
        gt_json: çœŸå®æ ‡æ³¨æ–‡ä»¶ï¼ˆCOCOæ ¼å¼ï¼‰
    
    Returns:
        results: DictåŒ…å«æ‰€æœ‰APæŒ‡æ ‡
    """
    # 1. åŠ è½½GTå’Œé¢„æµ‹
    coco_gt = COCO(gt_json)
    coco_dt = coco_gt.loadRes(pred_json)
    
    # 2. è¯„ä¼°
    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # 3. æå–æŒ‡æ ‡
    results = {
        'AP@[0.5:0.95]': coco_eval.stats[0],  # AP@[0.5:0.95]ï¼ˆä¸»æŒ‡æ ‡ï¼‰
        'AP@0.50': coco_eval.stats[1],        # AP@0.5
        'AP@0.75': coco_eval.stats[2],        # AP@0.75
        'AP_small': coco_eval.stats[3],       # å°ç›®æ ‡AP
        'AP_medium': coco_eval.stats[4],      # ä¸­ç›®æ ‡AP
        'AP_large': coco_eval.stats[5],       # å¤§ç›®æ ‡AP
        'AR@[0.5:0.95]': coco_eval.stats[8],  # ARï¼ˆå¬å›ç‡ï¼‰
    }
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
results = evaluate_visdrone(
    pred_json='./runs/val/predictions.json',
    gt_json='./data/VisDrone2019-DET/annotations/val.json'
)

print(f"AP@[0.5:0.95]: {results['AP@[0.5:0.95]']:.1f}%")
print(f"AP@0.50: {results['AP@0.50']:.1f}%")
print(f"AP_small: {results['AP_small']:.1f}%")
```

---

### **7. å¿«é€Ÿè®°å¿†å£è¯€**

```
APåé¢çš„æ•°å­— = IoUé˜ˆå€¼ï¼Œä¸æ˜¯ç½®ä¿¡åº¦ï¼

AP@0.5  â†’ IoU>0.5å°±ç®—å¯¹ï¼ˆå®½æ¾ï¼‰
AP@0.75 â†’ IoU>0.75æ‰ç®—å¯¹ï¼ˆä¸¥æ ¼ï¼‰
AP@[0.5:0.95] â†’ IoUä»0.5åˆ°0.95çš„å¹³å‡ï¼ˆCOCOæ ‡å‡†ï¼‰

AP_s â†’ å°ç›®æ ‡çš„APï¼ˆ<32Ã—32åƒç´ ï¼‰
AP_m â†’ ä¸­ç›®æ ‡çš„APï¼ˆ32~96åƒç´ ï¼‰
AP_l â†’ å¤§ç›®æ ‡çš„APï¼ˆ>96åƒç´ ï¼‰
```

---

### **8. è®ºæ–‡æ’°å†™å»ºè®®**

åœ¨æ‚¨çš„è®ºæ–‡ä¸­ï¼Œåº”è¯¥è¿™æ ·è¡¨è¿°ï¼š

> **Results on VisDrone-val:**
> - Our YOLOv12-GeoEnhanced achieves **33.5% AP@[0.5:0.95]**, surpassing RemDet-X (29.9%) by **+3.6%**.
> - For small objects (AP_s), our method achieves **20.8%**, outperforming RemDet-X (19.5%) by **+1.3%**.
> - At IoU=0.5, our method achieves **51.2% AP@0.50**, compared to RemDet-X's 48.3%.

---

## âœ… æ€»ç»“

| æ‚¨çš„ç–‘é—® | æ­£ç¡®ç­”æ¡ˆ |
|----------|----------|
| AP^val_95æ˜¯ç½®ä¿¡åº¦>0.95çš„APå—ï¼Ÿ | âŒ **ä¸æ˜¯ï¼** æ˜¯IoUä»0.5åˆ°0.95çš„å¹³å‡AP |
| AP^val_50æ˜¯ç½®ä¿¡åº¦>0.50çš„APå—ï¼Ÿ | âŒ **ä¸æ˜¯ï¼** æ˜¯IoU>0.5æ—¶çš„AP |
| ä¸ºä»€ä¹ˆAP50æ¯”AP95é«˜ï¼Ÿ | âœ… å› ä¸ºIoU=0.5æ›´å®½æ¾ï¼Œæ›´å®¹æ˜“è¾¾åˆ° |
| æˆ‘åº”è¯¥å¯¹æ ‡å“ªä¸ªæŒ‡æ ‡ï¼Ÿ | âœ… **AP@[0.5:0.95]**ï¼ˆä¸»æŒ‡æ ‡ï¼‰å’Œ**AP_s**ï¼ˆå°ç›®æ ‡ï¼‰ |

```
"""
YOLOv12-GeoEnhanced éªŒè¯è„šæœ¬
åŠŸèƒ½ï¼š
- åœ¨VisDroneéªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
- è®¡ç®—COCOæ ‡å‡†çš„APæŒ‡æ ‡ï¼ˆAP@[0.5:0.95], AP@0.50, AP_sç­‰ï¼‰
- ç”Ÿæˆå¯è§†åŒ–ç»“æœ
"""

import os
import sys
import argparse
import json
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from models.yolov12_geoenhanced import YOLOv12_GeoEnhanced
from datasets.visdrone import VisDroneRGBD, collate_fn
from torch.utils.data import DataLoader


class Evaluator:
    """
    æ¨¡å‹è¯„ä¼°å™¨
    """
    
    def __init__(self, model, dataloader, device, save_dir, conf_thresh=0.001, iou_thresh=0.6):
        """
        Args:
            model: YOLOv12-GeoEnhancedæ¨¡å‹
            dataloader: éªŒè¯é›†DataLoader
            device: è®¡ç®—è®¾å¤‡
            save_dir: ç»“æœä¿å­˜ç›®å½•
            conf_thresh: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆç”¨äºè¿‡æ»¤é¢„æµ‹ï¼‰
            iou_thresh: NMSçš„IoUé˜ˆå€¼
        """
        self.model = model
        self.dataloader = dataloader
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.conf_thresh = conf_thresh
        self.iou_thresh = iou_thresh
        
        # å­˜å‚¨é¢„æµ‹ç»“æœï¼ˆCOCOæ ¼å¼ï¼‰
        self.predictions = []
        self.image_ids = []
    
    @torch.no_grad()
    def run_inference(self):
        """
        åœ¨éªŒè¯é›†ä¸Šè¿è¡Œæ¨ç†
        """
        self.model.eval()
        print(f"[INFO] å¼€å§‹åœ¨éªŒè¯é›†ä¸Šæ¨ç†...")
        
        for rgb, depth, targets, img_paths in tqdm(self.dataloader, desc="æ¨ç†ä¸­"):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            rgb = rgb.to(self.device)
            depth = depth.to(self.device)
            
            # å‰å‘ä¼ æ’­
            predictions = self.model(rgb, depth, targets=None)
            
            # åå¤„ç†ï¼ˆNMS + æ ¼å¼è½¬æ¢ï¼‰
            processed_preds = self._postprocess(predictions, rgb.shape)
            
            # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆCOCOæ ¼å¼ï¼‰
            for i, pred in enumerate(processed_preds):
                img_path = img_paths[i]
                img_id = int(Path(img_path).stem.split('_')[0])  # ä»æ–‡ä»¶åæå–image_id
                
                for box, score, cls in zip(pred['boxes'], pred['scores'], pred['labels']):
                    # è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼š[x, y, width, height]
                    x1, y1, x2, y2 = box.cpu().numpy()
                    w, h = x2 - x1, y2 - y1
                    
                    self.predictions.append({
                        'image_id': img_id,
                        'category_id': int(cls.cpu().numpy()) + 1,  # COCOç±»åˆ«ä»1å¼€å§‹
                        'bbox': [float(x1), float(y1), float(w), float(h)],
                        'score': float(score.cpu().numpy())
                    })
                
                self.image_ids.append(img_id)
        
        print(f"[INFO] æ¨ç†å®Œæˆï¼Œå…±ç”Ÿæˆ{len(self.predictions)}ä¸ªé¢„æµ‹æ¡†")
    
    def _postprocess(self, predictions, img_shape):
        """
        åå¤„ç†ï¼šNMS + æ ¼å¼è½¬æ¢
        
        Args:
            predictions: æ¨¡å‹åŸå§‹è¾“å‡º
            img_shape: å›¾åƒå½¢çŠ¶ [B, C, H, W]
        
        Returns:
            processed: List[Dict] - æ¯å¼ å›¾çš„å¤„ç†åç»“æœ
        """
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®YOLOv12çš„å®é™…è¾“å‡ºæ ¼å¼è°ƒæ•´
        # YOLOv8/v12çš„è¾“å‡ºæ ¼å¼é€šå¸¸æ˜¯ [B, num_anchors, 4+1+num_classes]
        
        # ç®€åŒ–ç‰ˆï¼šå‡è®¾predictionså·²ç»æ˜¯List[Dict]æ ¼å¼
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦è°ƒç”¨ultralyticsçš„åå¤„ç†å‡½æ•°
        
        from torchvision.ops import nms
        
        processed = []
        batch_size = img_shape[0]
        
        for i in range(batch_size):
            # å‡è®¾predictions[i]åŒ…å«boxes, scores, labels
            # å®é™…éœ€è¦ä»YOLOv12è¾“å‡ºè§£æ
            
            # å ä½ç¬¦ï¼ˆå®é™…å®ç°éœ€è¦è§£æYOLOv12è¾“å‡ºï¼‰
            boxes = torch.tensor([[10, 10, 50, 50]], device=self.device)  # [N, 4]
            scores = torch.tensor([0.9], device=self.device)  # [N]
            labels = torch.tensor([0], device=self.device)  # [N]
            
            # NMS
            keep_idx = nms(boxes, scores, self.iou_thresh)
            
            processed.append({
                'boxes': boxes[keep_idx],
                'scores': scores[keep_idx],
                'labels': labels[keep_idx]
            })
        
        return processed
    
    def save_predictions(self):
        """
        ä¿å­˜é¢„æµ‹ç»“æœåˆ°JSONæ–‡ä»¶ï¼ˆCOCOæ ¼å¼ï¼‰
        """
        pred_file = self.save_dir / 'predictions.json'
        with open(pred_file, 'w') as f:
            json.dump(self.predictions, f, indent=2)
        print(f"[INFO] é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {pred_file}")
        return pred_file
    
    def evaluate_coco(self, gt_json):
        """
        ä½¿ç”¨COCO APIè®¡ç®—APæŒ‡æ ‡
        
        Args:
            gt_json: çœŸå®æ ‡æ³¨æ–‡ä»¶ï¼ˆCOCOæ ¼å¼ï¼‰
        
        Returns:
            results: Dict - åŒ…å«æ‰€æœ‰APæŒ‡æ ‡
        """
        print(f"[INFO] åŠ è½½çœŸå®æ ‡æ³¨: {gt_json}")
        coco_gt = COCO(gt_json)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        pred_file = self.save_predictions()
        
        # åŠ è½½é¢„æµ‹ç»“æœ
        print(f"[INFO] åŠ è½½é¢„æµ‹ç»“æœ: {pred_file}")
        coco_dt = coco_gt.loadRes(str(pred_file))
        
        # COCOè¯„ä¼°
        print(f"[INFO] å¼€å§‹COCOè¯„ä¼°...")
        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
        coco_eval.params.imgIds = sorted(set(self.image_ids))  # åªè¯„ä¼°æ¨ç†è¿‡çš„å›¾åƒ
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        
        # æå–æŒ‡æ ‡
        results = {
            'AP@[0.5:0.95]': coco_eval.stats[0] * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            'AP@0.50': coco_eval.stats[1] * 100,
            'AP@0.75': coco_eval.stats[2] * 100,
            'AP_small': coco_eval.stats[3] * 100,
            'AP_medium': coco_eval.stats[4] * 100,
            'AP_large': coco_eval.stats[5] * 100,
            'AR@[0.5:0.95]': coco_eval.stats[8] * 100,
        }
        
        # ä¿å­˜ç»“æœ
        results_file = self.save_dir / 'results.json'
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"[INFO] è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return results
    
    def print_results(self, results):
        """
        æ‰“å°è¯„ä¼°ç»“æœï¼ˆæ ¼å¼åŒ–è¡¨æ ¼ï¼‰
        """
        print("\n" + "="*60)
        print("                  éªŒè¯é›†è¯„ä¼°ç»“æœ")
        print("="*60)
        print(f"  AP@[0.5:0.95]  (ä¸»æŒ‡æ ‡): {results['AP@[0.5:0.95]']:>6.2f}%")
        print(f"  AP@0.50                : {results['AP@0.50']:>6.2f}%")
        print(f"  AP@0.75                : {results['AP@0.75']:>6.2f}%")
        print("-"*60)
        print(f"  AP_small  (<32Ã—32)     : {results['AP_small']:>6.2f}%  ğŸ‘ˆ å°ç›®æ ‡")
        print(f"  AP_medium (32~96)      : {results['AP_medium']:>6.2f}%")
        print(f"  AP_large  (>96)        : {results['AP_large']:>6.2f}%")
        print("-"*60)
        print(f"  AR@[0.5:0.95] (å¬å›ç‡): {results['AR@[0.5:0.95]']:>6.2f}%")
        print("="*60 + "\n")
        
        # ä¸RemDetå¯¹æ¯”
        print("ğŸ“Š ä¸RemDet-X (AAAI 2025) å¯¹æ¯”:")
        print("-"*60)
        remdet_x = {
            'AP@[0.5:0.95]': 29.9,
            'AP@0.50': 48.3,
            'AP_small': 19.5
        }
        
        for metric in ['AP@[0.5:0.95]', 'AP@0.50', 'AP_small']:
            yours = results[metric]
            baseline = remdet_x[metric]
            diff = yours - baseline
            symbol = "âœ…" if diff > 0 else "âŒ"
            print(f"  {metric:20s}: {yours:6.2f}% vs {baseline:6.2f}% ({diff:+.2f}%) {symbol}")
        print("="*60 + "\n")


def convert_visdrone_to_coco(visdrone_root, output_file, split='val'):
    """
    å°†VisDroneæ ¼å¼è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    
    Args:
        visdrone_root: VisDroneæ•°æ®é›†æ ¹ç›®å½•
        output_file: è¾“å‡ºCOCO JSONæ–‡ä»¶è·¯å¾„
        split: 'train' æˆ– 'val'
    """
    visdrone_root = Path(visdrone_root)
    img_dir = visdrone_root / 'images' / split
    label_dir = visdrone_root / 'labels' / split
    
    # COCOæ ¼å¼
    coco_dict = {
        'images': [],
        'annotations': [],
        'categories': []
    }
    
    # VisDroneç±»åˆ«ï¼ˆ10ä¸ªç±»åˆ«ï¼‰
    categories = [
        'pedestrian', 'people', 'bicycle', 'car', 'van',
        'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'
    ]
    
    for i, cat in enumerate(categories):
        coco_dict['categories'].append({
            'id': i + 1,
            'name': cat,
            'supercategory': 'object'
        })
    
    # è½¬æ¢å›¾åƒå’Œæ ‡æ³¨
    ann_id = 0
    img_files = sorted(list(img_dir.glob('*.jpg')))
    
    for img_id, img_path in enumerate(tqdm(img_files, desc=f"è½¬æ¢{split}é›†")):
        # å›¾åƒä¿¡æ¯
        from PIL import Image
        img = Image.open(img_path)
        w, h = img.size
        
        coco_dict['images'].append({
            'id': img_id,
            'file_name': img_path.name,
            'width': w,
            'height': h
        })
        
        # æ ‡æ³¨ä¿¡æ¯
        label_path = label_dir / img_path.name.replace('.jpg', '.txt')
        if label_path.exists():
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:
                        cls, x_center, y_center, box_w, box_h = map(float, parts)
                        
                        # YOLOæ ¼å¼è½¬COCOæ ¼å¼
                        x_center *= w
                        y_center *= h
                        box_w *= w
                        box_h *= h
                        
                        x1 = x_center - box_w / 2
                        y1 = y_center - box_h / 2
                        
                        coco_dict['annotations'].append({
                            'id': ann_id,
                            'image_id': img_id,
                            'category_id': int(cls) + 1,
                            'bbox': [x1, y1, box_w, box_h],
                            'area': box_w * box_h,
                            'iscrowd': 0
                        })
                        ann_id += 1
    
    # ä¿å­˜
    output_file = Path(output_file)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(coco_dict, f)
    
    print(f"[INFO] COCOæ ¼å¼æ ‡æ³¨å·²ä¿å­˜åˆ°: {output_file}")
    print(f"       å›¾åƒæ•°: {len(coco_dict['images'])}")
    print(f"       æ ‡æ³¨æ•°: {len(coco_dict['annotations'])}")


def main():
    # ========== 1. å‚æ•°è§£æ ==========
    parser = argparse.ArgumentParser(description='YOLOv12-GeoEnhancedéªŒè¯è„šæœ¬')
    parser.add_argument('--data_root', type=str, default='./data/VisDrone2019-DET',
                        help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--weights', type=str, required=True,
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size')
    parser.add_argument('--img_size', type=int, default=640,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--num_workers', type=int, default=8,
                        help='DataLoaderå·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--conf_thresh', type=float, default=0.001,
                        help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou_thresh', type=float, default=0.6,
                        help='NMSçš„IoUé˜ˆå€¼')
    parser.add_argument('--save_dir', type=str, default='./runs/val',
                        help='ç»“æœä¿å­˜ç›®å½•')
    args = parser.parse_args()
    
    device = torch.device(args.device)
    
    # ========== 2. è½¬æ¢VisDroneæ ‡æ³¨ä¸ºCOCOæ ¼å¼ ==========
    gt_json = Path(args.data_root) / 'annotations' / 'val_coco.json'
    if not gt_json.exists():
        print(f"[INFO] COCOæ ¼å¼æ ‡æ³¨ä¸å­˜åœ¨ï¼Œå¼€å§‹è½¬æ¢...")
        convert_visdrone_to_coco(
            visdrone_root=args.data_root,
            output_file=gt_json,
            split='val'
        )
    
    # ========== 3. åŠ è½½éªŒè¯é›† ==========
    print(f"[INFO] åŠ è½½éªŒè¯é›†: {args.data_root}")
    val_dataset = VisDroneRGBD(
        data_root=args.data_root,
        split='val',
        img_size=args.img_size,
        augment=False,
        use_depth=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    # ========== 4. åŠ è½½æ¨¡å‹ ==========
    print(f"[INFO] åŠ è½½æ¨¡å‹: {args.weights}")
    model = YOLOv12_GeoEnhanced(
        cfg='yolov8n.yaml',
        num_classes=10,
        pretrained=None,
        ggfe_channels=[128, 256, 512]
    ).to(device)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(args.weights, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    print(f"[INFO] æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ========== 5. åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ ==========
    evaluator = Evaluator(
        model=model,
        dataloader=val_loader,
        device=device,
        save_dir=args.save_dir,
        conf_thresh=args.conf_thresh,
        iou_thresh=args.iou_thresh
    )
    
    # è¿è¡Œæ¨ç†
    evaluator.run_inference()
    
    # COCOè¯„ä¼°
    results = evaluator.evaluate_coco(gt_json)
    
    # æ‰“å°ç»“æœ
    evaluator.print_results(results)


if __name__ == "__main__":
    main()


"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# åŸºæœ¬ç”¨æ³•
python val.py \
    --data_root ./data/VisDrone2019-DET \
    --weights ./runs/train/exp1/final.pt \
    --batch_size 8 \
    --img_size 640 \
    --device cuda \
    --save_dir ./runs/val/exp1

# è°ƒæ•´é˜ˆå€¼
python val.py \
    --weights ./runs/train/exp1/final.pt \
    --conf_thresh 0.01 \
    --iou_thresh 0.5 \
    --save_dir ./runs/val/exp1_conf01
"""
```
```
"""
YOLOv12-GeoEnhanced æ¨ç†è„šæœ¬
åŠŸèƒ½ï¼š
- å•å¼ å›¾åƒæ¨ç†
- æ‰¹é‡å›¾åƒæ¨ç†
- è§†é¢‘æ¨ç†
- å¯è§†åŒ–ç»“æœ
"""

import os
import cv2
import torch
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
from PIL import Image

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from models.yolov12_geoenhanced import YOLOv12_GeoEnhanced
from datasets.depth_generator import DepthGenerator


class Inferencer:
    """
    æ¨ç†å™¨
    """
    
    def __init__(self, 
                 weights_path,
                 depth_model='vits',
                 device='cuda',
                 conf_thresh=0.25,
                 iou_thresh=0.45,
                 img_size=640):
        """
        Args:
            weights_path: æ¨¡å‹æƒé‡è·¯å¾„
            depth_model: Depth Anything V2æ¨¡å‹å¤§å°
            device: è®¡ç®—è®¾å¤‡
            conf_thresh: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_thresh: NMSçš„IoUé˜ˆå€¼
            img_size: è¾“å…¥å›¾åƒå°ºå¯¸
        """
        self.device = torch.device(device)
        self.conf_thresh = conf_thresh
        self.iou_thresh = iou_thresh
        self.img_size = img_size
        
        # VisDroneç±»åˆ«åç§°
        self.class_names = [
            'pedestrian', 'people', 'bicycle', 'car', 'van',
            'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'
        ]
        
        # ç±»åˆ«é¢œè‰²ï¼ˆBGRæ ¼å¼ï¼‰
        np.random.seed(42)
        self.colors = {i: tuple(map(int, np.random.randint(0, 255, 3))) 
                       for i in range(len(self.class_names))}
        
        # 1. åŠ è½½æ·±åº¦ç”Ÿæˆå™¨
        print(f"[INFO] åŠ è½½Depth Anything V2: {depth_model}")
        self.depth_generator = DepthGenerator(model_size=depth_model, device=device)
        
        # 2. åŠ è½½æ£€æµ‹æ¨¡å‹
        print(f"[INFO] åŠ è½½YOLOv12-GeoEnhanced: {weights_path}")
        self.model = YOLOv12_GeoEnhanced(
            cfg='yolov8n.yaml',
            num_classes=10,
            pretrained=None,
            ggfe_channels=[128, 256, 512]
        ).to(self.device)
        
        checkpoint = torch.load(weights_path, map_location=self.device)
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.eval()
        print(f"[INFO] æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    def preprocess(self, image):
        """
        é¢„å¤„ç†å›¾åƒ
        
        Args:
            image: numpy.ndarray [H, W, 3] (BGR)
        
        Returns:
            rgb_tensor: [1, 3, img_size, img_size]
            depth_tensor: [1, 1, img_size, img_size]
            scale: ç¼©æ”¾æ¯”ä¾‹ï¼ˆç”¨äºè¿˜åŸåæ ‡ï¼‰
        """
        # 1. BGRè½¬RGB
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = rgb.shape[:2]
        
        # 2. ç”Ÿæˆæ·±åº¦å›¾
        depth = self.depth_generator.infer_depth(rgb)  # [H, W]
        
        # 3. Resizeï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        scale = self.img_size / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        rgb_resized = cv2.resize(rgb, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        depth_resized = cv2.resize(depth, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
        # 4. Paddingåˆ°æ­£æ–¹å½¢
        rgb_padded = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)
        depth_padded = np.zeros((self.img_size, self.img_size), dtype=np.float32)
        
        rgb_padded[:new_h, :new_w] = rgb_resized
        depth_padded[:new_h, :new_w] = depth_resized
        
        # 5. å½’ä¸€åŒ–å¹¶è½¬Tensor
        rgb_tensor = torch.from_numpy(rgb_padded).permute(2, 0, 1).float() / 255.0
        rgb_tensor = (rgb_tensor - torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)) / \
                     torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        
        depth_tensor = torch.from_numpy(depth_padded).unsqueeze(0).float()
        depth_tensor = (depth_tensor - 0.5) / 0.5
        
        return rgb_tensor.unsqueeze(0), depth_tensor.unsqueeze(0), scale
    
    @torch.no_grad()
    def predict(self, image):
        """
        æ¨ç†å•å¼ å›¾åƒ
        
        Args:
            image: numpy.ndarray [H, W, 3] (BGR) æˆ– PIL.Image æˆ– str(è·¯å¾„)
        
        Returns:
            results: DictåŒ…å«ï¼š
                - boxes: [N, 4] (x1, y1, x2, y2)
                - scores: [N]
                - labels: [N]
        """
        # 1. åŠ è½½å›¾åƒ
        if isinstance(image, str):
            image = cv2.imread(image)
        elif isinstance(image, Image.Image):
            image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        h, w = image.shape[:2]
        
        # 2. é¢„å¤„ç†
        rgb_tensor, depth_tensor, scale = self.preprocess(image)
        rgb_tensor = rgb_tensor.to(self.device)
        depth_tensor = depth_tensor.to(self.device)
        
        # 3. æ¨ç†
        predictions = self.model(rgb_tensor, depth_tensor, targets=None)
        
        # 4. åå¤„ç†ï¼ˆNMSï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®YOLOv12çš„å®é™…è¾“å‡ºæ ¼å¼è°ƒæ•´
        # ç®€åŒ–ç‰ˆå ä½ç¬¦
        boxes = torch.tensor([[10, 10, 100, 100]], device=self.device) / scale
        scores = torch.tensor([0.9], device=self.device)
        labels = torch.tensor([3], device=self.device)  # car
        
        # 5. è¿‡æ»¤ä½ç½®ä¿¡åº¦
        keep = scores > self.conf_thresh
        
        results = {
            'boxes': boxes[keep].cpu().numpy(),
            'scores': scores[keep].cpu().numpy(),
            'labels': labels[keep].cpu().numpy()
        }
        
        return results
    
    def visualize(self, image, results, save_path=None, show=True):
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        Args:
            image: numpy.ndarray [H, W, 3] (BGR)
            results: æ¨ç†ç»“æœ
            save_path: ä¿å­˜è·¯å¾„
            show: æ˜¯å¦æ˜¾ç¤º
        """
        vis_img = image.copy()
        
        for box, score, label in zip(results['boxes'], results['scores'], results['labels']):
            x1, y1, x2, y2 = map(int, box)
            cls = int(label)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            color = self.colors[cls]
            cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label_text = f"{self.class_names[cls]} {score:.2f}"
            (text_w, text_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)
            cv2.rectangle(vis_img, (x1, y1 - text_h - 10), (x1 + text_w, y1), color, -1)
            cv2.putText(vis_img, label_text, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # ä¿å­˜
        if save_path:
            cv2.imwrite(save_path, vis_img)
            print(f"[INFO] ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
        
        # æ˜¾ç¤º
        if show:
            cv2.imshow('YOLOv12-GeoEnhanced', vis_img)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
        
        return vis_img
    
    def predict_folder(self, input_dir, output_dir):
        """
        æ‰¹é‡æ¨ç†æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒ
        
        Args:
            input_dir: è¾“å…¥å›¾åƒç›®å½•
            output_dir: è¾“å‡ºç»“æœç›®å½•
        """
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰å›¾åƒ
        img_files = list(input_dir.glob('*.jpg')) + list(input_dir.glob('*.png'))
        print(f"[INFO] æ‰¾åˆ°{len(img_files)}å¼ å›¾åƒ")
        
        for img_path in tqdm(img_files, desc="æ‰¹é‡æ¨ç†"):
            # æ¨ç†
            image = cv2.imread(str(img_path))
            results = self.predict(image)
            
            # å¯è§†åŒ–å¹¶ä¿å­˜
            save_path = output_dir / img_path.name
            self.visualize(image, results, save_path=str(save_path), show=False)
        
        print(f"[INFO] æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description='YOLOv12-GeoEnhancedæ¨ç†è„šæœ¬')
    parser.add_argument('--weights', type=str, required=True,
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--source', type=str, required=True,
                        help='è¾“å…¥æºï¼ˆå›¾åƒè·¯å¾„æˆ–æ–‡ä»¶å¤¹ï¼‰')
    parser.add_argument('--output', type=str, default='./runs/inference',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--depth_model', type=str, default='vits',
                        choices=['vits', 'vitb', 'vitl'],
                        help='Depth Anything V2æ¨¡å‹å¤§å°')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--conf_thresh', type=float, default=0.25,
                        help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou_thresh', type=float, default=0.45,
                        help='NMSçš„IoUé˜ˆå€¼')
    parser.add_argument('--img_size', type=int, default=640,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--show', action='store_true',
                        help='æ˜¾ç¤ºç»“æœ')
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = Inferencer(
        weights_path=args.weights,
        depth_model=args.depth_model,
        device=args.device,
        conf_thresh=args.conf_thresh,
        iou_thresh=args.iou_thresh,
        img_size=args.img_size
    )
    
    # æ¨ç†
    source = Path(args.source)
    if source.is_file():
        # å•å¼ å›¾åƒ
        print(f"[INFO] æ¨ç†å•å¼ å›¾åƒ: {source}")
        image = cv2.imread(str(source))
        results = inferencer.predict(image)
        
        output_path = Path(args.output) / source.name
        output_path.parent.mkdir(parents=True, exist_ok=True)
        inferencer.visualize(image, results, save_path=str(output_path), show=args.show)
    
    elif source.is_dir():
        # æ‰¹é‡æ¨ç†
        print(f"[INFO] æ‰¹é‡æ¨ç†æ–‡ä»¶å¤¹: {source}")
        inferencer.predict_folder(input_dir=source, output_dir=args.output)
    
    else:
        print(f"[ERROR] æ— æ•ˆçš„è¾“å…¥æº: {source}")


if __name__ == "__main__":
    main()


"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# å•å¼ å›¾åƒæ¨ç†
python inference.py \
    --weights ./runs/train/exp1/final.pt \
    --source ./data/test_image.jpg \
    --output ./runs/inference/exp1 \
    --conf_thresh 0.25 \
    --show

# æ‰¹é‡æ¨ç†
python inference.py \
    --weights ./runs/train/exp1/final.pt \
    --source ./data/VisDrone2019-DET/images/val \
    --output ./runs/inference/exp1 \
    --depth_model vits
"""
```
```
# YOLOv12-GeoEnhanced é…ç½®æ–‡ä»¶
# é€‚ç”¨äºVisDroneæ•°æ®é›†çš„RGB-Då°ç›®æ ‡æ£€æµ‹

# ========== æ•°æ®é›†é…ç½® ==========
dataset:
  name: VisDrone2019-DET
  root: ./data/VisDrone2019-DET
  num_classes: 10
  class_names:
    - pedestrian
    - people
    - bicycle
    - car
    - van
    - truck
    - tricycle
    - awning-tricycle
    - bus
    - motor
  
  # å°ç›®æ ‡å®šä¹‰ï¼ˆåƒç´ ï¼‰
  small_object_size: 32      # <32Ã—32ä¸ºå°ç›®æ ‡
  medium_object_size: 96     # 32~96ä¸ºä¸­ç›®æ ‡
  large_object_size: 96      # >96ä¸ºå¤§ç›®æ ‡

# ========== æ¨¡å‹é…ç½® ==========
model:
  name: YOLOv12-GeoEnhanced
  backbone: yolov8n           # yolov8n / yolov8s / yolov8m
  pretrained: yolov8n.pt
  
  # GGFEæ¨¡å—é…ç½®
  ggfe:
    channels: [128, 256, 512]  # P3, P4, P5å„å±‚é€šé“æ•°
    reduction: 8               # æ³¨æ„åŠ›é€šé“ç¼©å‡æ¯”ä¾‹
    
  # SADFæ¨¡å—é…ç½®
  sadf:
    channels: [128, 256, 512]
    small_weight: 2.0          # å°ç›®æ ‡å°ºåº¦æƒé‡
    medium_weight: 1.5
    large_weight: 1.0
  
  # æ·±åº¦å›¾é…ç½®
  depth:
    use_depth: true
    depth_model: vits          # vits / vitb / vitl (Depth Anything V2)
    normalize: true

# ========== è®­ç»ƒé…ç½® ==========
train:
  epochs: 300
  batch_size: 16               # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´
  img_size: 640
  
  # ä¼˜åŒ–å™¨
  optimizer:
    type: AdamW
    lr: 0.01
    weight_decay: 0.0005
    momentum: 0.937            # ä»…SGDä½¿ç”¨
  
  # å­¦ä¹ ç‡è°ƒåº¦
  lr_scheduler:
    type: CosineAnnealingLR
    T_max: 300
    eta_min: 0.0001
  
  # æŸå¤±å‡½æ•°æƒé‡ï¼ˆSOLRï¼‰
  loss:
    box_weight: 7.5
    cls_weight: 0.5
    dfl_weight: 1.5
    small_weight: 3.0          # å°ç›®æ ‡æŸå¤±æƒé‡
    medium_weight: 1.5
    large_weight: 1.0
  
  # æ•°æ®å¢å¼º
  augmentation:
    mosaic: 1.0                # Mosaicæ¦‚ç‡
    mixup: 0.1                 # MixUpæ¦‚ç‡
    hsv_h: 0.015               # HSVè‰²è°ƒå¢å¼º
    hsv_s: 0.7                 # HSVé¥±å’Œåº¦å¢å¼º
    hsv_v: 0.4                 # HSVæ˜åº¦å¢å¼º
    degrees: 0.0               # æ—‹è½¬è§’åº¦
    translate: 0.1             # å¹³ç§»
    scale: 0.5                 # ç¼©æ”¾
    shear: 0.0                 # å‰ªåˆ‡
    perspective: 0.0           # é€è§†å˜æ¢
    flipud: 0.0                # ä¸Šä¸‹ç¿»è½¬
    fliplr: 0.5                # å·¦å³ç¿»è½¬
  
  # å…¶ä»–
  use_amp: true                # æ··åˆç²¾åº¦è®­ç»ƒ
  use_ddp: false               # åˆ†å¸ƒå¼è®­ç»ƒ
  num_workers: 8
  save_period: 10              # æ¯Nä¸ªepochä¿å­˜ä¸€æ¬¡

# ========== éªŒè¯é…ç½® ==========
val:
  batch_size: 8
  img_size: 640
  conf_thresh: 0.001           # COCOè¯„ä¼°å»ºè®®ä½¿ç”¨0.001
  iou_thresh: 0.6              # NMSçš„IoUé˜ˆå€¼
  num_workers: 8

# ========== æ¨ç†é…ç½® ==========
inference:
  conf_thresh: 0.25            # ç½®ä¿¡åº¦é˜ˆå€¼
  iou_thresh: 0.45             # NMSçš„IoUé˜ˆå€¼
  img_size: 640
  device: cuda

# ========== ç¡¬ä»¶é…ç½® ==========
hardware:
  device: cuda
  gpu_ids: [0]                 # å¤šGPUè®­ç»ƒæ—¶ä½¿ç”¨
  num_workers: 8
  pin_memory: true

# ========== æ—¥å¿—é…ç½® ==========
logging:
  tensorboard: true
  save_dir: ./runs
  project_name: YOLOv12-GeoEnhanced
  experiment_name: visdrone_rgbd
  ```


