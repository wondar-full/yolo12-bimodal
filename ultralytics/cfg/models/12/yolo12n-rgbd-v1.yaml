# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLO12-N RGB-D object detection model (Dual-Modal: RGB + Depth)
# Created: 2025-11-04 for yoloDepth project - NANO VERSION
# Target: Exceed RemDet (AAAI2025) on UAV small object detection
# Optimized for: Fast training, low resource consumption

# ================================================================================================
# Model Configuration
# ================================================================================================
nc: 10 # Number of classes (VisDrone: 10 classes)
# 0=pedestrian, 1=people, 2=bicycle, 3=car, 4=van, 5=truck, 6=tricycle, 7=awning-tricycle, 8=bus, 9=motor

# Model compound scaling constants
# Using 'n' (nano) scale for fast experimentation and lightweight deployment
scales:
  # [depth, width, max_channels]
  n: [0.50, 0.25, 1024] # nano - **fastest, most lightweight**
  s: [0.50, 0.50, 1024] # small
  m: [0.50, 1.00, 512] # medium
  l: [1.00, 1.00, 512] # large
  x: [1.00, 1.50, 512] # extra-large

# ================================================================================================
# Backbone (Feature Extractor)
# ================================================================================================
# Key modifications for RGB-D:
# 1. Layer 0: RGBDStem replaces Conv (dual-branch early fusion)
# 2. Input channels: 4 (RGB=3 + Depth=1)
# 3. RGBDStem outputs 64 channels for Nano (32*2 from dual branches)
#    Note: Reduced from Small's 128 to 64 for efficiency
# 4. Remaining layers use Nano scaling (width=0.25, depth=0.50)

backbone:
  # [from, repeats, module, args]

  # Layer 0: RGB-D Dual-Branch Stem (P1/2 - First Downsampling)
  # Args: [c1, c2, k, s, depth_channels, c_mid, fusion, reduction, act]
  # - c1=4: Input channels (RGB=3 + Depth=1)
  # - c2=64: Output channels (must be even, 64 for Nano vs 128 for Small)
  # - k=3: Kernel size
  # - s=2: Stride (downsample by 2)
  # - depth_channels=1: Number of depth channels
  # - c_mid=32: Mid-layer channels per branch (32 RGB + 32 Depth â†’ 64 total)
  # - fusion="gated_add": Adaptive gating fusion (RemDet-inspired)
  # - reduction=16: Channel reduction ratio for gating network
  # - act=True: Use default SiLU activation
  - [-1, 1, RGBDStem, [4, 64, 3, 2, 1, 32, "gated_add", 16, True]] # 0-P1/2 (RGB-Då…¥å£)

  # Layer 1: Standard Conv (P2/4 - Second Downsampling)
  # Input: 64 channels from RGBDStem output (Nano scale)
  - [-1, 1, Conv, [64, 3, 2]] # 1-P2/4

  # Layers 2-8: Standard YOLOv12 backbone (MUST align with yolo12.yaml for A2C2f compatibility)
  # CRITICAL: A2C2f requires channel dimension to be multiple of 32
  # ä½¿ç”¨yolo12.yamlçš„é€šé“æ•°: 256â†’64, 512â†’128, 1024â†’256 (å…¨æ˜¯32çš„å€æ•° âœ…)
  - [-1, 2, C3k2, [256, False, 0.25]] # 2 (ç¼©æ”¾å64)
  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 (ç¼©æ”¾å64)
  - [-1, 2, C3k2, [512, False, 0.25]] # 4 (ç¼©æ”¾å128)
  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 (ç¼©æ”¾å128)
  - [-1, 4, A2C2f, [512, True, 4]] # 6 (ç¼©æ”¾å128, âœ… 128%32==0)
  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 (ç¼©æ”¾å256)
  - [-1, 4, A2C2f, [1024, True, 1]] # 8 (ç¼©æ”¾å256, âœ… 256%32==0)

# ================================================================================================
# Head (Detection Neck + Prediction)
# ================================================================================================
# Unchanged from yolo12.yaml - uses standard multi-scale feature pyramid
# Future work: Consider adding RGBDMidFusion at P3/P4/P5 entry points (Phase 3)

head:
  # Upsample path (Top-Down)
  # MUSTä½¿ç”¨yolo12.yamlçš„é€šé“æ•°ï¼Œç»è¿‡ç¼©æ”¾åä¿è¯A2C2fçš„è¾“å…¥æ˜¯32å€æ•°
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]] # 9
  - [[-1, 6], 1, Concat, [1]] # 10 - cat backbone P4
  - [-1, 2, A2C2f, [512, False, -1]] # 11 (ç¼©æ”¾å128, âœ…)

  - [-1, 1, nn.Upsample, [None, 2, "nearest"]] # 12
  - [[-1, 4], 1, Concat, [1]] # 13 - cat backbone P3
  - [-1, 2, A2C2f, [256, False, -1]] # 14 (ç¼©æ”¾å64, âœ…)

  # Downsample path (Bottom-Up)
  - [-1, 1, Conv, [256, 3, 2]] # 15 (ç¼©æ”¾å64)
  - [[-1, 11], 1, Concat, [1]] # 16 - cat head P4
  - [-1, 2, A2C2f, [512, False, -1]] # 17 (ç¼©æ”¾å128, âœ…)

  - [-1, 1, Conv, [512, 3, 2]] # 18 (ç¼©æ”¾å128)
  - [[-1, 8], 1, Concat, [1]] # 19 - cat head P5
  - [-1, 2, C3k2, [1024, True]] # 20 (P5/32-large, ç¼©æ”¾å256)

  # Detection heads (P3, P4, P5)
  - [[14, 17, 20], 1, Detect, [nc]] # 21 - Detect(P3, P4, P5)

# ================================================================================================
# Training Hyperparameters (RemDet-Aligned)
# ================================================================================================
# Reference: RemDet (AAAI2025) - https://arxiv.org/abs/2408.xxxxx
# These hyperparameters should be set in train_depth.py, not in YAML

# Recommended settings for Nano (lighter than Small):
# - optimizer: SGD (momentum=0.937, weight_decay=0.0005)
# - lr_scheduler: CosineAnnealing
# - initial_lr: 0.01 (same as Small)
# - warmup_epochs: 3
# - total_epochs: 300
# - batch_size: 16 (or 32 if GPU allows, Nano is lighter)
# - input_size: 640Ã—640
# - mosaic: 1.0
# - mixup: 0.15
# - hsv_h: 0.015
# - hsv_s: 0.7
# - hsv_v: 0.4
# - translate: 0.1
# - scale: 0.9
# - fliplr: 0.5
# - conf_threshold: 0.001
# - iou_threshold: 0.6
# - max_det: 300

# ================================================================================================
# Model Summary (Expected - yolo12n-rgbd-v1)
# ================================================================================================
# - Parameters: ~3.2M (+0.15M from RGBDStem vs standard yolo12n)
# - FLOPs: ~6.5G (+0.8G from geometry prior generation)
# - Input: [B, 4, 640, 640] (RGB + Depth)
# - Output: [B, num_anchors, 4+nc] where num_anchors â‰ˆ 8400 (P3+P4+P5)
# - Inference Speed (RTX 4090): ~120-140 FPS (2-2.5x faster than Small)
# - Training Speed: ~30-40 hours for 300 epochs (vs Small: 40-50 hours)
#
# IMPORTANT: Channel Numbers
#   - å¿…é¡»ä¸yolo12.yamlä¿æŒä¸€è‡´ï¼ˆ256/512/1024ï¼‰ï¼Œç»è¿‡width=0.25ç¼©æ”¾åä¸º64/128/256
#   - A2C2fæ¨¡å—è¦æ±‚è¾“å…¥é€šé“å¿…é¡»æ˜¯32çš„å€æ•°ï¼Œå¦åˆ™ä¼šæŠ¥é”™
#   - ä¸èƒ½ç›´æ¥ä½¿ç”¨64/128/256ä½œä¸ºYAMLé…ç½®å€¼ï¼Œå¦åˆ™ç¼©æ”¾åä¼šå˜æˆ16/32/64ï¼ˆå¤ªå°ï¼‰

# ================================================================================================
# Performance Expectations vs Small
# ================================================================================================
# Nano vs Small trade-offs:
# âœ… Advantages:
#   - 3.5x faster training (30-40h vs 40-50h)
#   - 3.5x less parameters (3.2M vs 11.5M)
#   - 3.7x faster inference (130 FPS vs 55 FPS)
#   - Lower memory usage (can use batch_size=32)
# âš ï¸ Disadvantages:
#   - Slightly lower mAP (-1 to -2 points typically)
#   - Less capacity for complex scenes
#
# Expected Performance (yolo12n-rgbd-v1 on VisDrone+UAVDT):
#   - Overall mAP@0.5: 40-42% (vs Small: 42-44%)
#   - Small mAP: 14-16% (vs Small: 15-17%)
#   - Medium mAP: 45-47% (vs Small: 46-48%)
#   - Large mAP: 40-42% (vs Small: 41-43%)
#
# Recommendation: Start with Nano for fast iteration, upgrade to Small if needed

# ================================================================================================
# Version History
# ================================================================================================
# v1.0 (2025-11-04): Initial Nano RGB-D version
#   - Derived from yolo12s-rgbd-v1.yaml
#   - Scaled down to Nano (width=0.25)
#   - RGBDStem: 64 channels (vs Small: 128)
#   - Target: Fast experimentation and validation ofè”åˆè®­ç»ƒ

# ================================================================================================
# Usage Examples
# ================================================================================================
# 1. Training (Recommended for exp_joint_v14):
#    ```bash
#    CUDA_VISIBLE_DEVICES=7 python train_depth.py \
#        --model ultralytics/cfg/models/12/yolo12n-rgbd-v1.yaml \
#        --data data/visdrone_uavdt_joint.yaml \
#        --weights yolo12n.pt \
#        --epochs 300 \
#        --batch 16 \
#        --name exp_joint_v14_nano_fixed \
#        --device 7
#    ```
#
# 2. Validation:
#    ```bash
#    python val_depth.py \
#        --weights runs/train/exp_joint_v14_nano_fixed/weights/best.pt \
#        --data data/visdrone_uavdt_joint.yaml
#    ```
#
# 3. Inference:
#    ```bash
#    from ultralytics import YOLO
#    model = YOLO('runs/train/exp_joint_v14_nano_fixed/weights/best.pt')
#    results = model.predict(source='rgb_image.jpg', depth='depth_image.png')
#    ```

# ================================================================================================
# Notes & Limitations
# ================================================================================================
# 1. **Weight Loading from yolo12n.pt**:
#    - yolo12n.pt has 3-channel input, this model has 4-channel (RGB+D)
#    - First layer (RGBDStem) will NOT load weights (expected)
#    - Remaining layers SHOULD load successfully (backbone/head compatible)
#    - Check training log for "Transferred XXX/YYY items" message
#    - Expected transfer rate: 85-95% (only first layer skipped)
#
# 2. **Input Requirements**:
#    - RGB and Depth must have same dimensions (H, W)
#    - Depth format: Single-channel grayscale (8-bit or 16-bit PNG)
#    - Depth preprocessing: Automatic (medianâ†’gaussianâ†’confidence weighting)
#
# 3. **Compatibility**:
#    - Requires YOLORGBDDataset for data loading
#    - Requires RGBDStem, DepthGatedFusion, GeometryPriorGenerator modules
#    - Compatible with Ultralytics v8.3.155+
#
# 4. **Performance Expectations**:
#    - Baseline (RGB-only yolo12n): ~37% mAP@0.5, ~11% mAP_small (VisDrone)
#    - Target (RGB-D Nano v1.0): ~40% mAP@0.5, ~14% mAP_small (+3pt improvement)
#    - RemDet-Tiny benchmark: ~38.9% mAP@0.5, ~12.7% mAP_small (æˆ‘ä»¬çš„ç›®æ ‡)
#
# 5. **Known Issues**:
#    - First training may be slower due to geometry prior computation
#    - Depth quality directly impacts performance (use Depth Anything V2 for best results)
#    - Small batch sizes (<8) may cause training instability
#
# 6. **Debugging Tips**:
#    - Check depth alignment: Ensure RGB and Depth files have matching filenames
#    - Monitor RGBDStem statistics: last_geo_quality (should be >0.5), last_gate_mean (should be 0.3-0.7)
#    - Visualize depth preprocessing: Save intermediate depth maps to verify quality
#    - Compare with exp_joint_v13: mAP should be MUCH higher (40% vs 22%)

# ================================================================================================
# Why Nano is Better for Debugging exp_joint_v13 Failure
# ================================================================================================
# exp_joint_v13 used yolo12s-rgbd-v1.yaml + yolo12n.pt (MISMATCH!)
#
# Using yolo12n-rgbd-v1.yaml + yolo12n.pt fixes this:
# âœ… Architecture matches pretrained weights (both Nano)
# âœ… Only first layer (RGBDStem) needs random initialization
# âœ… Remaining ~95% of weights can be transferred
# âœ… Training starts from good initialization â†’ converges faster
# âœ… Expected mAP after 300 epochs: 40-42% (vs v13's 22%)
#
# If Nano still underperforms (<35% mAP), then we know it's a data/label issue.
# If Nano performs well (40%+), then upgrade to Small for extra 1-2 points.
