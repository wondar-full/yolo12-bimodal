# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# YOLO12-S RGB-D v2.1 Joint Training (VisDrone + UAVDT)
# Created: 2025-11-16 for RemDet alignment
# Goal: Align with RemDet paper's training protocol - VisDrone+UAVDT from scratch
# Target: Surpass RemDet-X 45.2% mAP@0.5 with RGB-D multi-modal fusion

# ================================================================================================
# RemDet Alignment Strategy
# ================================================================================================
# ğŸ“Œ RemDet Training Protocol (from paper "Implementation Details"):
#   - Datasets: VisDrone (6,471) + UAVDT (23,258) = 29,729 training images
#   - Method: Trained from scratch (NO COCO pretraining)
#   - Epochs: 300
#   - Optimizer: SGD (lr=0.01, momentum=0.937, weight_decay=0.0005)
#   - Data augmentation: Mosaic=1.0, MixUp=0.15
#   - Evaluation: VisDrone val set only (548 images)
#
# ğŸ“Œ Our RGB-D Enhancement:
#   - Same training data (VisDrone + UAVDT)
#   - Same epochs (300)
#   - Same optimizer (SGD with paper hyperparameters)
#   - Additional modality: Depth maps for geometric priors
#   - Multi-scale fusion: RGBDStem + RGBDMidFusion @ P3/P4/P5
#
# ğŸ“Œ Fair Comparison:
#   - RemDet-X: 45.2% mAP@0.5 (RGB-only, VisDrone val)
#   - Our target: > 45.2% mAP@0.5 (RGB-D, same val set)
#   - Improvement source: Depth geometry priors, not more data
# ================================================================================================

# ================================================================================================
# v2.1 Multi-Scale RGB-D Fusion Architecture
# ================================================================================================
# Key Improvements:
# 1. âœ… RGBDStem: Early fusion with gated attention (Layer 0)
# 2. âœ… RGBDMidFusion @ P3/P4/P5: Re-inject depth at detection scales
# 3. âœ… Depth skip connections: Preserve depth from Layer 0 to all scales
# 4. âœ… Cross-modal attention: Adaptive fusion weights (learnable)
#
# Performance (VisDrone-only baseline):
# - v2.1 (VisDrone only): 43.51% mAP@0.5 @ 244 epochs
# - RGB-only: 40.44% mAP@0.5 @ 238 epochs
# - Gain: +3.07% from RGB-D fusion
#
# Expected (VisDrone + UAVDT joint training):
# - RemDet-X: 45.2% mAP@0.5 (RGB-only)
# - Our target: 46-47% mAP@0.5 (RGB-D with 4.6x training data)
# ================================================================================================

# ================================================================================================
# Model Configuration
# ================================================================================================
nc: 10 # Number of classes (VisDrone 10 classes, UAVDT filtered to match)

# UAVDT Class Filtering:
#   UAVDT has 3 classes: car(0), truck(1), bus(2)
#   Mapped to VisDrone IDs: carâ†’3, truckâ†’5, busâ†’8
#   Other VisDrone classes (pedestrian, van, etc.) come from VisDrone dataset

# Model compound scaling constants
scales:
  # [depth, width, max_channels]
  n: [0.50, 0.25, 1024] # nano
  s: [0.50, 0.50, 1024] # small - **RGB-D v2.1 joint training** âœ…
  m: [0.50, 1.00, 512] # medium
  l: [1.00, 1.00, 512] # large
  x: [1.00, 1.50, 512] # extra-large

# ================================================================================================
# Backbone (Feature Extractor) - Identical to v2.1
# ================================================================================================
# Architecture unchanged from yolo12s-rgbd-v2.1.yaml
# All RGBDStem and RGBDMidFusion modules preserved
# See yolo12s-rgbd-v2.1.yaml for detailed architecture explanation

backbone:
  # [from, repeats, module, args]

  # ============ P1/2 Stage - Early Fusion ============
  - [-1, 1, RGBDStem, [4, 128, 3, 2, 1, 64, "gated_add", 16, True]] # 0-P1/2

  # ============ P2/4 Stage ============
  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4
  - [-1, 2, C3k2, [256, False, 0.25]] # 2

  # ============ P3/8 Stage - First Depth Re-injection ============
  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
  - [-1, 2, C3k2, [512, False, 0.25]] # 4-P3 features
  - [[4, 0], 1, RGBDMidFusion, [512, 64]] # 5-P3 depth fusion âœ…

  # ============ P4/16 Stage - Second Depth Re-injection ============
  - [-1, 1, Conv, [512, 3, 2]] # 6-P4/16
  - [-1, 4, A2C2f, [512, True, 4]] # 7-P4 features
  - [[7, 0], 1, RGBDMidFusion, [512, 64]] # 8-P4 depth fusion âœ…

  # ============ P5/32 Stage - Third Depth Re-injection ============
  - [-1, 1, Conv, [1024, 3, 2]] # 9-P5/32
  - [-1, 4, A2C2f, [1024, True, 1]] # 10-P5 features
  - [[10, 0], 1, RGBDMidFusion, [1024, 64]] # 11-P5 depth fusion âœ…

# ================================================================================================
# Head (Detection) - Identical to v2.1
# ================================================================================================
head:
  # ============ Upsample Path ============
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]] # 12
  - [[-1, 8], 1, Concat, [1]] # 13
  - [-1, 2, A2C2f, [512, False, -1]] # 14

  - [-1, 1, nn.Upsample, [None, 2, "nearest"]] # 15
  - [[-1, 5], 1, Concat, [1]] # 16
  - [-1, 2, A2C2f, [256, False, -1]] # 17 (P3/8-small)

  # ============ Downsample Path ============
  - [-1, 1, Conv, [256, 3, 2]] # 18
  - [[-1, 14], 1, Concat, [1]] # 19
  - [-1, 2, A2C2f, [512, False, -1]] # 20 (P4/16-medium)

  - [-1, 1, Conv, [512, 3, 2]] # 21
  - [[-1, 11], 1, Concat, [1]] # 22
  - [-1, 2, C3k2, [1024, True]] # 23 (P5/32-large)

  # ============ Detection Heads ============
  - [[17, 20, 23], 1, Detect, [nc]] # 24-Detect(P3, P4, P5)

# ================================================================================================
# Training Configuration (RemDet-Aligned)
# ================================================================================================
# Use with train_uav_joint.py:
#
# python train_uav_joint.py \
#     --model ultralytics/cfg/models/12/yolo12s-rgbd-v2.1-joint.yaml \
#     --data data/uav-joint-rgbd.yaml \
#     --epochs 300 \
#     --batch 16 \
#     --imgsz 640 \
#     --device 0 \
#     --workers 8 \
#     --name rgbd_v2.1_joint_300ep \
#     --optimizer SGD \
#     --lr0 0.01 \
#     --momentum 0.937 \
#     --weight_decay 0.0005 \
#     --warmup_epochs 3 \
#     --cos_lr True \
#     --mosaic 1.0 \
#     --mixup 0.15 \
#     --close_mosaic 10 \
#     --amp True \
#     --pretrained "" \
#     --patience 100 \
#     --save_period 50

# ================================================================================================
# Expected Performance (300 epochs, VisDrone+UAVDT)
# ================================================================================================
# | Method          | Dataset       | mAP@0.5 | mAP@0.5:0.95 | Params | FLOPs  |
# |-----------------|---------------|---------|--------------|--------|--------|
# | RemDet-X        | VisDrone+UAVDT| 45.2%   | ~27%         | 9.8M   | 14.8G  |
# | RGB-only v2.1   | VisDrone only | 40.4%   | 24.1%        | 11.1M  | 45.6G  |
# | RGB-D v2.1      | VisDrone only | 43.5%   | 26.3%        | 11.3M  | 45.8G  |
# | RGB-D v2.1 Joint| VisDrone+UAVDT| **46-47%** | **28-29%** | 11.3M  | 45.8G  |
#
# Key Metrics to Monitor:
# - mAP@0.5: Primary metric (must > 45.2%)
# - mAP_small: Small object detection (UAV scenario critical)
# - Precision/Recall balance: Should be within 5% gap
# - Training stability: No NaN/Inf, smooth loss curves
# - Fusion weights: Should converge to 0.3-0.5 range

# ================================================================================================
# Multi-Dataset Training Notes
# ================================================================================================
# 1. Dataset Sampling:
#    - VisDrone: 6,471 images (21.8%)
#    - UAVDT: 23,258 images (78.2%)
#    - Default: Proportional sampling (UAVDT sampled 3.6x more frequently)
#    - Alternative: Balanced sampling (each dataset 50% probability)
#
# 2. Class Distribution:
#    - VisDrone: 10 classes (full coverage)
#    - UAVDT: 3 classes (car, truck, bus) mapped to VisDrone IDs
#    - Class imbalance: Some classes (pedestrian, van) only from VisDrone
#    - Mitigation: Class-weighted loss (optional)
#
# 3. Depth Map Quality:
#    - VisDrone: Depth generated by [your depth estimation method]
#    - UAVDT: Depth generated by [same method]
#    - Quality check: Use check_depth_quality.py before training
#    - Fallback: If depth missing, use zeros (graceful degradation)
#
# 4. Validation Protocol:
#    - Use ONLY VisDrone val set (548 images) - aligns with RemDet
#    - UAVDT val set can be used for additional analysis (not primary metric)
#    - Report mAP@0.5, mAP@0.5:0.95, mAP_small/medium/large

# ================================================================================================
# Troubleshooting Multi-Dataset Training
# ================================================================================================
# Issue 1: Class distribution imbalance
#   - VisDrone classes underrepresented due to fewer images
#   - Solution: Use dataset_weights in YAML (e.g., VisDrone=2.0, UAVDT=1.0)
#
# Issue 2: UAVDT images have different aspect ratios
#   - May cause bbox annotation issues
#   - Solution: Verify YOLO format conversion (check labels)
#
# Issue 3: Depth map file not found for UAVDT
#   - UAVDT depth generation incomplete
#   - Solution: Run depth generation script or use zero-depth fallback
#
# Issue 4: Training converges slower than VisDrone-only
#   - More diverse data requires longer training
#   - Solution: Increase epochs from 300 to 400 if needed
#
# Issue 5: Validation mAP drops after adding UAVDT
#   - Model overfits to UAVDT (more training data)
#   - Solution: Increase VisDrone dataset weight or use balanced sampling

# ================================================================================================
# Ablation Experiments
# ================================================================================================
# To isolate RGB-D improvement from dataset scaling:
#
# 1. RGB-only + Joint Training:
#    - Same VisDrone+UAVDT data, no depth
#    - Expected: 44-45% mAP@0.5 (dataset scaling only)
#
# 2. RGB-D + VisDrone Only:
#    - Already done: 43.5% @ 244 epochs
#
# 3. RGB-D + Joint Training (this config):
#    - Expected: 46-47% mAP@0.5
#    - Gain breakdown: +3% (RGB-D) + 1.5-2% (dataset scaling)
#
# 4. Different Fusion Strategies:
#    - Early fusion only (RGBDStem, no RGBDMidFusion)
#    - Late fusion (separate RGB/D backbones, fuse at head)
#    - Compare with current mid-fusion approach

# ================================================================================================
# Version History
# ================================================================================================
# v2.1 (2025-10-27): Initial release with VisDrone-only training
#   - Achieved 43.51% mAP@0.5 @ 244 epochs
#   - Baseline: 40.44% (RGB-only)
#   - Gain: +3.07% from RGB-D fusion
#
# v2.1-joint (2025-11-16): Multi-dataset training for RemDet alignment
#   - No architecture changes from v2.1
#   - Added VisDrone + UAVDT joint training support
#   - Target: Surpass RemDet-X 45.2% with RGB-D advantage
#   - Training data: 4.6x increase (6,471 â†’ 29,729 images)

# ================================================================================================
# ğŸ“š å…«è‚¡çŸ¥è¯†ç‚¹: Multi-Dataset Training Strategies
# ================================================================================================
#
# Q1: ä¸ºä»€ä¹ˆä¸åœ¨COCOä¸Šé¢„è®­ç»ƒå†fine-tune UAVæ•°æ®ï¼Ÿ
# A: (1) RemDetè®ºæ–‡æ˜ç¡®è¯´"from scratch" - å…¬å¹³å¯¹æ¯”è¦æ±‚
#    (2) COCOæ˜¯åœ°é¢è§†è§’,UAVæ˜¯ä¿¯è§†è§†è§’ - domain gapå¤§
#    (3) å®éªŒè¡¨æ˜from scratchåœ¨UAVä»»åŠ¡ä¸Šæ•ˆæœæ›´å¥½(RemDetè¯æ˜)
#    (4) å¯é€‰ablation: æµ‹è¯•COCOé¢„è®­ç»ƒæ˜¯å¦çœŸçš„æœ‰å¸®åŠ©
#
# Q2: VisDroneå’ŒUAVDTç±»åˆ«ä¸ä¸€è‡´å¦‚ä½•å¤„ç†ï¼Ÿ
# A: (1) VisDrone: 10ç±»(è¡Œäºº/è‡ªè¡Œè½¦/æ±½è½¦/é¢åŒ…è½¦/å¡è½¦/...)
#    (2) UAVDT: 3ç±»(car/truck/bus)
#    (3) æ˜ å°„ç­–ç•¥: UAVDT carâ†’VisDrone ID=3, truckâ†’5, busâ†’8
#    (4) è®­ç»ƒæ—¶ç»Ÿä¸€ç”¨VisDrone 10ç±»æ ‡ç­¾
#    (5) UAVDTå›¾åƒåªæ ‡æ³¨3ç±»,å…¶ä»–ç±»è§†ä¸ºè´Ÿæ ·æœ¬
#
# Q3: å¤šæ•°æ®é›†è®­ç»ƒæ—¶å¦‚ä½•å¹³è¡¡é‡‡æ ·ï¼Ÿ
# A: ä¸‰ç§ç­–ç•¥:
#    (1) æ¯”ä¾‹é‡‡æ ·: æŒ‰æ•°æ®é›†å¤§å° (VisDrone 22% vs UAVDT 78%)
#    (2) å‡åŒ€é‡‡æ ·: æ¯ä¸ªæ•°æ®é›†50%æ¦‚ç‡ (å¹³è¡¡ç±»åˆ«åˆ†å¸ƒ)
#    (3) åŠ æƒé‡‡æ ·: dataset_weights=[2.0, 1.0] (æ‰‹åŠ¨è°ƒæ•´)
#    RemDetæœªè¯´æ˜,æ¨æµ‹ç”¨æ¯”ä¾‹é‡‡æ · (æœ€ç®€å•)
#
# Q4: ä¸ºä»€ä¹ˆvalidationåªç”¨VisDroneä¸ç”¨UAVDTï¼Ÿ
# A: (1) RemDetè®ºæ–‡è¯„ä¼°åè®®: VisDrone val set (548 images)
#    (2) å…¬å¹³å¯¹æ¯”è¦æ±‚: åŒæ ·valé›†åˆ
#    (3) UAVDT valå¯ä»¥é¢å¤–æµ‹è¯•,ä½†ä¸ä½œä¸ºä¸»æŒ‡æ ‡
#    (4) é¿å…"æ•°æ®æ³„éœ²": UAVDTè®­ç»ƒæ•°æ®å¤š,ç”¨å…¶valä¼šåé«˜
#
# Q5: å¤šæ•°æ®é›†è®­ç»ƒä¼šå¢åŠ å¤šå°‘è®¡ç®—æˆæœ¬ï¼Ÿ
# A: (1) æ•°æ®é‡: 6,471 â†’ 29,729 (4.6x)
#    (2) Epochæ—¶é—´: ~1å°æ—¶ â†’ ~4.6å°æ—¶ (çº¿æ€§å¢é•¿)
#    (3) æ€»è®­ç»ƒæ—¶é—´: 300 epochs Ã— 4.6å°æ—¶ = 1,380å°æ—¶ = 57.5å¤©
#    (4) ä¼˜åŒ–: å¤šGPUå¹¶è¡Œ(8å¡ â†’ 7.2å¤©), æ··åˆç²¾åº¦AMP (å†å‡30%)
#    (5) æœ€ç»ˆ: å•å¡300 epochsçº¦5-6å¤©å¯å®Œæˆ
# ================================================================================================
