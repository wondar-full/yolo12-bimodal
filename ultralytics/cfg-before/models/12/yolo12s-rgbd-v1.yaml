# Ultralytics YOLO üöÄ, AGPL-3.0 license
# YOLO12-S RGB-D object detection model (Dual-Modal: RGB + Depth)
# Created: 2025-10-26 for yoloDepth project
# Target: Exceed RemDet (AAAI2025) on UAV small object detection

# ================================================================================================
# Model Configuration
# ================================================================================================
nc:
  10 # Number of classes (VisDrone: 10 classes)
  # 0=pedestrian, 1=people, 2=bicycle, 3=car, 4=van, 5=truck, 6=tricycle, 7=awning-tricycle, 8=bus, 9=motor

# Model compound scaling constants
# Using 's' (small) scale as baseline for initial experiments
scales:
  # [depth, width, max_channels]
  n: [0.50, 0.25, 1024] # nano - fastest
  s: [0.50, 0.50, 1024] # small - **baseline for RGB-D**
  m: [0.50, 1.00, 512] # medium
  l: [1.00, 1.00, 512] # large
  x: [1.00, 1.50, 512] # extra-large

# ================================================================================================
# Backbone (Feature Extractor)
# ================================================================================================
# Key modifications for RGB-D:
# 1. Layer 0: RGBDStem replaces Conv (dual-branch early fusion)
# 2. Input channels: 4 (RGB=3 + Depth=1)
# 3. RGBDStem outputs 128 channels (64*2 from dual branches)
# 4. Remaining layers unchanged (inherit from yolo12.yaml)

backbone:
  # [from, repeats, module, args]

  # Layer 0: RGB-D Dual-Branch Stem (P1/2 - First Downsampling)
  # Args: [c1, c2, k, s, depth_channels, c_mid, fusion, reduction, act]
  # - c1=4: Input channels (RGB=3 + Depth=1)
  # - c2=128: Output channels (must be even for dual-branch concatenation)
  # - k=3: Kernel size
  # - s=2: Stride (downsample by 2)
  # - depth_channels=1: Number of depth channels
  # - c_mid=64: Mid-layer channels per branch (64 RGB + 64 Depth ‚Üí 128 total)
  # - fusion="gated_add": Adaptive gating fusion (RemDet-inspired)
  # - reduction=16: Channel reduction ratio for gating network
  # - act=True: Use default SiLU activation
  - [-1, 1, RGBDStem, [4, 128, 3, 2, 1, 64, "gated_add", 16, True]] # 0-P1/2 (RGB-DÂÖ•Âè£)

  # Layer 1: Standard Conv (P2/4 - Second Downsampling)
  # Input: 128 channels from RGBDStem output
  # Note: RGBDStem outputs [fused_feat, depth_feat] concatenated
  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4

  # Layers 2-8: Standard YOLOv12 backbone (unchanged)
  - [-1, 2, C3k2, [256, False, 0.25]] # 2
  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8
  - [-1, 2, C3k2, [512, False, 0.25]] # 4
  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16
  - [-1, 4, A2C2f, [512, True, 4]] # 6
  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32
  - [-1, 4, A2C2f, [1024, True, 1]] # 8

# ================================================================================================
# Head (Detection Neck + Prediction)
# ================================================================================================
# Unchanged from yolo12.yaml - uses standard multi-scale feature pyramid
# Future work: Consider adding RGBDMidFusion at P3/P4/P5 entry points (Phase 3)

head:
  # Upsample path (Top-Down)
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]] # 9
  - [[-1, 6], 1, Concat, [1]] # 10 - cat backbone P4
  - [-1, 2, A2C2f, [512, False, -1]] # 11

  - [-1, 1, nn.Upsample, [None, 2, "nearest"]] # 12
  - [[-1, 4], 1, Concat, [1]] # 13 - cat backbone P3
  - [-1, 2, A2C2f, [256, False, -1]] # 14

  # Downsample path (Bottom-Up)
  - [-1, 1, Conv, [256, 3, 2]] # 15
  - [[-1, 11], 1, Concat, [1]] # 16 - cat head P4
  - [-1, 2, A2C2f, [512, False, -1]] # 17

  - [-1, 1, Conv, [512, 3, 2]] # 18
  - [[-1, 8], 1, Concat, [1]] # 19 - cat head P5
  - [-1, 2, C3k2, [1024, True]] # 20 (P5/32-large)

  # Detection heads (P3, P4, P5)
  - [[14, 17, 20], 1, Detect, [nc]] # 21 - Detect(P3, P4, P5)

# ================================================================================================
# Training Hyperparameters (RemDet-Aligned)
# ================================================================================================
# Reference: RemDet (AAAI2025) - https://arxiv.org/abs/2408.xxxxx
# These hyperparameters should be set in train_depth.py, not in YAML

# Recommended settings (to be used in training script):
# - optimizer: SGD (momentum=0.937, weight_decay=0.0005)
# - lr_scheduler: CosineAnnealing
# - initial_lr: 0.01
# - warmup_epochs: 3
# - total_epochs: 300
# - batch_size: 16
# - input_size: 640√ó640
# - mosaic: 1.0
# - mixup: 0.15
# - hsv_h: 0.015
# - hsv_s: 0.7
# - hsv_v: 0.4
# - translate: 0.1
# - scale: 0.9
# - fliplr: 0.5
# - conf_threshold: 0.001
# - iou_threshold: 0.6
# - max_det: 300

# ================================================================================================
# Model Summary (Expected - yolo12s-rgbd-v1)
# ================================================================================================
# - Parameters: ~11.5M (+0.3M from RGBDStem vs standard yolo12s)
# - FLOPs: ~24.0G (+2.3G from geometry prior generation)
# - Input: [B, 4, 640, 640] (RGB + Depth)
# - Output: [B, num_anchors, 4+nc] where num_anchors ‚âà 8400 (P3+P4+P5)
# - Inference Speed (RTX 4090): ~50-60 FPS (target)

# ================================================================================================
# Version History
# ================================================================================================
# v1.0 (2025-10-26): Initial RGB-D baseline
#   - Added RGBDStem at layer 0
#   - Kept backbone/head unchanged (conservative approach)
#   - Target: Establish stable training pipeline

# Future versions:
# v1.1 (Phase 2): Add gradient clipping, AMP optimization
# v1.2 (Phase 3): Add RGBDMidFusion at P3/P4/P5
# v1.3 (Phase 4): Add adaptive attention mechanisms
# v2.0 (Phase 5): Lightweight optimization (pruning/distillation)

# ================================================================================================
# Usage Examples
# ================================================================================================
# 1. Training:
#    ```bash
#    python train_depth.py --model yolo12s-rgbd-v1.yaml --data visdrone-rgbd.yaml --epochs 300
#    ```
#
# 2. Validation:
#    ```bash
#    python val_depth.py --weights runs/train/exp1/weights/best.pt --data visdrone-rgbd.yaml
#    ```
#
# 3. Inference:
#    ```bash
#    from ultralytics import YOLO
#    model = YOLO('runs/train/exp1/weights/best.pt')
#    results = model.predict(source='rgb_image.jpg', depth='depth_image.png')
#    ```

# ================================================================================================
# Notes & Limitations
# ================================================================================================
# 1. **Input Requirements**:
#    - RGB and Depth must have same dimensions (H, W)
#    - Depth format: Single-channel grayscale (8-bit or 16-bit PNG)
#    - Depth preprocessing: Automatic (median‚Üígaussian‚Üíconfidence weighting)
#
# 2. **Compatibility**:
#    - Requires YOLORGBDDataset for data loading
#    - Requires RGBDStem, DepthGatedFusion, GeometryPriorGenerator modules
#    - Compatible with Ultralytics v8.3.155+
#
# 3. **Performance Expectations**:
#    - Baseline (RGB-only yolo12s): ~39% mAP@0.5, ~13% mAP_small (VisDrone)
#    - Target (RGB-D v1.0): ~41% mAP@0.5, ~15% mAP_small (+2pt improvement)
#    - RemDet-X benchmark: ~45.2% mAP@0.5, ~21.3% mAP_small (Ë∂ÖË∂äÁõÆÊ†á)
#
# 4. **Known Issues**:
#    - First training may be slower due to geometry prior computation
#    - Depth quality directly impacts performance (use Depth Anything V2 for best results)
#    - Small batch sizes (<8) may cause training instability
#
# 5. **Debugging Tips**:
#    - Check depth alignment: Ensure RGB and Depth files have matching filenames
#    - Monitor RGBDStem statistics: last_geo_quality (should be >0.5), last_gate_mean (should be 0.3-0.7)
#    - Visualize depth preprocessing: Save intermediate depth maps to verify quality
