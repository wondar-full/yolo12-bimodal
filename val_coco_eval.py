#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
COCOé£æ ¼è¯„ä¼°è„šæœ¬ (æ–¹æ¡ˆA - åˆ†å¼€è®­ç»ƒ)
ä½¿ç”¨ pycocotools ç”Ÿæˆå®Œæ•´çš„ COCO æ ‡å‡†æŒ‡æ ‡,å¯¹é½ RemDet è®ºæ–‡ Table 1 & Table 2

ä½¿ç”¨æ–¹æ³•:
    # VisDrone è¯„ä¼°
    python val_coco_eval.py \
        --weights runs/train/visdrone_rgbd_n_300ep/weights/best.pt \
        --data data/visdrone-rgbd.yaml \
        --name visdrone_coco_eval
    
    # UAVDT è¯„ä¼°
    python val_coco_eval.py \
        --weights runs/train/uavdt_rgbd_n_300ep/weights/best.pt \
        --data data/uavdt-rgbd.yaml \
        --name uavdt_coco_eval

è¾“å‡ºæŒ‡æ ‡ (å®Œå…¨å¯¹é½ RemDet):
    - AP@0.50:0.95 (IoU=0.50:0.95, area=all)
    - AP@0.50      (IoU=0.50, area=all)
    - AP@0.75      (IoU=0.75, area=all)
    - AP_small     (IoU=0.50:0.95, area=small)
    - AP_medium    (IoU=0.50:0.95, area=medium)
    - AP_large     (IoU=0.50:0.95, area=large)
    + AR (Average Recall) ç³»åˆ—æŒ‡æ ‡
"""

import argparse
import json
import os
from pathlib import Path
import yaml
import numpy as np
import torch
from tqdm import tqdm

from ultralytics import YOLO


def create_coco_annotations(data_yaml, split='val'):
    """
    ä» YOLO æ ‡æ³¨åˆ›å»º COCO æ ¼å¼çš„ ground truth JSON
    
    Args:
        data_yaml: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        split: 'val' æˆ– 'test'
    
    Returns:
        coco_gt: COCOæ ¼å¼çš„å­—å…¸ (å¯ç›´æ¥ä¿å­˜ä¸ºJSON)
    """
    # è¯»å–é…ç½®
    with open(data_yaml, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    dataset_root = Path(data['path'])
    img_dir = dataset_root / data[split]
    label_dir = dataset_root / 'labels' / split
    
    nc = data['nc']
    names = data['names']
    
    # COCOæ ¼å¼å­—å…¸
    coco_gt = {
        "images": [],
        "annotations": [],
        "categories": []
    }
    
    # æ·»åŠ ç±»åˆ«ä¿¡æ¯
    for i in range(nc):
        coco_gt["categories"].append({
            "id": i + 1,  # COCOç±»åˆ«IDä»1å¼€å§‹
            "name": names[i],
            "supercategory": "object"
        })
    
    # éå†å›¾ç‰‡
    img_files = sorted(Path(img_dir).glob('*.jpg')) + sorted(Path(img_dir).glob('*.png'))
    ann_id = 1  # æ ‡æ³¨IDè®¡æ•°å™¨
    
    for img_id, img_path in enumerate(tqdm(img_files, desc=f"Creating COCO GT for {split}"), 1):
        # è¯»å–å›¾ç‰‡å°ºå¯¸
        from PIL import Image
        img = Image.open(img_path)
        width, height = img.size
        
        # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
        coco_gt["images"].append({
            "id": img_id,
            "file_name": img_path.name,
            "width": width,
            "height": height
        })
        
        # è¯»å–å¯¹åº”çš„æ ‡æ³¨
        label_path = label_dir / f"{img_path.stem}.txt"
        if not label_path.exists():
            continue
        
        with open(label_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) < 5:
                    continue
                
                cls_id = int(parts[0])
                x_center, y_center, w, h = map(float, parts[1:5])
                
                # YOLOæ ¼å¼ â†’ COCOæ ¼å¼ (å½’ä¸€åŒ–åæ ‡ â†’ åƒç´ åæ ‡)
                x_min = (x_center - w / 2) * width
                y_min = (y_center - h / 2) * height
                bbox_w = w * width
                bbox_h = h * height
                area = bbox_w * bbox_h
                
                coco_gt["annotations"].append({
                    "id": ann_id,
                    "image_id": img_id,
                    "category_id": cls_id + 1,  # COCOç±»åˆ«IDä»1å¼€å§‹
                    "bbox": [x_min, y_min, bbox_w, bbox_h],  # [x, y, width, height]
                    "area": area,
                    "iscrowd": 0
                })
                ann_id += 1
    
    return coco_gt


def yolo_results_to_coco(results, img_id_map):
    """
    å°† YOLO æ£€æµ‹ç»“æœè½¬ä¸º COCO æ ¼å¼çš„é¢„æµ‹ JSON
    
    Args:
        results: YOLO model.val() è¿”å›çš„ç»“æœ
        img_id_map: {img_filename: coco_image_id} æ˜ å°„
    
    Returns:
        coco_pred: COCOæ ¼å¼çš„é¢„æµ‹åˆ—è¡¨
    """
    coco_pred = []
    
    for result in results:
        img_name = Path(result.path).name
        img_id = img_id_map.get(img_name)
        if img_id is None:
            continue
        
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            
            # è½¬æ¢ä¸ºCOCOæ ¼å¼ [x, y, width, height]
            bbox = [
                float(x1),
                float(y1),
                float(x2 - x1),
                float(y2 - y1)
            ]
            
            coco_pred.append({
                "image_id": img_id,
                "category_id": cls_id + 1,  # COCOç±»åˆ«IDä»1å¼€å§‹
                "bbox": bbox,
                "score": conf
            })
    
    return coco_pred


def evaluate_with_pycocotools(gt_json_path, pred_json_path):
    """
    ä½¿ç”¨ pycocotools è®¡ç®— COCO æ ‡å‡†æŒ‡æ ‡
    
    Args:
        gt_json_path: Ground truth JSON è·¯å¾„
        pred_json_path: Predictions JSON è·¯å¾„
    
    Returns:
        metrics: å­—å…¸,åŒ…å«æ‰€æœ‰ COCO æŒ‡æ ‡
    """
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval
    except ImportError:
        print("âŒ pycocotools not installed!")
        print("   Install with: pip install pycocotools")
        return None
    
    # åŠ è½½GTå’Œé¢„æµ‹
    coco_gt = COCO(gt_json_path)
    coco_pred = coco_gt.loadRes(pred_json_path)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')
    
    # è¿è¡Œè¯„ä¼°
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # æå–æŒ‡æ ‡
    metrics = {
        "AP@0.50:0.95": coco_eval.stats[0],  # AP at IoU=0.50:0.95
        "AP@0.50": coco_eval.stats[1],       # AP at IoU=0.50
        "AP@0.75": coco_eval.stats[2],       # AP at IoU=0.75
        "AP_small": coco_eval.stats[3],      # AP for small objects
        "AP_medium": coco_eval.stats[4],     # AP for medium objects
        "AP_large": coco_eval.stats[5],      # AP for large objects
        "AR@0.50:0.95 (max=1)": coco_eval.stats[6],
        "AR@0.50:0.95 (max=10)": coco_eval.stats[7],
        "AR@0.50:0.95 (max=100)": coco_eval.stats[8],
        "AR_small": coco_eval.stats[9],
        "AR_medium": coco_eval.stats[10],
        "AR_large": coco_eval.stats[11]
    }
    
    return metrics


def print_remdet_comparison(metrics, dataset_name):
    """
    æ‰“å°ä¸ RemDet çš„å¯¹æ¯”è¡¨æ ¼
    
    Args:
        metrics: COCOè¯„ä¼°æŒ‡æ ‡å­—å…¸
        dataset_name: 'VisDrone' æˆ– 'UAVDT'
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š {dataset_name} Results - RemDet Comparison")
    print("="*80)
    
    # RemDet baseline (æ ¹æ®æ•°æ®é›†é€‰æ‹©)
    if dataset_name == 'VisDrone':
        remdet_baselines = {
            'RemDet-Tiny': {'AP@0.50:0.95': 20.1, 'AP@0.50': 33.5, 'AP@0.75': 20.9, 'AP_small': 9.6, 'AP_medium': 30.4, 'AP_large': 49.7},
            'RemDet-S':    {'AP@0.50:0.95': 26.3, 'AP@0.50': 42.3, 'AP@0.75': 27.8, 'AP_small': 14.5, 'AP_medium': 39.1, 'AP_large': 55.8},
            'RemDet-M':    {'AP@0.50:0.95': 28.0, 'AP@0.50': 45.0, 'AP@0.75': 29.6, 'AP_small': 16.2, 'AP_medium': 41.5, 'AP_large': 57.3},
            'RemDet-L':    {'AP@0.50:0.95': 29.5, 'AP@0.50': 47.4, 'AP@0.75': 30.9, 'AP_small': 18.5, 'AP_medium': 43.5, 'AP_large': 58.1},
            'RemDet-X':    {'AP@0.50:0.95': 29.9, 'AP@0.50': 48.3, 'AP@0.75': 31.0, 'AP_small': 19.5, 'AP_medium': 44.1, 'AP_large': 58.6}
        }
        primary_baseline = 'RemDet-X'
    else:  # UAVDT
        remdet_baselines = {
            'RemDet-L': {'AP@0.50:0.95': 20.6, 'AP@0.50': 34.5, 'AP@0.75': 20.5, 'AP_small': 12.6, 'AP_medium': 29.0, 'AP_large': 46.8}
        }
        primary_baseline = 'RemDet-L'
    
    # æ‰“å°ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    print("\nğŸ¯ Main Metrics (vs {})".format(primary_baseline))
    print("-"*80)
    print(f"{'Metric':<20} {'YoloDepth':<15} {primary_baseline:<15} {'Î”':<15}")
    print("-"*80)
    
    main_metrics = ['AP@0.50:0.95', 'AP@0.50', 'AP@0.75', 'AP_small', 'AP_medium', 'AP_large']
    baseline = remdet_baselines[primary_baseline]
    
    for metric in main_metrics:
        our_val = metrics[metric] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        baseline_val = baseline[metric]
        delta = our_val - baseline_val
        delta_str = f"{delta:+.1f}%" if delta != 0 else "0.0%"
        
        # é¢œè‰²æ ‡è®° (ä»…åœ¨ç»ˆç«¯æ˜¾ç¤ºæ—¶æœ‰æ•ˆ)
        if delta > 0:
            delta_str = f"âœ… {delta_str}"
        elif delta < -2:
            delta_str = f"âŒ {delta_str}"
        else:
            delta_str = f"â– {delta_str}"
        
        print(f"{metric:<20} {our_val:>6.1f}%{'':<8} {baseline_val:>6.1f}%{'':<8} {delta_str}")
    
    # æ‰“å°å®Œæ•´åŸºçº¿å¯¹æ¯”
    if len(remdet_baselines) > 1:
        print("\nğŸ“‹ Full RemDet Baseline Comparison (AP@0.50)")
        print("-"*80)
        print(f"{'Model':<20} {'AP@0.50':<15} {'vs Ours':<15}")
        print("-"*80)
        for model, values in remdet_baselines.items():
            our_val = metrics['AP@0.50'] * 100
            baseline_val = values['AP@0.50']
            delta = our_val - baseline_val
            delta_str = f"{delta:+.1f}%"
            print(f"{model:<20} {baseline_val:>6.1f}%{'':<8} {delta_str}")
    
    # æ‰“å°æ¬¡è¦æŒ‡æ ‡
    print("\nğŸ“ˆ Additional Metrics")
    print("-"*80)
    print(f"{'Metric':<30} {'Value':<15}")
    print("-"*80)
    for metric in ['AR@0.50:0.95 (max=100)', 'AR_small', 'AR_medium', 'AR_large']:
        val = metrics[metric] * 100
        print(f"{metric:<30} {val:>6.1f}%")
    
    print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description='COCO-style evaluation for yoloDepth')
    parser.add_argument('--weights', type=str, required=True, help='Model weights path')
    parser.add_argument('--data', type=str, required=True, help='Dataset YAML path')
    parser.add_argument('--name', type=str, default='coco_eval', help='Experiment name')
    parser.add_argument('--imgsz', type=int, default=640, help='Image size')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--device', type=str, default='0', help='CUDA device')
    parser.add_argument('--split', type=str, default='val', choices=['val', 'test'], help='Dataset split')
    parser.add_argument('--save-json', action='store_true', help='Save COCO JSONs for manual inspection')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    save_dir = Path('runs') / 'val' / args.name
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ğŸš€ Starting COCO-style Evaluation")
    print("="*80)
    print(f"ğŸ“ Weights:  {args.weights}")
    print(f"ğŸ“ Data:     {args.data}")
    print(f"ğŸ“ Save dir: {save_dir}")
    print(f"ğŸ–¼ï¸  Image size: {args.imgsz}")
    print(f"ğŸ”¢ Batch size: {args.batch}")
    print(f"ğŸ® Device:   {args.device}")
    print("="*80 + "\n")
    
    # 1. åˆ›å»º COCO Ground Truth JSON
    print("ğŸ“ Step 1/4: Creating COCO Ground Truth JSON...")
    gt_json_path = save_dir / f"gt_{args.split}.json"
    
    if not gt_json_path.exists():
        coco_gt = create_coco_annotations(args.data, split=args.split)
        with open(gt_json_path, 'w') as f:
            json.dump(coco_gt, f)
        print(f"   âœ… Saved to {gt_json_path}")
        print(f"   ğŸ“Š {len(coco_gt['images'])} images, {len(coco_gt['annotations'])} annotations")
    else:
        print(f"   â™»ï¸  Using existing GT JSON: {gt_json_path}")
        with open(gt_json_path, 'r') as f:
            coco_gt = json.load(f)
        print(f"   ğŸ“Š {len(coco_gt['images'])} images, {len(coco_gt['annotations'])} annotations")
    
    # åˆ›å»ºå›¾ç‰‡IDæ˜ å°„
    img_id_map = {img['file_name']: img['id'] for img in coco_gt['images']}
    
    # 2. è¿è¡Œ YOLO éªŒè¯ç”Ÿæˆé¢„æµ‹
    print("\nğŸ” Step 2/4: Running YOLO Validation...")
    model = YOLO(args.weights)
    
    # ä½¿ç”¨ model.val() å¹¶ä¿å­˜ç»“æœ
    results = model.val(
        data=args.data,
        imgsz=args.imgsz,
        batch=args.batch,
        device=args.device,
        split=args.split,
        save_json=False,  # æˆ‘ä»¬æ‰‹åŠ¨ç”ŸæˆCOCOæ ¼å¼JSON
        verbose=False
    )
    
    print("   âœ… Validation completed")
    
    # 3. è½¬æ¢é¢„æµ‹ä¸º COCO æ ¼å¼
    print("\nğŸ“ Step 3/4: Converting predictions to COCO format...")
    
    # é‡æ–°è¿è¡Œä¸€æ¬¡è·å–è¯¦ç»†ç»“æœ (model.val()ä¸è¿”å›è¯¦ç»†boxes)
    pred_results = model.predict(
        source=Path(args.data).parent / 'data' / args.split / 'images' / 'rgb',
        imgsz=args.imgsz,
        device=args.device,
        verbose=False,
        stream=True
    )
    
    coco_pred = yolo_results_to_coco(pred_results, img_id_map)
    
    pred_json_path = save_dir / f"pred_{args.split}.json"
    with open(pred_json_path, 'w') as f:
        json.dump(coco_pred, f)
    
    print(f"   âœ… Saved to {pred_json_path}")
    print(f"   ğŸ“Š {len(coco_pred)} predictions")
    
    # 4. ä½¿ç”¨ pycocotools è¯„ä¼°
    print("\nğŸ“Š Step 4/4: Evaluating with pycocotools...")
    metrics = evaluate_with_pycocotools(str(gt_json_path), str(pred_json_path))
    
    if metrics is None:
        print("âŒ Evaluation failed (pycocotools not available)")
        return
    
    # ç¡®å®šæ•°æ®é›†åç§°
    dataset_name = 'VisDrone' if 'visdrone' in args.data.lower() else 'UAVDT'
    
    # æ‰“å°ä¸RemDetçš„å¯¹æ¯”
    print_remdet_comparison(metrics, dataset_name)
    
    # ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶
    metrics_file = save_dir / 'metrics.json'
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"ğŸ’¾ Metrics saved to {metrics_file}")
    
    # å¯é€‰: ä¿å­˜JSONä¾›æ‰‹åŠ¨æ£€æŸ¥
    if args.save_json:
        print(f"\nğŸ“¦ COCO JSON files saved:")
        print(f"   GT:   {gt_json_path}")
        print(f"   Pred: {pred_json_path}")
    
    print("\nâœ… Evaluation completed!")
    print("="*80)


if __name__ == '__main__':
    main()
