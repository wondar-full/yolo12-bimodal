#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
COCOæ ‡å‡†è¯„ä¼°è„šæœ¬ (æ•´åˆç‰ˆ - æ–¹æ¡ˆA)
Step 1: ä½¿ç”¨ model.val() ç”Ÿæˆ predictions.json
Step 2: ä½¿ç”¨ pycocotools åŠ è½½ç°æœ‰çš„ val.json å’Œ predictions.json è¿›è¡ŒCOCOæ ‡å‡†è¯„ä¼°

ä½¿ç”¨æ–¹æ³•:
    # VisDrone è¯„ä¼°
    python val_coco_standard.py \
        --weights runs/train/visdrone_rgbd_n_300ep/weights/best.pt \
        --data data/visdrone-rgbd.yaml \
        --gt-json /data2/user/2024/lzy/Datasets/VisDrone2019-DET-COCO/annotations/VisDrone2019-DET_val_coco.json \
        --name visdrone_coco_eval
    
    # UAVDT è¯„ä¼°
    python val_coco_standard.py \
        --weights runs/train/uavdt_rgbd_n_300ep/weights/best.pt \
        --data data/uavdt-rgbd.yaml \
        --gt-json /data2/user/2024/lzy/Datasets/UAVDT/annotations/UAV-benchmark-M-Val.json \
        --name uavdt_coco_eval

è¾“å‡ºæŒ‡æ ‡ (å®Œå…¨å¯¹é½ RemDet):
    - AP@0.50:0.95 (IoU=0.50:0.95, area=all)
    - AP@0.50      (IoU=0.50, area=all) â† ä¸»è¦å¯¹æ¯”æŒ‡æ ‡
    - AP@0.75      (IoU=0.75, area=all)
    - AP_small     (IoU=0.50:0.95, area=small) â† UAVå…³é”®æŒ‡æ ‡
    - AP_medium    (IoU=0.50:0.95, area=medium)
    - AP_large     (IoU=0.50:0.95, area=large)
    + AR (Average Recall) ç³»åˆ—æŒ‡æ ‡
"""

import argparse
import json
from pathlib import Path
import sys

from ultralytics import YOLO


def evaluate_with_pycocotools(gt_json_path, pred_json_path):
    """
    ä½¿ç”¨ pycocotools è®¡ç®— COCO æ ‡å‡†æŒ‡æ ‡
    
    Args:
        gt_json_path: Ground truth JSON è·¯å¾„ (ä½ å·²æœ‰çš„val.json)
        pred_json_path: Predictions JSON è·¯å¾„ (model.val()ç”Ÿæˆçš„)
    
    Returns:
        metrics: å­—å…¸,åŒ…å«æ‰€æœ‰ COCO æŒ‡æ ‡
    """
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval
    except ImportError:
        print("âŒ pycocotools not installed!")
        print("   Install with:")
        print("   - Linux: pip install pycocotools")
        print("   - Windows: pip install pycocotools-windows")
        return None
    
    print(f"\nğŸ“‚ Loading Ground Truth: {gt_json_path}")
    coco_gt = COCO(gt_json_path)
    
    print(f"ğŸ“‚ Loading Predictions: {pred_json_path}")
    coco_pred = coco_gt.loadRes(pred_json_path)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    print("\nğŸ” Running COCO evaluation...")
    coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')
    
    # è¿è¡Œè¯„ä¼°
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # æå–æŒ‡æ ‡
    metrics = {
        "AP@0.50:0.95": coco_eval.stats[0],  # AP at IoU=0.50:0.95
        "AP@0.50": coco_eval.stats[1],       # AP at IoU=0.50
        "AP@0.75": coco_eval.stats[2],       # AP at IoU=0.75
        "AP_small": coco_eval.stats[3],      # AP for small objects
        "AP_medium": coco_eval.stats[4],     # AP for medium objects
        "AP_large": coco_eval.stats[5],      # AP for large objects
        "AR@0.50:0.95 (max=1)": coco_eval.stats[6],
        "AR@0.50:0.95 (max=10)": coco_eval.stats[7],
        "AR@0.50:0.95 (max=100)": coco_eval.stats[8],
        "AR_small": coco_eval.stats[9],
        "AR_medium": coco_eval.stats[10],
        "AR_large": coco_eval.stats[11]
    }
    
    return metrics


def print_remdet_comparison(metrics, dataset_name):
    """
    æ‰“å°ä¸ RemDet çš„å¯¹æ¯”è¡¨æ ¼
    
    Args:
        metrics: COCOè¯„ä¼°æŒ‡æ ‡å­—å…¸
        dataset_name: 'VisDrone' æˆ– 'UAVDT'
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š {dataset_name} Results - RemDet Comparison")
    print("="*80)
    
    # RemDet baseline (æ ¹æ®æ•°æ®é›†é€‰æ‹©)
    if dataset_name == 'VisDrone':
        remdet_baselines = {
            'RemDet-Tiny': {'AP@0.50:0.95': 20.1, 'AP@0.50': 33.5, 'AP@0.75': 20.9, 'AP_small': 9.6, 'AP_medium': 30.4, 'AP_large': 49.7},
            'RemDet-S':    {'AP@0.50:0.95': 26.3, 'AP@0.50': 42.3, 'AP@0.75': 27.8, 'AP_small': 14.5, 'AP_medium': 39.1, 'AP_large': 55.8},
            'RemDet-M':    {'AP@0.50:0.95': 28.0, 'AP@0.50': 45.0, 'AP@0.75': 29.6, 'AP_small': 16.2, 'AP_medium': 41.5, 'AP_large': 57.3},
            'RemDet-L':    {'AP@0.50:0.95': 29.5, 'AP@0.50': 47.4, 'AP@0.75': 30.9, 'AP_small': 18.5, 'AP_medium': 43.5, 'AP_large': 58.1},
            'RemDet-X':    {'AP@0.50:0.95': 29.9, 'AP@0.50': 48.3, 'AP@0.75': 31.0, 'AP_small': 19.5, 'AP_medium': 44.1, 'AP_large': 58.6}
        }
        primary_baseline = 'RemDet-X'
    else:  # UAVDT
        remdet_baselines = {
            'RemDet-L': {'AP@0.50:0.95': 20.6, 'AP@0.50': 34.5, 'AP@0.75': 20.5, 'AP_small': 12.6, 'AP_medium': 29.0, 'AP_large': 46.8}
        }
        primary_baseline = 'RemDet-L'
    
    # æ‰“å°ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    print("\nğŸ¯ Main Metrics (vs {})".format(primary_baseline))
    print("-"*80)
    print(f"{'Metric':<20} {'YoloDepth':<15} {primary_baseline:<15} {'Î”':<15}")
    print("-"*80)
    
    main_metrics = ['AP@0.50:0.95', 'AP@0.50', 'AP@0.75', 'AP_small', 'AP_medium', 'AP_large']
    baseline = remdet_baselines[primary_baseline]
    
    for metric in main_metrics:
        our_val = metrics[metric] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        baseline_val = baseline[metric]
        delta = our_val - baseline_val
        delta_str = f"{delta:+.1f}%" if delta != 0 else "0.0%"
        
        # é¢œè‰²æ ‡è®° (ä»…åœ¨ç»ˆç«¯æ˜¾ç¤ºæ—¶æœ‰æ•ˆ)
        if delta > 0:
            delta_str = f"âœ… {delta_str}"
        elif delta < -2:
            delta_str = f"âŒ {delta_str}"
        else:
            delta_str = f"â– {delta_str}"
        
        print(f"{metric:<20} {our_val:>6.1f}%{'':<8} {baseline_val:>6.1f}%{'':<8} {delta_str}")
    
    # æ‰“å°å®Œæ•´åŸºçº¿å¯¹æ¯”
    if len(remdet_baselines) > 1:
        print("\nğŸ“‹ Full RemDet Baseline Comparison (AP@0.50)")
        print("-"*80)
        print(f"{'Model':<20} {'AP@0.50':<15} {'vs Ours':<15}")
        print("-"*80)
        for model, values in remdet_baselines.items():
            our_val = metrics['AP@0.50'] * 100
            baseline_val = values['AP@0.50']
            delta = our_val - baseline_val
            delta_str = f"{delta:+.1f}%"
            print(f"{model:<20} {baseline_val:>6.1f}%{'':<8} {delta_str}")
    
    # æ‰“å°æ¬¡è¦æŒ‡æ ‡
    print("\nğŸ“ˆ Additional Metrics")
    print("-"*80)
    print(f"{'Metric':<30} {'Value':<15}")
    print("-"*80)
    for metric in ['AR@0.50:0.95 (max=100)', 'AR_small', 'AR_medium', 'AR_large']:
        val = metrics[metric] * 100
        print(f"{metric:<30} {val:>6.1f}%")
    
    print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description='COCOæ ‡å‡†è¯„ä¼° (æ•´åˆç‰ˆ)')
    parser.add_argument('--weights', type=str, required=True, help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--data', type=str, required=True, help='æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶')
    parser.add_argument('--gt-json', type=str, required=True, help='Ground Truth COCO JSONè·¯å¾„')
    parser.add_argument('--name', type=str, default='coco_eval', help='å®éªŒåç§°')
    parser.add_argument('--imgsz', type=int, default=640, help='å›¾ç‰‡å°ºå¯¸')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--device', type=str, default='0', help='CUDAè®¾å¤‡')
    parser.add_argument('--split', type=str, default='val', choices=['val', 'test'], help='æ•°æ®é›†åˆ†å‰²')
    parser.add_argument('--save-json', action='store_true', help='ä¿å­˜predictions.jsonä¾›åç»­æ£€æŸ¥')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    save_dir = Path('runs') / 'val' / args.name
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ğŸš€ COCOæ ‡å‡†è¯„ä¼° (ä¸¤æ­¥æ³•)")
    print("="*80)
    print(f"ğŸ“ Weights:     {args.weights}")
    print(f"ğŸ“ Data YAML:   {args.data}")
    print(f"ğŸ“ GT JSON:     {args.gt_json}")
    print(f"ğŸ“ Save dir:    {save_dir}")
    print(f"ğŸ–¼ï¸  Image size:  {args.imgsz}")
    print(f"ğŸ”¢ Batch size:  {args.batch}")
    print(f"ğŸ® Device:      {args.device}")
    print("="*80 + "\n")
    
    # æ£€æŸ¥GT JSONæ˜¯å¦å­˜åœ¨
    if not Path(args.gt_json).exists():
        print(f"âŒ Error: Ground Truth JSON not found: {args.gt_json}")
        print("\nå¯ç”¨çš„GT JSONè·¯å¾„:")
        print("  VisDrone: /data2/user/2024/lzy/Datasets/VisDrone2019-DET-COCO/annotations/VisDrone2019-DET_val_coco.json")
        print("  UAVDT:    /data2/user/2024/lzy/Datasets/UAVDT/annotations/UAV-benchmark-M-Val.json")
        sys.exit(1)
    
    # =================================================================
    # Step 1: è¿è¡Œ YOLO éªŒè¯ç”Ÿæˆ predictions.json
    # =================================================================
    print("="*80)
    print("ğŸ“ Step 1/2: Running YOLO Validation to Generate predictions.json")
    print("="*80)
    
    model = YOLO(args.weights)
    
    # è¿è¡ŒéªŒè¯ (save_json=True ä¼šè‡ªåŠ¨ç”Ÿæˆ predictions.json)
    results = model.val(
        data=args.data,
        imgsz=args.imgsz,
        batch=args.batch,
        device=args.device,
        split=args.split,
        save_json=True,  # å…³é”®: ç”ŸæˆCOCOæ ¼å¼çš„predictions.json
        project=str(save_dir.parent),
        name=args.name,
        verbose=True
    )
    
    print("   âœ… YOLO validation completed")
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„ predictions.json
    # Ultralytics é»˜è®¤ä¿å­˜åœ¨ runs/val/{name}/predictions.json
    pred_json_path = save_dir / 'predictions.json'
    
    if not pred_json_path.exists():
        print(f"âŒ Error: predictions.json not found at {pred_json_path}")
        print("   Please check if save_json=True worked correctly")
        sys.exit(1)
    
    print(f"   ğŸ“‚ predictions.json saved to: {pred_json_path}")
    
    # =================================================================
    # Step 2: ä½¿ç”¨ pycocotools è¿›è¡Œ COCO æ ‡å‡†è¯„ä¼°
    # =================================================================
    print("\n" + "="*80)
    print("ğŸ“Š Step 2/2: Evaluating with pycocotools")
    print("="*80)
    
    metrics = evaluate_with_pycocotools(args.gt_json, str(pred_json_path))
    
    if metrics is None:
        print("âŒ Evaluation failed (pycocotools not available)")
        sys.exit(1)
    
    # ç¡®å®šæ•°æ®é›†åç§°
    if 'visdrone' in args.data.lower() or 'visdrone' in args.gt_json.lower():
        dataset_name = 'VisDrone'
    elif 'uavdt' in args.data.lower() or 'uavdt' in args.gt_json.lower():
        dataset_name = 'UAVDT'
    else:
        dataset_name = 'Unknown'
    
    # æ‰“å°ä¸RemDetçš„å¯¹æ¯”
    print_remdet_comparison(metrics, dataset_name)
    
    # ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶
    metrics_file = save_dir / 'coco_metrics.json'
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"ğŸ’¾ COCO metrics saved to {metrics_file}")
    
    # ç”Ÿæˆç®€æ´çš„ç»“æœæŠ¥å‘Š
    report_file = save_dir / 'evaluation_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write(f"{dataset_name} COCO Evaluation Report\n")
        f.write("="*80 + "\n\n")
        f.write(f"Model: {args.weights}\n")
        f.write(f"Data:  {args.data}\n")
        f.write(f"GT:    {args.gt_json}\n\n")
        
        f.write("Main Metrics:\n")
        f.write("-"*80 + "\n")
        for metric in ['AP@0.50:0.95', 'AP@0.50', 'AP@0.75', 'AP_small', 'AP_medium', 'AP_large']:
            val = metrics[metric] * 100
            f.write(f"{metric:<20} {val:>6.2f}%\n")
        
        f.write("\nAdditional Metrics:\n")
        f.write("-"*80 + "\n")
        for metric in ['AR@0.50:0.95 (max=100)', 'AR_small', 'AR_medium', 'AR_large']:
            val = metrics[metric] * 100
            f.write(f"{metric:<30} {val:>6.2f}%\n")
    
    print(f"ğŸ“„ Evaluation report saved to {report_file}")
    
    # å¯é€‰: ä¿å­˜JSONä¾›æ‰‹åŠ¨æ£€æŸ¥
    if args.save_json:
        print(f"\nğŸ“¦ JSON files for manual inspection:")
        print(f"   GT:   {args.gt_json}")
        print(f"   Pred: {pred_json_path}")
    
    print("\nâœ… Evaluation completed!")
    print("="*80)
    
    # è¿”å›ä¸»è¦æŒ‡æ ‡ç”¨äºåç»­åˆ†æ
    return {
        'AP@0.50:0.95': metrics['AP@0.50:0.95'] * 100,
        'AP@0.50': metrics['AP@0.50'] * 100,
        'AP_small': metrics['AP_small'] * 100
    }


if __name__ == '__main__':
    main()
