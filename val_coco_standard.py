#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
COCOæ ‡å‡†è¯„ä¼°è„šæœ¬ (æ•´åˆç‰ˆ - æ–¹æ¡ˆA)
Step 1: ä½¿ç”¨ model.val() ç”Ÿæˆ predictions.json
Step 2: ä¿®æ­£ predictions.json ä¸­çš„ image_id æ ¼å¼(åŒ¹é…GT JSON)
Step 3: ä½¿ç”¨ pycocotools åŠ è½½ç°æœ‰çš„ val.json å’Œ predictions.json è¿›è¡ŒCOCOæ ‡å‡†è¯„ä¼°

ä½¿ç”¨æ–¹æ³•:
    # VisDrone è¯„ä¼°
    python val_coco_standard.py \
        --weights runs/train/visdrone_rgbd_n_300ep/weights/best.pt \
        --data data/visdrone-rgbd.yaml \
        --gt-json /data2/user/2024/lzy/Datasets/VisDrone2019-DET-COCO/annotations/VisDrone2019-DET_val_coco.json \
        --name visdrone_coco_eval
    
    # UAVDT è¯„ä¼°
    python val_coco_standard.py \
        --weights runs/train/uavdt_rgbd_n_300ep/weights/best.pt \
        --data data/uavdt-rgbd.yaml \
        --gt-json /data2/user/2024/lzy/Datasets/UAVDT/annotations/UAV-benchmark-M-Val.json \
        --name uavdt_coco_eval

è¾“å‡ºæŒ‡æ ‡ (å®Œå…¨å¯¹é½ RemDet):
    - AP@0.50:0.95 (IoU=0.50:0.95, area=all)
    - AP@0.50      (IoU=0.50, area=all) â† ä¸»è¦å¯¹æ¯”æŒ‡æ ‡
    - AP@0.75      (IoU=0.75, area=all)
    - AP_small     (IoU=0.50:0.95, area=small) â† UAVå…³é”®æŒ‡æ ‡
    - AP_medium    (IoU=0.50:0.95, area=medium)
    - AP_large     (IoU=0.50:0.95, area=large)
    + AR (Average Recall) ç³»åˆ—æŒ‡æ ‡
"""

import argparse
import json
from pathlib import Path
import sys

from ultralytics import YOLO


def evaluate_with_pycocotools(gt_json_path, pred_json_path):
    """
    ä½¿ç”¨ pycocotools è®¡ç®— COCO æ ‡å‡†æŒ‡æ ‡
    
    Args:
        gt_json_path: Ground truth JSON è·¯å¾„ (ä½ å·²æœ‰çš„val.json)
        pred_json_path: Predictions JSON è·¯å¾„ (model.val()ç”Ÿæˆçš„)
    
    Returns:
        metrics: å­—å…¸,åŒ…å«æ‰€æœ‰ COCO æŒ‡æ ‡
    """
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval
    except ImportError:
        print("âŒ pycocotools not installed!")
        print("   Install with:")
        print("   - Linux: pip install pycocotools")
        print("   - Windows: pip install pycocotools-windows")
        return None
    
    print(f"\nğŸ“‚ Loading Ground Truth: {gt_json_path}")
    coco_gt = COCO(gt_json_path)
    
    print(f"ğŸ“‚ Loading Predictions: {pred_json_path}")
    coco_pred = coco_gt.loadRes(pred_json_path)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    print("\nğŸ” Running COCO evaluation...")
    coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')
    
    # è¿è¡Œè¯„ä¼°
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    
    # æå–æŒ‡æ ‡
    metrics = {
        "AP@0.50:0.95": coco_eval.stats[0],  # AP at IoU=0.50:0.95
        "AP@0.50": coco_eval.stats[1],       # AP at IoU=0.50
        "AP@0.75": coco_eval.stats[2],       # AP at IoU=0.75
        "AP_small": coco_eval.stats[3],      # AP for small objects
        "AP_medium": coco_eval.stats[4],     # AP for medium objects
        "AP_large": coco_eval.stats[5],      # AP for large objects
        "AR@0.50:0.95 (max=1)": coco_eval.stats[6],
        "AR@0.50:0.95 (max=10)": coco_eval.stats[7],
        "AR@0.50:0.95 (max=100)": coco_eval.stats[8],
        "AR_small": coco_eval.stats[9],
        "AR_medium": coco_eval.stats[10],
        "AR_large": coco_eval.stats[11]
    }
    
    return metrics


def print_remdet_comparison(metrics, dataset_name):
    """
    æ‰“å°ä¸ RemDet çš„å¯¹æ¯”è¡¨æ ¼
    
    Args:
        metrics: COCOè¯„ä¼°æŒ‡æ ‡å­—å…¸
        dataset_name: 'VisDrone' æˆ– 'UAVDT'
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š {dataset_name} Results - RemDet Comparison")
    print("="*80)
    
    # RemDet baseline (æ ¹æ®æ•°æ®é›†é€‰æ‹©)
    if dataset_name == 'VisDrone':
        remdet_baselines = {
            'RemDet-Tiny': {'AP@0.50:0.95': 20.1, 'AP@0.50': 33.5, 'AP@0.75': 20.9, 'AP_small': 9.6, 'AP_medium': 30.4, 'AP_large': 49.7},
            'RemDet-S':    {'AP@0.50:0.95': 26.3, 'AP@0.50': 42.3, 'AP@0.75': 27.8, 'AP_small': 14.5, 'AP_medium': 39.1, 'AP_large': 55.8},
            'RemDet-M':    {'AP@0.50:0.95': 28.0, 'AP@0.50': 45.0, 'AP@0.75': 29.6, 'AP_small': 16.2, 'AP_medium': 41.5, 'AP_large': 57.3},
            'RemDet-L':    {'AP@0.50:0.95': 29.5, 'AP@0.50': 47.4, 'AP@0.75': 30.9, 'AP_small': 18.5, 'AP_medium': 43.5, 'AP_large': 58.1},
            'RemDet-X':    {'AP@0.50:0.95': 29.9, 'AP@0.50': 48.3, 'AP@0.75': 31.0, 'AP_small': 19.5, 'AP_medium': 44.1, 'AP_large': 58.6}
        }
        primary_baseline = 'RemDet-X'
    else:  # UAVDT
        remdet_baselines = {
            'RemDet-L': {'AP@0.50:0.95': 20.6, 'AP@0.50': 34.5, 'AP@0.75': 20.5, 'AP_small': 12.6, 'AP_medium': 29.0, 'AP_large': 46.8}
        }
        primary_baseline = 'RemDet-L'
    
    # æ‰“å°ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    print("\nğŸ¯ Main Metrics (vs {})".format(primary_baseline))
    print("-"*80)
    print(f"{'Metric':<20} {'YoloDepth':<15} {primary_baseline:<15} {'Î”':<15}")
    print("-"*80)
    
    main_metrics = ['AP@0.50:0.95', 'AP@0.50', 'AP@0.75', 'AP_small', 'AP_medium', 'AP_large']
    baseline = remdet_baselines[primary_baseline]
    
    for metric in main_metrics:
        our_val = metrics[metric] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        baseline_val = baseline[metric]
        delta = our_val - baseline_val
        delta_str = f"{delta:+.1f}%" if delta != 0 else "0.0%"
        
        # é¢œè‰²æ ‡è®° (ä»…åœ¨ç»ˆç«¯æ˜¾ç¤ºæ—¶æœ‰æ•ˆ)
        if delta > 0:
            delta_str = f"âœ… {delta_str}"
        elif delta < -2:
            delta_str = f"âŒ {delta_str}"
        else:
            delta_str = f"â– {delta_str}"
        
        print(f"{metric:<20} {our_val:>6.1f}%{'':<8} {baseline_val:>6.1f}%{'':<8} {delta_str}")
    
    # æ‰“å°å®Œæ•´åŸºçº¿å¯¹æ¯”
    if len(remdet_baselines) > 1:
        print("\nğŸ“‹ Full RemDet Baseline Comparison (AP@0.50)")
        print("-"*80)
        print(f"{'Model':<20} {'AP@0.50':<15} {'vs Ours':<15}")
        print("-"*80)
        for model, values in remdet_baselines.items():
            our_val = metrics['AP@0.50'] * 100
            baseline_val = values['AP@0.50']
            delta = our_val - baseline_val
            delta_str = f"{delta:+.1f}%"
            print(f"{model:<20} {baseline_val:>6.1f}%{'':<8} {delta_str}")
    
    # æ‰“å°æ¬¡è¦æŒ‡æ ‡
    print("\nğŸ“ˆ Additional Metrics")
    print("-"*80)
    print(f"{'Metric':<30} {'Value':<15}")
    print("-"*80)
    for metric in ['AR@0.50:0.95 (max=100)', 'AR_small', 'AR_medium', 'AR_large']:
        val = metrics[metric] * 100
        print(f"{metric:<30} {val:>6.1f}%")
    
    print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description='COCOæ ‡å‡†è¯„ä¼° (æ•´åˆç‰ˆ)')
    parser.add_argument('--weights', type=str, required=True, help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--data', type=str, required=True, help='æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶')
    parser.add_argument('--gt-json', type=str, required=True, help='Ground Truth COCO JSONè·¯å¾„')
    parser.add_argument('--name', type=str, default='coco_eval', help='å®éªŒåç§°')
    parser.add_argument('--imgsz', type=int, default=640, help='å›¾ç‰‡å°ºå¯¸')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--device', type=str, default='0', help='CUDAè®¾å¤‡')
    parser.add_argument('--split', type=str, default='val', choices=['val', 'test'], help='æ•°æ®é›†åˆ†å‰²')
    parser.add_argument('--save-json', action='store_true', help='ä¿å­˜predictions.jsonä¾›åç»­æ£€æŸ¥')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    save_dir = Path('runs') / 'val' / args.name
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ğŸš€ COCOæ ‡å‡†è¯„ä¼° (ä¸‰æ­¥æ³•)")
    print("="*80)
    print(f"ğŸ“ Weights:     {args.weights}")
    print(f"ğŸ“ Data YAML:   {args.data}")
    print(f"ğŸ“ GT JSON:     {args.gt_json}")
    print(f"ğŸ“ Save dir:    {save_dir}")
    print(f"ğŸ–¼ï¸  Image size:  {args.imgsz}")
    print(f"ğŸ”¢ Batch size:  {args.batch}")
    print(f"ğŸ® Device:      {args.device}")
    print("="*80 + "\n")
    
    # æ£€æŸ¥GT JSONæ˜¯å¦å­˜åœ¨
    if not Path(args.gt_json).exists():
        print(f"âŒ Error: Ground Truth JSON not found: {args.gt_json}")
        print("\nå¯ç”¨çš„GT JSONè·¯å¾„:")
        print("  VisDrone: /data2/user/2024/lzy/Datasets/VisDrone2019-DET-COCO/annotations/VisDrone2019-DET_val_coco.json")
        print("  UAVDT:    /data2/user/2024/lzy/Datasets/UAVDT/annotations/UAV-benchmark-M-Val.json")
        sys.exit(1)
    
    # =================================================================
    # Step 1: è¿è¡Œ YOLO éªŒè¯ç”Ÿæˆ predictions.json
    # =================================================================
    print("="*80)
    print("ğŸ“ Step 1/3: Running YOLO Validation to Generate predictions.json")
    print("="*80)
    
    model = YOLO(args.weights)
    
    # è¿è¡ŒéªŒè¯ (save_json=True ä¼šè‡ªåŠ¨ç”Ÿæˆ predictions.json)
    # æ³¨æ„: Ultralyticsä¼šè‡ªåŠ¨å¤„ç†é‡å¤æ–‡ä»¶å¤¹å(name, name2, name3...)
    results = model.val(
        data=args.data,
        imgsz=args.imgsz,
        batch=args.batch,
        device=args.device,
        split=args.split,
        save_json=True,  # å…³é”®: ç”ŸæˆCOCOæ ¼å¼çš„predictions.json
        project=str(save_dir.parent),
        name=args.name,
        verbose=True
    )
    
    print("   âœ… YOLO validation completed")
    
    # ä»resultså¯¹è±¡ä¸­è·å–å®é™…ä¿å­˜è·¯å¾„
    # results.save_dir åŒ…å«äº†Ultralyticså®é™…ä½¿ç”¨çš„ç›®å½•(å¯èƒ½æœ‰æ•°å­—åç¼€)
    actual_save_dir = Path(results.save_dir)
    pred_json_path = actual_save_dir / 'predictions.json'
    
    print(f"   ğŸ“‚ Actual save directory: {actual_save_dir}")
    
    if not pred_json_path.exists():
        print(f"âŒ Error: predictions.json not found at {pred_json_path}")
        print("   Please check if save_json=True worked correctly")
        sys.exit(1)
    
    print(f"   ğŸ“‚ predictions.json found: {pred_json_path}")
    
    # æ›´æ–°save_dirä¸ºå®é™…ä½¿ç”¨çš„ç›®å½•
    save_dir = actual_save_dir
    
    # =================================================================
    # Step 2: ä¿®æ­£ predictions.json çš„ image_id æ ¼å¼
    # =================================================================
    print("\n" + "="*80)
    print("ğŸ”§ Step 2/3: Fixing predictions.json image_id format")
    print("="*80)
    
    # è¯»å– GT JSON è·å–æ­£ç¡®çš„ image_id æ˜ å°„
    with open(args.gt_json, 'r') as f:
        gt_data = json.load(f)
    
    # åˆ›å»ºæ–‡ä»¶å -> image_id çš„æ˜ å°„
    # åŒæ—¶åˆ›å»ºä¸å¸¦æ‰©å±•åçš„ç‰ˆæœ¬,å› ä¸ºpredictions.jsonå¯èƒ½ä¸åŒ…å«æ‰©å±•å
    filename_to_id = {}
    stem_to_id = {}  # ä¸å¸¦æ‰©å±•åçš„æ˜ å°„
    for img in gt_data['images']:
        # GT JSON ä¸­çš„ file_name å¯èƒ½åŒ…å«è·¯å¾„æˆ–åªæœ‰æ–‡ä»¶å
        filename = Path(img['file_name']).name
        stem = Path(img['file_name']).stem  # ä¸å¸¦æ‰©å±•å
        filename_to_id[filename] = img['id']
        stem_to_id[stem] = img['id']
    
    print(f"   ğŸ“Š Loaded {len(filename_to_id)} image mappings from GT JSON")
    print(f"   ğŸ“Š Created {len(stem_to_id)} stem (no extension) mappings")
    
    # è¯»å– Ultralytics ç”Ÿæˆçš„ predictions.json
    with open(pred_json_path, 'r') as f:
        pred_data = json.load(f)
    
    print(f"   ğŸ“Š Original predictions: {len(pred_data)} detections")
    
    # ä¿®æ­£ image_id
    fixed_predictions = []
    skipped = 0
    img_id_set = set(img['id'] for img in gt_data['images'])
    
    # è°ƒè¯•: æ‰“å°å‰å‡ ä¸ªé¢„æµ‹çš„image_idæ ¼å¼
    if len(pred_data) > 0:
        print(f"   ğŸ” Sample prediction image_id formats:")
        for i, pred in enumerate(pred_data[:3]):
            print(f"      [{i}] image_id: {pred['image_id']} (type: {type(pred['image_id']).__name__})")
        print(f"   ğŸ” Sample GT filename formats:")
        for i, img in enumerate(gt_data['images'][:3]):
            print(f"      [{i}] id={img['id']}, file_name={img['file_name']}")
    
    for pred in pred_data:
        # Ultralytics çš„ predictions.json ä¸­ image_id å¯èƒ½æ˜¯æ–‡ä»¶è·¯å¾„æˆ–æ•´æ•°
        img_id = pred['image_id']
        
        # æƒ…å†µ1: å¦‚æœå·²ç»æ˜¯æ•´æ•°ä¸”åœ¨GTä¸­,ç›´æ¥ä½¿ç”¨
        if isinstance(img_id, int) and img_id in img_id_set:
            fixed_predictions.append(pred)
            continue
        
        # æƒ…å†µ2: å¦‚æœæ˜¯å­—ç¬¦ä¸²(æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å)
        if isinstance(img_id, str):
            # å…ˆå°è¯•æå–å®Œæ•´æ–‡ä»¶å(å¸¦æ‰©å±•å)
            filename = Path(img_id).name
            if filename in filename_to_id:
                pred['image_id'] = filename_to_id[filename]
                fixed_predictions.append(pred)
                continue
            
            # å¦‚æœæ²¡åŒ¹é…,å°è¯•ä¸å¸¦æ‰©å±•åçš„stem
            stem = Path(img_id).stem
            if stem in stem_to_id:
                pred['image_id'] = stem_to_id[stem]
                fixed_predictions.append(pred)
                continue
            
            # å¦‚æœè¿˜æ²¡åŒ¹é…,å°è¯•æ·»åŠ å¸¸è§æ‰©å±•å
            for ext in ['.jpg', '.png', '.jpeg']:
                test_filename = stem + ext
                if test_filename in filename_to_id:
                    pred['image_id'] = filename_to_id[test_filename]
                    fixed_predictions.append(pred)
                    break
            else:
                # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
                skipped += 1
                if skipped <= 5:
                    print(f"   âš ï¸  Cannot match image_id: {img_id}")
            continue
        
        # æƒ…å†µ3: å¦‚æœæ˜¯æ•´æ•°ä½†ä¸åœ¨GTä¸­,å°è¯•ä½œä¸ºç´¢å¼•(ä»0æˆ–1å¼€å§‹)
        if isinstance(img_id, int):
            # å°è¯•ä½œä¸º1-basedç´¢å¼•
            if 0 < img_id <= len(gt_data['images']):
                pred['image_id'] = gt_data['images'][img_id - 1]['id']
                fixed_predictions.append(pred)
                continue
            # å°è¯•ä½œä¸º0-basedç´¢å¼•
            if 0 <= img_id < len(gt_data['images']):
                pred['image_id'] = gt_data['images'][img_id]['id']
                fixed_predictions.append(pred)
                continue
        
        # æ— æ³•åŒ¹é…
        skipped += 1
        if skipped <= 5:  # åªæ‰“å°å‰5ä¸ªæ— æ³•åŒ¹é…çš„
            print(f"   âš ï¸  Cannot match image_id: {img_id}")
    
    print(f"   âœ… Fixed {len(fixed_predictions)} predictions")
    if skipped > 0:
        print(f"   âš ï¸  Skipped {skipped} predictions (no matching image in GT)")
        if skipped > 100:
            print(f"      This is unusual! Please check the image_id format.")
    
    # ä¿å­˜ä¿®æ­£åçš„ predictions.json
    fixed_pred_json_path = save_dir / 'predictions_fixed.json'
    
    if len(fixed_predictions) == 0:
        print(f"\nâŒ ERROR: No predictions could be matched to GT images!")
        print(f"   This usually means:")
        print(f"   1. The image filenames in GT JSON don't match the actual image files")
        print(f"   2. The predictions.json format is unexpected")
        print(f"\n   Please check:")
        print(f"   - Original predictions.json: {pred_json_path}")
        print(f"   - GT JSON: {args.gt_json}")
        sys.exit(1)
    
    with open(fixed_pred_json_path, 'w') as f:
        json.dump(fixed_predictions, f)
    
    print(f"   ğŸ’¾ Saved fixed predictions to: {fixed_pred_json_path}")
    
    # =================================================================
    # Step 3: ä½¿ç”¨ pycocotools è¿›è¡Œ COCO æ ‡å‡†è¯„ä¼°
    # =================================================================
    print("\n" + "="*80)
    print("ğŸ“Š Step 3/3: Evaluating with pycocotools")
    print("="*80)
    
    metrics = evaluate_with_pycocotools(args.gt_json, str(fixed_pred_json_path))
    
    if metrics is None:
        print("âŒ Evaluation failed (pycocotools not available)")
        sys.exit(1)
    
    # ç¡®å®šæ•°æ®é›†åç§°
    if 'visdrone' in args.data.lower() or 'visdrone' in args.gt_json.lower():
        dataset_name = 'VisDrone'
    elif 'uavdt' in args.data.lower() or 'uavdt' in args.gt_json.lower():
        dataset_name = 'UAVDT'
    else:
        dataset_name = 'Unknown'
    
    # æ‰“å°ä¸RemDetçš„å¯¹æ¯”
    print_remdet_comparison(metrics, dataset_name)
    
    # ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶
    metrics_file = save_dir / 'coco_metrics.json'
    with open(metrics_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"ğŸ’¾ COCO metrics saved to {metrics_file}")
    
    # ç”Ÿæˆç®€æ´çš„ç»“æœæŠ¥å‘Š
    report_file = save_dir / 'evaluation_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write(f"{dataset_name} COCO Evaluation Report\n")
        f.write("="*80 + "\n\n")
        f.write(f"Model: {args.weights}\n")
        f.write(f"Data:  {args.data}\n")
        f.write(f"GT:    {args.gt_json}\n\n")
        
        f.write("Main Metrics:\n")
        f.write("-"*80 + "\n")
        for metric in ['AP@0.50:0.95', 'AP@0.50', 'AP@0.75', 'AP_small', 'AP_medium', 'AP_large']:
            val = metrics[metric] * 100
            f.write(f"{metric:<20} {val:>6.2f}%\n")
        
        f.write("\nAdditional Metrics:\n")
        f.write("-"*80 + "\n")
        for metric in ['AR@0.50:0.95 (max=100)', 'AR_small', 'AR_medium', 'AR_large']:
            val = metrics[metric] * 100
            f.write(f"{metric:<30} {val:>6.2f}%\n")
    
    print(f"ğŸ“„ Evaluation report saved to {report_file}")
    
    # å¯é€‰: ä¿å­˜JSONä¾›æ‰‹åŠ¨æ£€æŸ¥
    if args.save_json:
        print(f"\nğŸ“¦ JSON files for manual inspection:")
        print(f"   GT:   {args.gt_json}")
        print(f"   Pred: {pred_json_path}")
    
    print("\nâœ… Evaluation completed!")
    print("="*80)
    
    # è¿”å›ä¸»è¦æŒ‡æ ‡ç”¨äºåç»­åˆ†æ
    return {
        'AP@0.50:0.95': metrics['AP@0.50:0.95'] * 100,
        'AP@0.50': metrics['AP@0.50'] * 100,
        'AP_small': metrics['AP_small'] * 100
    }


if __name__ == '__main__':
    main()
