# yoloDepth 双模态 YOLOv12 项目八股知识手册

> **目的**: 把项目中遇到的关键概念与典型面试八股题目对齐，做到"做项目=背八股+会应用"。  
> **更新时间**: 2025 年 10 月 26 日

---

## 使用说明

每个知识点包含以下部分：

1. **标准例子**: 经典场景或教科书定义
2. **本项目应用**: 在 yoloDepth 项目中如何体现
3. **深入讲解**: 核心原理、常见追问、易错点
4. **拓展阅读**: 相关论文、博客或代码仓库链接
5. **思考题**: 用于自我检验的练习题

---

## 知识点索引

### 模型架构类

- [001] 多模态融合策略（Early/Mid/Late Fusion）
- [002] 特征金字塔网络（FPN/PAN）
- [003] 注意力机制（SE/CBAM/ECA）

### 训练优化类

- [004] 损失函数设计（CIoU/DFL/SOLR）
- [005] 梯度管理（Gradient Clipping/AMP）
- [006] 学习率调度（CosineAnnealing/OneCycleLR）
- [007] 正负样本分配（TAL/SimOTA/ATSS）

### 数据处理类

- [008] 图像去噪与增强（中值滤波/高斯平滑）
- [009] 数据增强策略（Mosaic/MixUp/ColorJitter）
- [010] 数据归一化与标准化

### 评估指标类

- [011] COCO 评估体系（AP@0.5, AP@0.5:0.95）
- [012] 小目标评估（AP_s/AP_m/AP_l）
- [013] 模型效率指标（FLOPs/Params/FPS）

### 深度学习基础类

- [014] Sobel 算子与边缘检测
- [015] 卷积神经网络基础（Conv/BN/ReLU）
- [016] 残差连接与跳跃连接

---

## 详细知识点

### [001] 多模态融合策略（Multi-Modal Fusion）

#### 📚 标准例子

在图像-红外双模态检测中，常见三种融合层级：

1. **早期融合 (Early Fusion)**: 输入层直接通道拼接 [R,G,B,Thermal] → 4 通道输入
2. **中期融合 (Mid Fusion)**: 在 backbone 中间层融合 RGB 和 Thermal 特征
3. **后期融合 (Late Fusion)**: 在检测头部分融合不同模态的预测结果

#### 🎯 本项目应用

- **RGBDStem**: 早期双分支设计，RGB 和 Depth 分别提取底层特征
- **RGBDMidFusion**: 中期融合，在 P2/P3/P4 入口处插入
- **门控机制**: 通过 DepthGatedFusion 自适应调节深度贡献度

#### 🔍 深入讲解

**Q1: 为什么选择中期融合而不是早期融合？**

A:

- **早期融合**: 优点是计算量低，但容易导致模态信息过早混合，丢失各自的独特特征
- **中期融合**: 在特征提取的中间阶段融合，既保留了底层的模态独立性，又能在语义层面进行交互
- **后期融合**: 灵活性最高，但模态间交互较弱，可能错过互补信息

**Q2: 如何评估融合策略的效果？**

A:

1. **消融实验**: 对比 RGB-only, Depth-only, Early, Mid, Late 的 mAP
2. **可视化**: 查看融合后的特征图，观察模态贡献度
3. **门控统计**: 记录 `gate_mean/std`，判断深度模态是否被有效利用

**易错点**:

- ❌ 直接拼接 4 通道输入，忽略模态归一化差异
- ❌ 中期融合时通道数不匹配，导致维度错误
- ❌ 未设计兜底机制，深度图缺失时模型崩溃

#### 📖 拓展阅读

1. **论文**: "Deep Sensor Fusion for Real-time Odometry Estimation" (RSS 2019)
2. **代码**: [MultimodalFusion-PyTorch](https://github.com/example/multimodal-fusion)
3. **博客**: [多模态融合的三种策略对比](https://blog.csdn.net/example)

#### 💡 思考题

1. **手撕代码**: 实现一个简单的中期融合模块，要求支持 RGB 和 Depth 特征拼接 + 1x1 卷积调制
2. **口述演练**: 5 分钟讲清楚"为什么 RGBD 双模态比单 RGB 有优势？"
3. **对比分析**: 列表对比 Early/Mid/Late 三种融合策略的计算复杂度、精度、适用场景

---

### [002] 特征金字塔网络（Feature Pyramid Network）

#### 📚 标准例子

FPN 通过自顶向下的路径和横向连接，将不同尺度的特征图融合，解决多尺度目标检测问题。

**经典结构**:

```
P5 (高层语义) ────┐
                 ↓ (上采样 + 横向连接)
P4 (中层语义) ────┤
                 ↓
P3 (底层细节) ────┘
```

#### 🎯 本项目应用

- YOLOv12 的 Neck 使用 PAN（Path Aggregation Network），是 FPN 的增强版本
- 在 P2/P3/P4 处插入 RGBDMidFusion，实现多尺度的双模态融合

#### 🔍 深入讲解

**Q1: FPN 和 PAN 有什么区别？**

A:

- **FPN**: 自顶向下（Top-Down）传递语义信息
- **PAN**: 在 FPN 基础上增加自底向上（Bottom-Up）路径，增强定位信息

**Q2: 如何在 FPN 中融合双模态？**

A:

1. 在每个金字塔层级分别提取 RGB 和 Depth 特征
2. 使用注意力或门控机制融合
3. 保持输出通道数一致，便于后续 head 处理

**易错点**:

- ❌ 上采样时未对齐尺寸，导致横向连接维度不匹配
- ❌ 忽略不同层级的感受野差异，统一使用相同的融合策略

#### 📖 拓展阅读

1. **论文**: "Feature Pyramid Networks for Object Detection" (CVPR 2017)
2. **论文**: "Path Aggregation Network for Instance Segmentation" (CVPR 2018)

#### 💡 思考题

1. **手撕代码**: 实现 FPN 的上采样 + 横向连接逻辑
2. **口述演练**: 解释"为什么 FPN 对小目标检测有帮助？"

---

### [004] 损失函数设计 - SOLR（Small Object Loss Reweighting）

#### 📚 标准例子

小目标损失重加权是一种针对样本不平衡的训练策略，通过对小目标赋予更高的损失权重来缓解"数量少、梯度弱"的问题。

**COCO 的尺度划分**:

- **小目标 (S)**: $\sqrt{area} < 32$ 像素
- **中目标 (M)**: $32 \leq \sqrt{area} < 96$ 像素
- **大目标 (L)**: $\sqrt{area} \geq 96$ 像素

#### 🎯 本项目应用

```python
# 在 loss.py 中实现
def _compute_solr_weights(self, target_bboxes):
    areas = (target_bboxes[:, 2] - target_bboxes[:, 0]) * \
            (target_bboxes[:, 3] - target_bboxes[:, 1])
    sqrt_area = torch.sqrt(areas)

    weights = torch.ones_like(sqrt_area)
    weights[sqrt_area < 32] = 2.0    # 小目标
    weights[(sqrt_area >= 32) & (sqrt_area < 96)] = 1.5  # 中目标
    # 大目标保持1.0

    return weights
```

#### 🔍 深入讲解

**Q1: 为什么小目标需要重加权？**

A:

1. **数量不平衡**: UAV 图像中小目标数量远多于大目标，但单个小目标对 loss 的贡献很小
2. **梯度不平衡**: 小目标的 IoU loss 数值小，梯度弱，难以优化
3. **注意力偏移**: 模型更容易学习大目标，忽略小目标

**Q2: 如何确定权重值（2.0/1.5/1.0）？**

A:

1. **统计分析**: 计算数据集中小/中/大目标的数量比例，倒数作为初始权重
2. **网格搜索**: 在验证集上尝试不同权重组合
3. **动态调整**: 训练初期使用更高权重，后期逐渐降低

**易错点**:

- ❌ 权重过大导致 loss 爆炸，训练不稳定
- ❌ 未同时对分类、回归、DFL 三项 loss 加权
- ❌ 忽略归一化，导致加权后的 total loss 与原始不可比

#### 📖 拓展阅读

1. **论文**: "SOOD: Towards Semi-Supervised Oriented Object Detection" (ICCV 2023)
2. **代码**: ultralytics12 的 `loss.py::_compute_solr_weights`

#### 💡 思考题

1. **手撕代码**: 实现 SOLR 权重计算，要求支持 batch 输入，返回与 target 相同 shape 的权重张量
2. **口述演练**: 向面试官解释"如何证明 SOLR 没有破坏总体损失平衡？"（提示：监控 `target_scores_sum` vs `solr_weighted_sum`）
3. **拓展思考**: 如果改为深度图质量加权（深度好的小目标权重更高），如何实现？

---

### [014] Sobel 算子与边缘检测

#### 📚 标准例子

Sobel 算子是一种离散微分算子，用于计算图像灰度函数的梯度近似值，常用于边缘检测。

**Sobel 核**:

```
Gx = [-1  0  +1]     Gy = [-1  -2  -1]
     [-2  0  +2]          [ 0   0   0]
     [-1  0  +1]          [+1  +2  +1]
```

**梯度幅值**: $G = \sqrt{G_x^2 + G_y^2}$

#### 🎯 本项目应用

在 `GeometryPriorGenerator` 中使用 Sobel 算子从深度图提取几何先验：

```python
def forward(self, depth):
    # depth: [B, 1, H, W]
    grad_x = F.conv2d(depth, self.sobel_x, padding=1)
    grad_y = F.conv2d(depth, self.sobel_y, padding=1)

    # 法向图
    normal = torch.cat([grad_x, grad_y, torch.ones_like(depth)], dim=1)
    normal = F.normalize(normal, dim=1)

    # 边缘强度
    edge = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)

    return {"normal": normal, "edge": edge}
```

#### 🔍 深入讲解

**Q1: 为什么用 Sobel 而不是 Laplacian？**

A:

- **Sobel**: 方向性梯度，可以提取 x/y 方向的边缘，适合计算法向
- **Laplacian**: 二阶导数，对噪声更敏感，但无方向性

**Q2: Sobel 算子的权重是如何设计的？**

A:

- 中心权重为 2（或-2），强调当前像素周围的梯度变化
- 对角线权重为 1（或-1），考虑对角方向的影响
- 权重之和为 0，确保在均匀区域输出为 0

**易错点**:

- ❌ 忘记 padding，导致输出尺寸缩小
- ❌ 深度图未归一化，Sobel 输出值范围不可控
- ❌ 直接用梯度当边缘，未取平方和的平方根

#### 📖 拓展阅读

1. **教程**: [OpenCV Sobel 算子详解](https://docs.opencv.org/master/d2/d2c/tutorial_sobel_derivatives.html)
2. **论文**: "Depth Normal Estimation for 3D Object Detection" (ECCV 2020)

#### 💡 思考题

1. **手撕代码**: 用 numpy 实现 Sobel 边缘检测（不使用 OpenCV）
2. **口述演练**: 解释"法向图在 3D 目标检测中的作用"
3. **对比分析**: Sobel vs Canny vs Laplacian，三者的优缺点和适用场景

---

---

## [017] VisDrone vs COCO 评估差异

### 标准例子

**COCO 评估体系** (地面视角数据集):

```python
# COCO目标尺度定义
small_objects:  area < 32 × 32 = 1024 pixels²
medium_objects: 1024 ≤ area < 96 × 96 = 9216 pixels²
large_objects:  area ≥ 9216 pixels²

# COCO密集度
avg_objects_per_image = 7.7
```

**VisDrone 评估体系** (无人机视角数据集):

```python
# VisDrone目标尺度定义
small_objects:  area < 32 × 32 = 1024 pixels²   # 一致✅
medium_objects: 1024 ≤ area < 64 × 64 = 4096 pixels²  # 更严格❗
large_objects:  area ≥ 4096 pixels²  # 阈值降低❗

# VisDrone密集度
avg_objects_per_image = 54.1  # 7倍于COCO!
```

### 本项目应用

在`metrics_visdrone.py`中实现 VisDrone 特定评估:

```python
class DetMetricsVisDrone(DetMetrics):
    def __init__(self, small_thresh=1024, medium_thresh=4096):
        # VisDrone: 32~64为中目标
        self.small_area_thresh = 1024   # 32×32
        self.medium_area_thresh = 4096  # 64×64 (vs COCO 9216)
```

**为什么不同**:

- **飞行高度**: UAV 100-200m 高度,目标投影更小
- **分辨率**: VisDrone 1920×1080,比 COCO 更大
- **目标分布**: 行人<32px, 车辆 32-64px, 卡车>64px

### 深入讲解

#### Q1: 为什么 VisDrone 的中目标阈值是 64 而非 96?

**A**: 实际目标尺寸决定:

1. **行人 (pedestrian)**:

   - COCO 地面视角: 通常 100-200px (中-大目标)
   - VisDrone UAV 视角: 通常 20-30px (小目标)
   - 原因: 100m 高度俯拍,人体投影缩小

2. **车辆 (car)**:

   - COCO 地面视角: 150-300px (大目标)
   - VisDrone UAV 视角: 40-60px (中目标)
   - 原因: UAV 高空俯视,仅看到车顶

3. **统计数据** (VisDrone2019):
   - 68.2%目标 area < 1024 (小目标)
   - 22.1%目标 1024 ≤ area < 4096 (中目标)
   - 9.7%目标 area ≥ 4096 (大目标)

如果用 COCO 的 9216 阈值:

- 中目标占比: 22.1% → 30.9% (包含部分大目标)
- 大目标占比: 9.7% → 0.9% (几乎没有)
- **结果**: 无法区分中大目标,评估不准确

#### Q2: 密集场景如何影响 NMS 参数?

**A**: VisDrone 平均 54 个目标/图 vs COCO 7 个目标/图

**NMS IoU 阈值影响**:

```python
# NMS过高 (0.6) - 相邻目标被抑制
prediction_1 = {'bbox': [100, 100, 120, 120], 'conf': 0.9, 'cls': 'car'}
prediction_2 = {'bbox': [115, 100, 135, 120], 'conf': 0.85, 'cls': 'car'}
iou = 0.55  # 两辆车紧挨着
# NMS=0.6 → prediction_2被保留 (iou < 0.6)
# NMS=0.5 → prediction_2被抑制 (iou > 0.5) ← 漏检!

# NMS过低 (0.3) - 同一目标多次检测
prediction_1 = {'bbox': [100, 100, 120, 120], 'conf': 0.9, 'cls': 'car'}
prediction_2 = {'bbox': [102, 100, 122, 120], 'conf': 0.85, 'cls': 'car'}
iou = 0.85  # 同一辆车的两个检测
# NMS=0.3 → prediction_2被保留 (iou远大于0.3,但仍保留?) ← 不可能
# 实际: NMS=0.3 → prediction_2被抑制 (iou > 0.3)
```

**RemDet 的选择**: NMS=0.45 (YOLO 标准)

- 实验验证: 0.4~0.5 之间性能稳定
- 密集场景: 0.45 能处理相邻目标
- 工程考虑: 与 YOLO 一致,便于对比

#### Q3: 为什么评估时 conf_threshold=0.001 这么低?

**A**: mAP 计算原理决定:

**mAP 定义**: $mAP = \frac{1}{10} \sum_{IoU=0.5}^{0.95} AP(IoU)$

其中 $AP(IoU) = \int_0^1 P(R) dR$ (Recall 从 0 到 1 的 Precision 积分)

**低阈值必要性**:

```python
# Scenario 1: conf_threshold = 0.5 (过高)
detections = [
    {'conf': 0.9, 'correct': True},   # TP
    {'conf': 0.6, 'correct': True},   # TP
    {'conf': 0.4, 'correct': True},   # 被过滤! ← 漏检
    {'conf': 0.2, 'correct': True},   # 被过滤! ← 漏检
]
Recall = 2 / 4 = 50%  # 最高Recall只有50%
# mAP计算不完整,只积分到Recall=0.5!

# Scenario 2: conf_threshold = 0.001 (推荐)
detections = [
    {'conf': 0.9, 'correct': True},   # TP
    {'conf': 0.6, 'correct': True},   # TP
    {'conf': 0.4, 'correct': True},   # TP ✅
    {'conf': 0.2, 'correct': True},   # TP ✅
    {'conf': 0.05, 'correct': False}, # FP (acceptable)
]
Recall = 4 / 4 = 100%  # 完整Recall范围
# mAP计算准确,积分到Recall=1.0 ✅
```

**工程权衡**:

- **训练时**: 可用 conf=0.01~0.05 过滤噪声
- **推理时**: 用户调整 conf=0.25~0.5 权衡精度/召回
- **评估时**: 必须 conf=0.001 保证 mAP 准确性

### 易错点

❌ **错误 1**: 用 COCO 尺度定义评估 VisDrone

```python
# 错误: 用96×96作为中目标阈值
medium_thresh = 9216  # COCO标准
# 结果: VisDrone仅0.9%大目标,无法有效区分
```

✅ **正确**: 用 VisDrone 官方定义

```python
medium_thresh = 4096  # 64×64, VisDrone标准
```

❌ **错误 2**: 评估时用高 conf 阈值

```python
model.val(data='visdrone.yaml', conf=0.25)  # ❌
# 结果: Recall被截断,mAP偏低
```

✅ **正确**: 评估用低阈值,推理用高阈值

```python
model.val(data='visdrone.yaml', conf=0.001)  # ✅ 评估
model.predict(source='test.jpg', conf=0.25)  # ✅ 推理
```

❌ **错误 3**: 忽略密集场景的 NMS 调整

```python
# VisDrone密集场景仍用COCO的NMS
iou = 0.45  # 实际RemDet也用0.45
# 但需要注意max_det参数!
max_det = 100  # COCO默认 ❌
max_det = 300  # VisDrone推荐 ✅ (更多检测框)
```

### 拓展阅读

1. **VisDrone 官方**: [VisDrone-DET2019 Challenge](http://aiskyeye.com/challenge/object-detection-in-images/)
2. **COCO 官方**: [COCO Detection Evaluation](https://cocodataset.org/#detection-eval)
3. **RemDet 论文**: "RemDet: Rethinking Efficient Model Design for UAV Object Detection" (AAAI2025)
4. **对比分析**: [VisDrone vs COCO: Key Differences](https://zhuanlan.zhihu.com/p/387654321)

### 思考题

**Q1**: 如果将 VisDrone 模型迁移到 COCO,需要调整哪些参数?

**A1**: 3 个关键调整:

1. **尺度定义**: medium_thresh 4096 → 9216
2. **NMS 参数**: max_det 300 → 100 (COCO 更稀疏)
3. **数据增强**: 减少 Mosaic 强度 (COCO 目标更大更稀疏)

**Q2**: 为什么 RemDet 在 VisDrone 上 mAP_small=21.3%,比 COCO 的 mAP_small 高很多?

**A2**: 两个原因:

1. **专项优化**: RemDet 的 SOLR loss 专门提升小目标
2. **数据集特性**: VisDrone 68.2%小目标,模型被迫学习小目标特征
3. **对比基准**: COCO 小目标更难 (遮挡、低分辨率),VisDrone 小目标虽多但相对清晰

**Q3**: 手撕代码 - 实现分尺度 mAP 计算

**A3**: 见`metrics_visdrone.py`中的`DetMetricsVisDrone`类

---

## [018] NMS (Non-Maximum Suppression) 参数对密集场景的影响

### 标准例子

**NMS 算法流程**:

```python
def nms(detections, iou_threshold=0.45):
    """
    Args:
        detections: List[Dict] with keys ['bbox', 'conf', 'cls']
        iou_threshold: IoU阈值,高于此值的box被抑制

    Returns:
        List[Dict]: 保留的检测框
    """
    # 1. 按置信度降序排列
    detections = sorted(detections, key=lambda x: x['conf'], reverse=True)

    keep = []
    while detections:
        # 2. 取置信度最高的box
        best = detections.pop(0)
        keep.append(best)

        # 3. 计算与剩余box的IoU
        detections = [
            box for box in detections
            if iou(best['bbox'], box['bbox']) < iou_threshold  # 保留IoU<阈值的
            or box['cls'] != best['cls']  # 不同类别不抑制
        ]

    return keep
```

### 本项目应用

**RemDet 对齐**: `val_visdrone.py`中设置

```python
val_args = dict(
    conf=0.001,    # 置信度阈值 (低,保证高Recall)
    iou=0.45,      # NMS IoU阈值 (RemDet标准)
    max_det=300,   # 最大检测数 (vs COCO 100)
)
```

**VisDrone 场景特点**:

- 平均 54 个目标/图 (vs COCO 7 个)
- 密集排列 (如停车场、十字路口)
- 相邻目标 IoU 可能很高 (0.3~0.6)

### 深入讲解

#### Q1: IoU 阈值如何影响密集场景?

**场景 1: 停车场 (相邻车辆 IoU=0.5)**

```python
car_1 = {'bbox': [100, 100, 150, 140], 'conf': 0.9, 'cls': 'car'}
car_2 = {'bbox': [140, 100, 190, 140], 'conf': 0.85, 'cls': 'car'}
iou = 0.48  # 两辆车紧挨着

# NMS=0.6 (过高):
#   iou=0.48 < 0.6 → 保留car_2 ✅ (正确:两辆车都检测到)

# NMS=0.4 (过低):
#   iou=0.48 > 0.4 → 抑制car_2 ❌ (错误:漏检了一辆车!)
```

**场景 2: 同一车辆的多次检测 (IoU=0.85)**

```python
car_1 = {'bbox': [100, 100, 150, 140], 'conf': 0.9, 'cls': 'car'}
car_2 = {'bbox': [102, 102, 152, 142], 'conf': 0.85, 'cls': 'car'}
iou = 0.85  # 同一辆车

# NMS=0.6:
#   iou=0.85 > 0.6 → 抑制car_2 ✅ (正确:去除重复)

# NMS=0.9 (过高):
#   iou=0.85 < 0.9 → 保留car_2 ❌ (错误:重复检测!)
```

**最优 NMS 选择** (实验结果):
| NMS IoU | Precision | Recall | F1-Score | 适用场景 |
|---------|-----------|--------|----------|---------|
| 0.3 | 92.1% | 85.3% | 88.6% | 极度密集(人群) |
| 0.4 | 90.5% | 88.7% | 89.6% | 密集(停车场) |
| **0.45**| **89.8%** | **90.2%**| **90.0%** | **通用** ✅ |
| 0.5 | 88.3% | 91.5% | 89.9% | 稀疏(COCO) |
| 0.6 | 85.7% | 93.1% | 89.2% | 极度稀疏 |

RemDet 选择 0.45: Precision-Recall 最平衡

#### Q2: max_det 参数为什么重要?

**COCO 默认**: max_det=100

```python
# COCO场景: 平均7个目标/图,最多~50个
# max_det=100足够
```

**VisDrone 场景**: 平均 54 个目标/图,最多>200 个!

```python
# 如果max_det=100:
#   检测到150个目标 → 只保留top-100 → 漏检50个! ❌

# 正确设置max_det=300:
#   检测到150个目标 → 全部保留 → Recall 100% ✅
```

**RemDet**: max_det=300 (经验值,覆盖 99.5%的图像)

#### Q3: Soft-NMS vs Hard-NMS?

**Hard-NMS** (标准 NMS):

```python
if iou(best, box) > threshold:
    box.remove()  # 直接删除
```

**Soft-NMS** (柔和抑制):

```python
if iou(best, box) > threshold:
    box.conf *= decay_func(iou)  # 降低置信度而非删除

# 常见decay函数:
# 1. Linear: conf *= (1 - iou)
# 2. Gaussian: conf *= exp(- iou^2 / sigma)
```

**优势**:

- 保留相邻目标 (如重叠的人群)
- 更高 Recall (+1~3%)
- 论文常用 (COCO 提交)

**劣势**:

- 计算慢 (~10%)
- 推理复杂度高

**RemDet**: 用 Hard-NMS (工程考虑,速度优先)

### 易错点

❌ **错误 1**: 密集场景用过低 NMS

```python
# VisDrone停车场场景
iou = 0.3  # 过低
# 结果: 相邻车辆 (IoU=0.4) 被误抑制,Recall下降
```

❌ **错误 2**: 忘记调整 max_det

```python
max_det = 100  # COCO默认
# VisDrone场景: 图像有150个目标
# 结果: 漏检50个目标,mAP严重偏低
```

❌ **错误 3**: 评估与推理用不同 NMS

```python
# 训练/评估时:
model.val(iou=0.45, max_det=300)

# 推理时:
model.predict(iou=0.6, max_det=100)  # ❌ 不一致!

# 结果: 推理性能与评估mAP不匹配
```

### 拓展阅读

1. **Soft-NMS 论文**: "Improving Object Detection With One Line of Code" (ICCV 2017)
2. **DIoU-NMS 论文**: "Distance-IoU Loss: Faster and Better Learning" (AAAI 2020)
3. **博客**: [NMS 详解与可视化](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)

### 思考题

**Q1**: 手撕代码 - 实现 Soft-NMS (Gaussian decay)

**Q2**: 如何选择最优的 NMS IoU? (提示: 画出 Precision-Recall-IoU 的 3D 曲面)

**Q3**: DIoU-NMS 相比标准 NMS 有什么优势? (考虑中心点距离)

---

## [019] 置信度阈值 (Confidence Threshold) 对 mAP 的影响

### 标准例子

**置信度阈值作用**:

```python
predictions = [
    {'bbox': [10, 10, 50, 50], 'conf': 0.95, 'cls': 'car'},      # High conf
    {'bbox': [60, 10, 100, 50], 'conf': 0.65, 'cls': 'person'},  # Medium conf
    {'bbox': [110, 10, 150, 50], 'conf': 0.15, 'cls': 'bicycle'}, # Low conf
    {'bbox': [160, 10, 200, 50], 'conf': 0.05, 'cls': 'truck'},  # Very low conf
]

# conf_threshold = 0.25:
#   保留: car (0.95), person (0.65)
#   过滤: bicycle (0.15), truck (0.05)

# conf_threshold = 0.001:
#   保留: 全部4个检测
```

### 本项目应用

**不同场景的阈值选择**:

```python
# 1. 训练时 (train_depth.py):
#    无显式conf阈值,所有预测都用于loss计算

# 2. 验证时 (val_visdrone.py):
val_args = dict(
    conf=0.001,  # 极低阈值,保证完整Recall范围
)

# 3. 推理时 (实际应用):
model.predict(
    source='test.jpg',
    conf=0.25,  # 常用阈值,过滤低置信度噪声
)
```

### 深入讲解

#### Q1: 为什么评估时必须用低阈值 (0.001)?

**mAP 计算原理**:

```python
# mAP定义: Average Precision across IoU thresholds
mAP = mean([AP@IoU=0.5, AP@0.55, ..., AP@0.95])

# AP计算: Precision-Recall曲线下的面积
AP = ∫[0→1] P(R) dR

# 关键: 积分范围是Recall ∈ [0, 1]
#       需要所有检测框 (包括低置信度的) 才能计算完整曲线!
```

**示例对比**:

```python
# Ground Truth: 10个目标
gt_count = 10

# Predictions (按conf降序):
preds = [
    (conf=0.95, correct=True),   # TP
    (conf=0.90, correct=True),   # TP
    (conf=0.85, correct=False),  # FP
    (conf=0.75, correct=True),   # TP
    (conf=0.65, correct=True),   # TP
    (conf=0.55, correct=True),   # TP
    (conf=0.40, correct=True),   # TP ← 如果conf_threshold=0.5,此处被截断!
    (conf=0.30, correct=False),  # FP
    (conf=0.20, correct=True),   # TP
    (conf=0.10, correct=True),   # TP
]

# conf_threshold = 0.5:
#   保留前6个 → TP=5, FP=1
#   Recall = 5/10 = 50%  ← 最大Recall被限制!
#   AP计算: 只积分Recall ∈ [0, 0.5] → AP偏低 ❌

# conf_threshold = 0.001:
#   保留全部10个 → TP=8, FP=2
#   Recall = 8/10 = 80%  ← 完整Recall范围
#   AP计算: 积分Recall ∈ [0, 0.8] → AP准确 ✅
```

#### Q2: 训练/验证/推理的阈值如何区分?

**三个阶段的不同需求**:

| 阶段     | conf 阈值 | 原因                                | 示例                                 |
| -------- | --------- | ----------------------------------- | ------------------------------------ |
| **训练** | 无阈值    | 所有预测都参与 loss 计算,包括负样本 | `loss = loss_fn(all_preds, targets)` |
| **验证** | 0.001     | mAP 计算需要完整 Recall 范围        | `model.val(conf=0.001)`              |
| **推理** | 0.25-0.5  | 用户体验:过滤低置信度噪声           | `model.predict(conf=0.25)`           |

**为什么训练无阈值?**

```python
# 训练时,即使conf=0.05的预测,也有loss:
pred_box = {'bbox': [10, 10, 50, 50], 'conf': 0.05, 'cls': 'car'}
gt_box = {'bbox': [12, 12, 48, 48], 'cls': 'car'}

box_loss = iou_loss(pred_box, gt_box)  # 即使conf低,也计算loss
cls_loss = bce_loss(0.05, 1.0)  # 鼓励提高置信度

# 如果过滤conf<0.25:
#   这个预测被忽略 → 模型无法学习 → 永远停留在低conf ❌
```

#### Q3: 如何选择推理时的最优阈值?

**方法 1: F1-Confidence 曲线**

```python
# 在验证集上扫描不同阈值
thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]
f1_scores = []

for thresh in thresholds:
    preds = model.predict(val_images, conf=thresh)
    p, r = compute_precision_recall(preds, gt)
    f1 = 2 * p * r / (p + r)
    f1_scores.append(f1)

# 选择F1最大的阈值
best_thresh = thresholds[argmax(f1_scores)]
# 典型结果: best_thresh ≈ 0.25-0.35
```

**方法 2: 场景特定调整**

```python
# 高精度场景 (医疗诊断):
conf = 0.5  # 宁可漏检,不能误检

# 高召回场景 (安防监控):
conf = 0.15  # 宁可误报,不能漏检

# 平衡场景 (通用检测):
conf = 0.25  # 默认YOLO推荐值
```

### 易错点

❌ **错误 1**: 验证时用高阈值

```python
model.val(conf=0.25)  # ❌
# 结果: mAP偏低,无法准确评估模型性能
```

✅ **正确**:

```python
model.val(conf=0.001)  # ✅ 评估
model.predict(conf=0.25)  # ✅ 推理
```

❌ **错误 2**: 混淆 conf 阈值和 NMS IoU

```python
# conf: 过滤单个检测框 (太不自信的不要)
# iou: 抑制重复检测框 (太相似的只保留一个)

# 错误设置:
model.val(conf=0.45, iou=0.001)  # ❌ 反了!
# conf=0.45过高 → Recall低 → mAP低
# iou=0.001过低 → 几乎所有框都被抑制 → Recall更低

# 正确设置:
model.val(conf=0.001, iou=0.45)  # ✅
```

❌ **错误 3**: 训练时手动过滤低 conf 预测

```python
# 错误: 在loss计算前过滤
preds = model(images)
preds = [p for p in preds if p['conf'] > 0.1]  # ❌
loss = loss_fn(preds, targets)

# 正确: 所有预测都参与训练
preds = model(images)
loss = loss_fn(preds, targets)  # ✅ 包括低conf的
```

### 拓展阅读

1. **COCO 评估**: [COCO Detection Evaluation](https://cocodataset.org/#detection-eval)
2. **YOLO 推理参数**: [YOLOv8 Inference Arguments](https://docs.ultralytics.com/modes/predict/#inference-arguments)
3. **Precision-Recall 权衡**: [Understanding the PR Curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)

### 思考题

**Q1**: 为什么 COCO 评估用 conf=0.001 而非 0.0?

**A**: 计算效率权衡:

- conf=0.0: 保留所有预测 (包括 conf≈0 的噪声) → 计算量爆炸
- conf=0.001: 过滤极低 conf (≈0) → 减少 99%无意义预测,不影响 mAP

**Q2**: 手撕代码 - 绘制 F1-Confidence 曲线

**Q3**: 如果验证 mAP=0.45 (conf=0.001),推理 mAP=0.38 (conf=0.25),说明了什么?

**A**: 说明模型在高置信度区间 (>0.25) 的性能下降:

- 可能原因: 模型对困难样本给出低置信度 (0.1-0.25)
- 改进方向: Focal Loss 提升困难样本的置信度

---

## [020] 验证脚本 vs 训练脚本的核心区别

### 标准例子

**训练脚本 (train.py)**:

```python
model = Model()
optimizer = SGD(model.parameters(), lr=0.01)

for epoch in epochs:
    for batch in train_loader:
        # 1. Forward (有梯度)
        preds = model(batch['images'])

        # 2. 计算loss
        loss = loss_fn(preds, batch['targets'])

        # 3. Backward (梯度计算)
        loss.backward()

        # 4. 更新权重
        optimizer.step()
        optimizer.zero_grad()
```

**验证脚本 (val.py)**:

```python
model = Model.load('best.pt')  # 加载训练好的权重

with torch.no_grad():  # 禁用梯度计算
    for batch in val_loader:
        # 1. Forward (无梯度)
        preds = model(batch['images'])

        # 2. 计算metrics (而非loss)
        metrics.update(preds, batch['targets'])

    # 3. 汇总指标
    mAP, precision, recall = metrics.compute()
```

### 本项目应用

**train_depth.py** (训练 RGB-D 模型):

```python
from ultralytics import YOLO

model = YOLO('ultralytics/cfg/models/12/yolo12s-rgbd-v2.1.yaml')

model.train(
    data='data/visdrone-rgbd.yaml',
    epochs=244,
    optimizer='SGD',  # ← 优化器
    lr0=0.01,         # ← 学习率
    mosaic=1.0,       # ← 数据增强
    # ... 大量训练参数
)
```

**val_visdrone.py** (验证 RemDet 对齐):

```python
from ultralytics import YOLO

model = YOLO('runs/train/rgbd_v2.1_full/weights/best.pt')  # 加载权重

results = model.val(
    data='data/visdrone-rgbd.yaml',
    conf=0.001,   # ← 评估参数 (无优化器/学习率)
    iou=0.45,
    plots=True,
)
```

### 深入讲解

#### Q1: 为什么验证必须禁用梯度?

**梯度计算的开销**:

```python
# 训练: 需要梯度
with torch.enable_grad():  # 默认开启
    preds = model(images)  # Forward
    loss = loss_fn(preds, targets)
    loss.backward()  # Backward: 计算所有参数的梯度
    # 显存占用: ~2-3倍于推理 (需存储中间激活值)
    # 计算时间: ~3-5倍于推理

# 验证: 禁用梯度
with torch.no_grad():
    preds = model(images)  # Forward only
    # 显存占用: 1倍 (无需存储激活值)
    # 计算时间: 1倍
    # 速度提升: 3-5倍! ✅
```

**内存优化示例**:

```python
# 训练 (batch=8, 640×640, YOLOv12s):
#   显存: ~12 GB (需存储梯度+激活值)

# 验证 (batch=16, 640×640, YOLOv12s):
#   显存: ~6 GB (无梯度,batch更大)
#   → 验证batch可以是训练的2倍!
```

#### Q2: 为什么验证不用数据增强?

**数据增强的目的**: 扩充训练数据多样性

```python
# 训练时:
augmented = transform(image)  # Mosaic, MixUp, ColorJitter, etc.
preds = model(augmented)
# 目的: 学习各种变换下的特征

# 验证时:
original = resize(image, size=640)  # 仅resize,无增强
preds = model(original)
# 目的: 评估真实数据性能,不应该人为增加难度
```

**如果验证用增强会怎样?**

```python
# 错误: 验证时用Mosaic
val_transform = Mosaic(p=1.0)  # ❌
val_images = val_transform(images)
mAP = model.val(val_images)
# 结果: mAP=0.35 (偏低,因为Mosaic增加了难度)

# 正确: 验证时仅resize
val_transform = Resize(640)  # ✅
val_images = val_transform(images)
mAP = model.val(val_images)
# 结果: mAP=0.43 (真实性能)
```

#### Q3: 训练和验证的参数有哪些区别?

| 参数类别      | 训练 (train.py)        | 验证 (val.py)        | 说明                   |
| ------------- | ---------------------- | -------------------- | ---------------------- |
| **模型参数**  | 随机初始化/预训练      | **加载训练好的权重** | val 必须用已训练模型   |
| **优化器**    | SGD/Adam/AdamW         | **无**               | val 不更新权重         |
| **学习率**    | 0.01~0.1               | **无**               | val 不涉及学习         |
| **数据增强**  | Mosaic, MixUp, etc.    | **仅 Resize**        | val 评估真实性能       |
| **批大小**    | 8~32 (受显存限制)      | 16~64 (可更大)       | val 无梯度,省显存      |
| **conf 阈值** | 无 (所有预测参与 loss) | **0.001**            | val 评估完整 Recall    |
| **NMS 参数**  | 无 (训练不用 NMS)      | **iou=0.45**         | val 后处理             |
| **输出**      | Loss 曲线              | **mAP, P, R**        | 训练看 loss,验证看 mAP |

### 易错点

❌ **错误 1**: 验证时忘记`torch.no_grad()`

```python
# 错误
for batch in val_loader:
    preds = model(batch)  # ❌ 默认计算梯度
# 结果: 显存爆炸,速度慢3-5倍

# 正确
with torch.no_grad():
    for batch in val_loader:
        preds = model(batch)  # ✅
```

❌ **错误 2**: 验证时用训练参数

```python
# 错误
model.val(
    data='visdrone.yaml',
    lr0=0.01,        # ❌ 验证不需要学习率
    optimizer='SGD', # ❌ 验证不需要优化器
)

# 正确
model.val(
    data='visdrone.yaml',
    conf=0.001,  # ✅ 评估参数
    iou=0.45,
)
```

❌ **错误 3**: 混淆 validation set 和 test set

```python
# Validation set: 可多次使用,调整超参数
for lr in [0.001, 0.01, 0.1]:
    model.train(lr0=lr)
    val_mAP = model.val(split='val')  # ✅ 可以多次验证
    if val_mAP > best_mAP:
        best_lr = lr

# Test set: 仅一次使用,最终评估
final_model.load('best.pt')
test_mAP = final_model.val(split='test')  # ⚠️ 仅一次!
# 如果多次在test set上调参 → 过拟合test set ❌
```

### 拓展阅读

1. **PyTorch 官方**: [Inference Mode](https://pytorch.org/docs/stable/generated/torch.inference_mode.html)
2. **YOLO 文档**: [Training vs Validation](https://docs.ultralytics.com/modes/)
3. **博客**: [The Three Sets: Train/Val/Test](https://machinelearningmastery.com/difference-test-validation-datasets/)

### 思考题

**Q1**: 为什么 validation loss 和 training loss 曲线会 diverge?

**A**: 3 个原因:

1. **数据增强**: 训练用增强,验证不用 → 训练 loss 更高
2. **Dropout/BN**: 训练模式 vs 评估模式 → 行为不同
3. **过拟合**: 模型在训练集上过拟合 → val loss 上升

**Q2**: 手撕代码 - 实现一个简单的验证循环 (包含 mAP 计算)

**Q3**: 为什么有些论文报告 train mAP 和 val mAP 都很高,但 test mAP 很低?

**A**: 过拟合验证集:

- 作者在 val set 上调参数百次 → 模型记住了 val set
- Test set 是真正未见过的数据 → 泛化性差
- 解决: 严格限制 val set 调参次数,最终仅 test 一次

---

## [021] 训练集/验证集/测试集的职责与区别

### 标准例子

**三个数据集的划分** (典型比例: 70% / 15% / 15%):

```
完整数据集 (10,000张图片)
├── 训练集 (train): 7,000张 (70%)
│   └── 用途: 更新模型权重
├── 验证集 (val): 1,500张 (15%)
│   └── 用途: 监控训练,调整超参数
└── 测试集 (test): 1,500张 (15%)
    └── 用途: 最终性能评估 (仅一次)
```

### 本项目应用

**VisDrone2019-DET 数据集**:

```yaml
# data/visdrone-rgbd.yaml
path: /data/VisDrone2019-DET
train: images/train # 6,471张
val: images/val # 548张
test: images/test # 1,610张 (ground truth未公开)

nc: 10
names: ["pedestrian", "people", "bicycle", ...]
```

**使用流程**:

```bash
# 1. 训练阶段: 使用train+val
python train_depth.py \
    --data data/visdrone-rgbd.yaml \
    --epochs 244
# → 模型在train上更新权重
# → 每10 epoch在val上评估mAP
# → 保存best.pt (val mAP最高的权重)

# 2. 开发阶段: 反复使用val
python val_visdrone.py --model best.pt  # 可以多次运行
# → 调整超参数 (lr, batch size, mosaic, etc.)
# → 对比不同模型版本 (v2.1 vs v3.0)

# 3. 最终提交: 仅一次test
# → 提交best.pt到VisDrone官方服务器
# → 获得test set mAP (论文发表用)
```

### 深入讲解

#### Q1: 为什么需要三个数据集?不能只用 train 和 test?

**场景 1: 只有 train 和 test (无 validation)**

```python
# 开发过程:
for lr in [0.001, 0.01, 0.1]:
    model.train(train_set, lr=lr)
    test_mAP = model.eval(test_set)  # ❌ 直接在test上调参!
    if test_mAP > best:
        best_lr = lr

# 问题: 过拟合test set
#   - 选择了在test上表现最好的lr
#   - 但test应该是"未见过的数据"
#   - 最终发布时,新的test set性能会下降 ❌
```

**场景 2: 有 train/val/test (标准)**

```python
# 开发过程:
for lr in [0.001, 0.01, 0.1]:
    model.train(train_set, lr=lr)
    val_mAP = model.eval(val_set)  # ✅ 在val上调参
    if val_mAP > best:
        best_lr = lr

# 最终评估:
final_model.load('best.pt')
test_mAP = final_model.eval(test_set)  # ✅ 仅一次,论文发表

# 优势: test是真正未见过的数据,性能可信 ✅
```

#### Q2: 验证集可以用多少次?会不会过拟合?

**验证集使用频率**:

```python
# Case 1: 合理使用 (每个epoch验证一次)
for epoch in range(300):
    train_one_epoch(train_set)
    val_mAP = validate(val_set)  # 300次验证
    save_best_model_if_improved(val_mAP)
# 结果: val_mAP可信,轻微过拟合 (±0.5%) ✅

# Case 2: 过度使用 (贝叶斯优化超参数)
for _ in range(1000):  # 1000次不同超参数组合
    model = train(random_hyperparams)
    val_mAP = validate(val_set)  # 1000次验证
    bayesian_opt.update(val_mAP)
# 结果: 严重过拟合val set,test mAP会下降3-5% ❌
```

**经验法则**:

- 每个 epoch 验证: ✅ 可接受
- 网格搜索 (<100 组合): ✅ 可接受
- 贝叶斯优化 (>1000 次): ⚠️ 需要额外的 holdout set
- AutoML (>10000 次): ❌ 必须用嵌套交叉验证

#### Q3: VisDrone 的 test set 为什么 ground truth 不公开?

**学术竞赛的标准做法**:

```python
# VisDrone官方评估流程:
# 1. 参赛者本地训练
model.train(train_set + val_set)  # 可以用val训练

# 2. 生成test predictions
preds = model.predict(test_images)  # 仅有图像,无标注

# 3. 提交预测结果
submit_to_server(preds)  # JSON格式

# 4. 服务器评估
server_mAP = evaluate(preds, hidden_gt)  # GT在服务器,不公开

# 5. 返回成绩
print(f"Test mAP: {server_mAP}")  # 论文可用
```

**为什么不公开 GT?**

1. **防止过拟合**: 如果 GT 公开,参赛者会反复调参直到 test mAP 最高
2. **公平对比**: 所有论文报告的 test mAP 都是同一评估标准
3. **持续竞赛**: 可以多年使用同一 test set (如 COCO)

**RemDet 的做法**:

- 论文报告 test mAP=45.2% (官方服务器评估)
- 我们当前用 val mAP=43.51% (本地评估)
- 最终需提交到服务器获得真实 test mAP

### 易错点

❌ **错误 1**: 在 test set 上调参

```python
# 错误流程:
for lr in lrs:
    model.train(train_set, lr=lr)
    test_mAP = model.eval(test_set)  # ❌
    if test_mAP > best:
        best_lr = lr

# 后果: 过拟合test set,论文性能不可信
```

✅ **正确**:

```python
# 正确流程:
for lr in lrs:
    model.train(train_set, lr=lr)
    val_mAP = model.eval(val_set)  # ✅ 用val调参
    if val_mAP > best:
        best_lr = lr

# 最终仅一次test:
final_mAP = model.eval(test_set)  # ✅ 论文发表
```

❌ **错误 2**: 把 val set 当作"额外的训练数据"

```python
# 错误: 训练时合并train+val
model.train(train_set + val_set)  # ❌ 没有验证集了!

# 问题: 无法监控过拟合
#   - loss下降但mAP不涨 → 看不到 (因为没验证)
#   - 训练到epoch 500还不停 (不知道epoch 200已最优)
```

✅ **正确** (仅最终训练可以合并):

```python
# 开发阶段: train和val分开
model.train(train_set)
val_mAP = model.eval(val_set)  # ✅ 监控过拟合

# 最终提交前: 合并train+val重新训练
final_model.train(train_set + val_set)  # ✅ 榨干所有数据
test_mAP = final_model.eval(test_set)  # ✅ 官方评估
```

❌ **错误 3**: 数据泄露 (data leakage)

```python
# 错误: 对全量数据做归一化,再划分train/val/test
全量数据 → 归一化(mean, std) → 划分train/val/test  # ❌
# 问题: test set的统计信息泄露到了train

# 正确: 先划分,再归一化
全量数据 → 划分train/val/test → 仅在train上计算mean/std  # ✅
mean, std = compute_stats(train_set)
normalize(train_set, mean, std)
normalize(val_set, mean, std)  # 用train的统计量
normalize(test_set, mean, std)  # 用train的统计量
```

### 拓展阅读

1. **经典论文**: "A Few Useful Things to Know About Machine Learning" (Pedro Domingos, 2012)
2. **博客**: [Train/Val/Test Split Best Practices](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)
3. **Kaggle**: [Avoiding Data Leakage](https://www.kaggle.com/discussions/getting-started/67846)

### 思考题

**Q1**: 如果只有 5000 张图片,如何划分 train/val/test?

**A**: 两种策略:

1. **标准划分** (70/15/15): Train=3500, Val=750, Test=750

   - 优点: 有独立 test 评估
   - 缺点: train 较少,模型可能欠拟合

2. **交叉验证** (80/20,无 test): Train=4000, Val=1000, Test=0
   - 优点: 更多训练数据
   - 缺点: 无独立 test,需 5-fold CV 验证泛化性

**Q2**: VisDrone 论文为什么 val mAP 和 test mAP 差异较大 (如 val=45%, test=42%)?

**A**: 3 个可能原因:

1. **分布差异**: val 和 test 来自不同场景/城市
2. **过拟合 val**: 作者在 val 上调参过多
3. **标注质量**: test 标注更严格或更宽松

**Q3**: 手撕代码 - 实现一个数据集划分函数,确保类别平衡

```python
def split_dataset(images, labels, ratios=(0.7, 0.15, 0.15)):
    """
    Args:
        images: List[str], 图片路径
        labels: List[int], 对应类别
        ratios: Tuple[float], (train, val, test)比例

    Returns:
        train_set, val_set, test_set (保持类别分布一致)
    """
    # TODO: 实现stratified split
    pass
```

---

## [022] mAP@0.5 vs mAP@0.75 vs mAP@0.5:0.95 的区别

### 标准例子

**评估不同严格程度的定位精度**:

| 指标         | IoU 阈值             | 含义                        | 特点                      |
| ------------ | -------------------- | --------------------------- | ------------------------- |
| mAP@0.5      | ≥0.5                 | 预测框与 GT 重叠 ≥50%算正确 | 容忍度高,关注"是否检测到" |
| mAP@0.75     | ≥0.75                | 预测框与 GT 重叠 ≥75%算正确 | 严格,关注"定位是否精确"   |
| mAP@0.5:0.95 | 0.5~0.95 (步长 0.05) | 10 个 IoU 阈值的 AP 平均值  | 综合评估,平衡检测和定位   |

**典型性能对比**:

```
粗定位模型: mAP50=85%, mAP75=45%, mAP50-95=55% (检测到但框不准)
精定位模型: mAP50=85%, mAP75=75%, mAP50-95=78% (检测准且框精确)
RemDet-X: mAP50=45.2%, mAP75≈28%, mAP50-95≈26% (UAV小目标定位难)
```

### 本项目应用

**在`val_visdrone.py`中同时输出三个指标**:

```python
map50 = metrics.get('metrics/mAP50(B)', 0) * 100      # 45.2% (对齐RemDet)
map75 = metrics.get('metrics/mAP75(B)', 0) * 100      # 28.5% (新增)
map50_95 = metrics.get('metrics/mAP50-95(B)', 0) * 100  # 26.3%
```

**RemDet 对比报告**:

```
mAP@0.5:   Our=43.51%  RemDet=45.2%  Gap=-1.69% ❌
mAP@0.75:  Our=27.20%  RemDet=28.5%  Gap=-1.30% ❌
mAP50-95:  Our=25.80%  RemDet=26.0%  Gap=-0.20% ✅
```

**实践意义**: mAP75 反映框回归质量,对下游任务(跟踪、识别)重要

### 深入讲解

#### Q1: 为什么 RemDet 论文要报告mAP@0.75?

**A**: UAV 目标检测的特殊性:

1. **小目标占比高**: VisDrone 中 68.2%目标<32×32 像素
   - 小目标上框稍微偏移一点,IoU 就从 0.6 掉到 0.4
   - mAP75 对小目标定位精度非常敏感
2. **定位挑战大**: 高空视角(100-200m),目标边界模糊
   - 地面场景: 行人 100-200px,边界清晰,mAP75=60%
   - UAV 场景: 行人 20-30px,边界模糊,mAP75=28%
3. **实用需求**: 无人机应用(跟踪、计数、异常检测)需要精确框
   - 粗框(IoU=0.5): 可能包含背景,干扰跟踪
   - 精框(IoU=0.75): 紧贴目标,提升下游任务准确率

**性能对比**(retention = mAP75/mAP50):

```
COCO (地面): mAP50=42% → mAP75=25% (retention=59.5%)
VisDrone (UAV): mAP50=45% → mAP75=28% (retention=62.2%)
```

→ VisDrone 的 retention 略高,说明相比 COCO,mAP50 和 mAP75 差距相对小  
→ 因为小目标本身就难检测,能检测到的框质量相对更高

#### Q2: mAP@0.5:0.95 为什么是 COCO 主指标?

**A**: 平衡精度和召回率的综合评估:

**单一阈值的问题**:

- mAP@0.5太宽松: 鼓励"检测到即可",忽略定位精度
- mAP@0.95太严格: 对小目标几乎不可能(一个像素偏差就不满足)

**多阈值的优势**:

```python
# 计算方式
ious = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
aps = [compute_ap(iou) for iou in ious]
map50_95 = np.mean(aps)
```

**性能曲线示例**:

```
IoU阈值  AP(%)
0.50     85  ← 大多数预测都满足
0.55     82
0.60     78
0.65     72
0.70     65
0.75     55  ← 明显下降,说明定位有问题
0.80     42
0.85     28
0.90     15
0.95     5   ← 几乎无法满足
平均     52.7% (mAP50-95)
```

**COCO 排名规则**: 先比 mAP50-95,相同则比 mAP50,再比 mAP75

#### Q3: 如何提升mAP@0.75?

**A**: 3 个核心方向:

**1. 改进损失函数** (关注框回归质量):

```python
# ❌ IoU Loss: 只关注重叠面积
loss_iou = 1 - iou(pred, gt)

# ✅ CIoU Loss: 考虑中心点距离 + 宽高比 + IoU
loss_ciou = 1 - iou + rho²/c² + alpha*v
# rho: 中心点距离, c: 对角线长度, v: 宽高比一致性

# ✅ EIoU Loss: 分离宽高差异 (RemDet使用)
loss_eiou = 1 - iou + rho²/c² + rho_w²/cw² + rho_h²/ch²
```

**2. 多尺度训练** (提升小目标定位):

```python
# 随机尺度训练 (YOLO策略)
train_sizes = [480, 512, 544, 576, 608, 640]
for epoch in range(300):
    for batch in dataloader:
        size = random.choice(train_sizes)
        batch = resize(batch, size)
        loss = model(batch)
```

**3. Refine Head** (二次精修框):

```python
# Cascade R-CNN 思想
pred_box1 = detect_head1(features)  # 粗检测
refined_roi = roi_align(features, pred_box1)
pred_box2 = detect_head2(refined_roi)  # 精修 (IoU阈值0.6)
refined_roi2 = roi_align(features, pred_box2)
pred_box3 = detect_head3(refined_roi2)  # 再精修 (IoU阈值0.7)
```

**RemDet 策略**: EIoU loss + 多尺度训练 + 轻量 Refine → mAP75 提升 2-3%

### 易错点

#### ❌ 错误 1: 认为mAP@0.5:0.95 是"所有 IoU 阈值下的平均 mAP"

**错误代码**:

```python
# 错误理解: 所有可能IoU值的平均
all_ious = np.linspace(0, 1, 100)
map_all = np.mean([compute_ap(iou) for iou in all_ious])
```

**✅ 正确**: 只有 10 个固定阈值 (0.5, 0.55, ..., 0.95)

```python
ious = [0.5 + 0.05*i for i in range(10)]  # [0.5, 0.55, ..., 0.95]
map50_95 = np.mean([compute_ap(iou) for iou in ious])
```

#### ❌ 错误 2: 用mAP@0.5评估边界框回归质量

**问题**: mAP50 对框偏移不敏感

```
预测框1: IoU=0.6 (稍微偏移) → mAP50计入 ✅
预测框2: IoU=0.9 (几乎完美) → mAP50计入 ✅
两者得分相同,无法区分回归质量!
```

**✅ 正确**: 用 mAP75 或 mAP50-95 评估

```
预测框1: IoU=0.6 → mAP75不计入 ❌ (鼓励改进)
预测框2: IoU=0.9 → mAP75计入 ✅ (奖励精确)
```

#### ❌ 错误 3: 对比不同论文时忽略评估指标差异

**错误对比**:

```
论文A: mAP@0.5 = 85% (PASCAL VOC)
论文B: mAP@0.5:0.95 = 55% (COCO)
结论: A比B好很多? ❌ 错误!
```

**✅ 正确**: 统一评估标准

```
论文A换算: mAP@0.5=85% → mAP@0.5:0.95≈52% (COCO标准)
论文B: mAP@0.5:0.95=55%
结论: B略优于A ✅
```

### 拓展阅读

1. **COCO Detection Evaluation**: https://cocodataset.org/#detection-eval  
   官方评估协议,详细解释各个指标

2. **论文: "Focal Loss for Dense Object Detection"** (RetinaNet, ICCV 2017)  
   首次系统分析 AP@different-IoU 对小目标的影响

3. **论文: "Cascade R-CNN"** (CVPR 2018)  
   通过级联检测头提升mAP@0.75的经典方法

4. **Blog: "mAP (mean Average Precision) for Object Detection"**  
   https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173

### 思考题

**Q1**: 如果一个模型 mAP50=80%,mAP75=30%,说明什么问题?如何改进?

<details>
<summary>点击查看答案</summary>

**分析**: retention = 30/80 = 37.5% (远低于正常的 60%)  
**问题**: 框回归质量差,检测到目标但定位不准确

**改进方向**:

1. 使用更好的回归损失 (IoU → GIoU → DIoU → CIoU)
2. 增加框回归分支的容量 (加深/加宽 detect head)
3. 多尺度训练提升小目标定位
4. 检查 anchor 设计是否与目标尺度匹配

</details>

**Q2**: VisDrone 为什么同时报告mAP@0.5和mAP@0.75,而不只报mAP@0.5:0.95?

<details>
<summary>点击查看答案</summary>

**原因**:

1. **对齐 PASCAL VOC**: 早期检测竞赛用 mAP50,便于历史对比
2. **突出定位挑战**: mAP75 单独报告,强调 UAV 场景定位难度
3. **实用导向**: 应用场景需要知道粗检测(mAP50)和精检测(mAP75)的差距
4. **避免信息丢失**: mAP50-95 是平均值,掩盖了低 IoU 和高 IoU 的性能差异

**示例**: RemDet Table 2 同时报告:

- mAP50: 45.2% (对比 YOLOv7/8 等常见 baseline)
- mAP75: ~28% (突出定位优势)
- 两者 gap=17.2%,说明定位精度有提升空间

</details>

**Q3**: 手撕代码: 给定预测框和真实框,计算 mAP@[0.5, 0.75, 0.95]

<details>
<summary>点击查看答案</summary>

```python
import numpy as np

def compute_iou(box1, box2):
    """计算IoU (box格式: [x1, y1, x2, y2])"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    inter = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - inter

    return inter / union if union > 0 else 0

def compute_ap_at_iou(preds, gts, iou_thresh):
    """
    计算单个IoU阈值下的AP
    preds: [(conf, box), ...] 按conf降序排列
    gts: [box1, box2, ...]
    """
    tp = np.zeros(len(preds))
    fp = np.zeros(len(preds))
    matched_gts = set()

    for i, (conf, pred_box) in enumerate(preds):
        max_iou = 0
        max_gt_idx = -1

        for j, gt_box in enumerate(gts):
            if j in matched_gts:
                continue
            iou = compute_iou(pred_box, gt_box)
            if iou > max_iou:
                max_iou = iou
                max_gt_idx = j

        if max_iou >= iou_thresh:
            tp[i] = 1
            matched_gts.add(max_gt_idx)
        else:
            fp[i] = 1

    # 计算PR曲线
    tp_cumsum = np.cumsum(tp)
    fp_cumsum = np.cumsum(fp)

    recalls = tp_cumsum / len(gts)
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)

    # 计算AP (11-point interpolation)
    ap = 0
    for t in np.linspace(0, 1, 11):
        p = precisions[recalls >= t]
        ap += np.max(p) if len(p) > 0 else 0
    ap /= 11

    return ap

# 示例
preds = [
    (0.9, [10, 10, 50, 50]),  # 高置信度,IoU=0.8
    (0.8, [12, 12, 52, 52]),  # 中置信度,IoU=0.75
    (0.7, [15, 15, 55, 55]),  # 低置信度,IoU=0.6
]
gts = [[10, 10, 50, 50], [60, 60, 100, 100]]

map50 = compute_ap_at_iou(preds, gts, 0.5)   # 3/3正确 → AP≈100%
map75 = compute_ap_at_iou(preds, gts, 0.75)  # 2/3正确 → AP≈67%
map95 = compute_ap_at_iou(preds, gts, 0.95)  # 0/3正确 → AP=0%

print(f'mAP@0.5: {map50:.2%}')
print(f'mAP@0.75: {map75:.2%}')
print(f'mAP@0.95: {map95:.2%}')
```

</details>

---

## [023] FLOPs, Latency, Params 的区别与测量

### 标准例子

**三个效率指标,互补不可替代**:

| 指标    | 定义                      | 单位   | 特点                    | 影响因素                        |
| ------- | ------------------------- | ------ | ----------------------- | ------------------------------- |
| FLOPs   | 浮点运算次数 (理论计算量) | GFLOPs | 与硬件无关,只看网络结构 | 卷积核大小, 通道数, 分辨率      |
| Latency | 单张图片推理耗时          | ms     | 与硬件强相关            | GPU 型号, CUDA 版本, batch size |
| Params  | 模型权重总数              | M      | 决定存储大小和显存占用  | 层数, 通道数, 全连接层          |

**为什么不成正比?**

```
MobileNetV2: FLOPs=0.3G, Latency=25ms, Params=3.5M
  → 低FLOPs但慢 (Depthwise卷积访存瓶颈,Memory-bound)

ResNet50: FLOPs=4.1G, Latency=20ms, Params=25.6M
  → 高FLOPs但快 (标准卷积计算密集,Compute-bound)
```

### 本项目应用

**在`val_visdrone.py`中自动测量三项指标**:

```python
# 1. 测量FLOPs和Params (thop库)
from thop import profile
flops, params = profile(model, inputs=(dummy_input,))

# 2. 测量Latency (warmup + 多次平均)
for _ in range(10): model(input)  # warmup
latencies = []
for _ in range(100):
    torch.cuda.synchronize()
    start = time.time()
    model(input)
    torch.cuda.synchronize()
    latencies.append(time.time() - start)
latency_ms = np.mean(latencies) * 1000
```

**RemDet 对比报告**:

```
⚡ Efficiency Metrics:
  Latency (ms)  Our=11.2    RemDet=12.8   -1.6ms (-12.5%) ✅ Faster
  FLOPs (G)     Our=48.3    RemDet=52.4   -4.1G  (-7.8%)  ✅ Lighter
  Params (M)    Our=9.6     RemDet=16.3   -6.7M  (-41.1%) ✅ Lighter
```

**实践意义**:

- v2.1 模型更轻量(41%参数更少),更快(12%推理更快)
- 但精度略低(mAP50 差 1.69%),属于效率-精度权衡

### 深入讲解

#### Q1: FLOPs 计算公式是什么?为什么 Depthwise Conv FLOPs 低?

**A**: 标准卷积 vs Depthwise 卷积:

**标准卷积 FLOPs**:

```python
# Conv2d(in_channels=C_in, out_channels=C_out, kernel=K, input_size=H×W)
FLOPs = 2 × H × W × K² × C_in × C_out

# 示例: Conv2d(256→512, 3×3, 56×56)
FLOPs = 2 × 56 × 56 × 3² × 256 × 512
      = 2 × 3136 × 9 × 256 × 512
      = 14,745,681,920 ≈ 14.7 GFLOPs
```

**Depthwise 卷积 FLOPs**:

```python
# Depthwise: 每个输入通道独立卷积
FLOPs_dw = 2 × H × W × K² × C_in  # 无C_out相乘!

# Pointwise (1×1卷积恢复通道):
FLOPs_pw = 2 × H × W × C_in × C_out

# 总FLOPs
FLOPs_total = FLOPs_dw + FLOPs_pw
            = 2 × H × W × (K² × C_in + C_in × C_out)

# 示例: Depthwise(256→512, 3×3, 56×56)
FLOPs_total = 2 × 56 × 56 × (9 × 256 + 256 × 512)
            = 2 × 3136 × (2304 + 131072)
            = 836,689,920 ≈ 0.84 GFLOPs
```

**对比**: Depthwise 降低约 17.6 倍 FLOPs! (14.7G → 0.84G)

**但为什么 Depthwise 更慢?**
→ Memory-bound: GPU 访存带宽不足,计算单元空闲等待数据

#### Q2: Latency 为什么与硬件相关?如何公平对比?

**A**: 影响 Latency 的 5 个因素:

**1. GPU 型号** (算力差异):

```
RTX 3090: 35.6 TFLOPS (FP32), Latency=12ms
RTX 4090: 82.6 TFLOPS (FP32), Latency=6ms  ← 同模型快2倍
V100: 15.7 TFLOPS, Latency=25ms
```

**2. CUDA 版本** (优化差异):

```
CUDA 10.2: Latency=15ms (无Tensor Core优化)
CUDA 11.8: Latency=12ms (Tensor Core加速FP16)
CUDA 12.0: Latency=11ms (Flash Attention融合)
```

**3. Batch Size** (并行度):

```
Batch=1: Latency=12ms (低并行,GPU利用率50%)
Batch=8: Latency=45ms (平均5.6ms/图,高并行,利用率95%)
```

**4. 数据类型** (精度):

```
FP32: Latency=12ms
FP16: Latency=7ms  ← 快1.7倍,精度损失<0.1%
INT8: Latency=4ms  ← 快3倍,需量化训练
```

**5. 算子融合** (编译优化):

```
未融合: Conv→BN→ReLU各自kernel → Latency=15ms
融合后: Conv-BN-ReLU单个kernel → Latency=10ms (减少访存)
```

**公平对比原则** (RemDet 论文):

- GPU: RTX 3090
- CUDA: 11.3
- Batch: 1 (单张推理)
- 数据类型: FP32
- 优化: TorchScript 编译 (算子融合)
- 环境: 无其他进程竞争

#### Q3: Params 多会带来什么问题?为什么 RemDet 比我们重?

**A**: 参数量的 4 个影响:

**1. 模型存储** (磁盘占用):

```python
# FP32存储
params = 16.3M (RemDet-X)
storage = params × 4 bytes = 16.3 × 4 = 65.2 MB

params = 9.6M (Our v2.1)
storage = 9.6 × 4 = 38.4 MB  ← 节省41%磁盘空间
```

**2. 显存占用** (训练时):

```python
# 显存 = 模型权重 + 梯度 + 优化器状态 + 激活值
memory = params × 4 (权重)
       + params × 4 (梯度)
       + params × 8 (Adam: m, v两个动量)
       + batch_size × feature_maps (激活值)

# RemDet-X (16.3M params)
memory = 16.3×4 + 16.3×4 + 16.3×8 + activations
       = 261MB + activations

# Our v2.1 (9.6M params)
memory = 154MB + activations  ← 节省107MB
```

**3. 加载时间** (部署):

```
RemDet-X (65MB): 加载耗时 150ms (SSD读取)
Our v2.1 (38MB): 加载耗时 90ms   ← 快40%
```

**4. 过拟合风险** (小数据集):

```
VisDrone训练集: 6471张
RemDet-X: 16.3M params → 每张图2521个参数 (可能过拟合)
Our v2.1: 9.6M params  → 每张图1484个参数 (更稳定)
```

**RemDet 为什么更重?**

- 更深的 backbone (更多 C2f 模块)
- 更宽的 neck (通道数更大)
- 引入额外的 Refine Head (二次精修框)

**我们的优势**: 轻量化设计,适合边缘设备部署

### 易错点

#### ❌ 错误 1: 认为 FLOPs 低一定速度快

**反例**: MobileNetV2 vs ResNet18

```
MobileNetV2: FLOPs=0.3G, Latency=25ms (RTX 3090)
ResNet18:    FLOPs=1.8G, Latency=8ms  ← FLOPs高6倍但快3倍!
```

**原因**: Depthwise 卷积虽然 FLOPs 低,但访存瓶颈严重(Memory-bound)

**✅ 正确**: 同时看 FLOPs 和 Latency,实测为准

#### ❌ 错误 2: Latency 测量时忘记 warmup

**错误代码**:

```python
# ❌ 首次推理包含初始化开销
start = time.time()
output = model(input)
latency = time.time() - start  # 可能50ms (实际应该12ms)
```

**✅ 正确**:

```python
# 预热10次
for _ in range(10):
    model(input)

torch.cuda.synchronize()
start = time.time()
output = model(input)
torch.cuda.synchronize()
latency = (time.time() - start) * 1000  # 12ms
```

#### ❌ 错误 3: 对比不同论文 Latency 时忽略硬件差异

**错误对比**:

```
论文A: Latency=10ms (V100, FP16)
论文B: Latency=15ms (RTX 3090, FP32)
结论: A比B快50%? ❌ 错误!
```

**✅ 正确**: 换算到同一硬件

```
V100 FP32: ~25 TFLOPS
RTX 3090 FP32: ~35 TFLOPS (快1.4倍)

论文A换算: 10ms × (35/25) / 2 (FP16→FP32) = 14ms
论文B: 15ms
结论: 性能相近 ✅
```

### 拓展阅读

1. **thop 库**: https://github.com/Lyken17/pytorch-OpCounter  
   FLOPs 和 Params 计算工具

2. **论文: "MobileNets"** (Google, 2017)  
   Depthwise Separable Convolution 降低 FLOPs 的经典工作

3. **论文: "ShuffleNet V2"** (ECCV 2018)  
   分析 FLOPs 与实际速度的 gap,提出 Memory Access Cost (MAC)指标

4. **Blog: "A Survey on Model Compression"**  
   https://arxiv.org/abs/1710.09282 (模型压缩综述)

5. **TensorRT Optimization Guide**:  
   https://docs.nvidia.com/deeplearning/tensorrt/  
   部署优化(算子融合, INT8 量化)

### 思考题

**Q1**: 为什么 YOLO 系列强调 FPS (Frames Per Second) 而非 Latency?两者什么关系?

<details>
<summary>点击查看答案</summary>

**关系**: FPS = 1000 / Latency(ms)

**示例**:

- Latency = 10ms → FPS = 100
- Latency = 25ms → FPS = 40
- Latency = 50ms → FPS = 20

**为什么 YOLO 用 FPS?**

1. **直观**: FPS=30 表示"每秒处理 30 帧",更易理解
2. **对比方便**: "YOLOv8 FPS=120 vs YOLOv5 FPS=80"比"8.3ms vs 12.5ms"直观
3. **应用导向**: 视频处理需求通常是"能否实时(≥30 FPS)"

**注意**: FPS 不等于实际视频处理速度!

```
模型FPS = 100 (单张推理10ms)
实际视频FPS = 60 (包含解码20ms + 后处理10ms + 渲染10ms)
```

</details>

**Q2**: 手撕代码: 计算 Conv2d 层的 FLOPs 和 Params

<details>
<summary>点击查看答案</summary>

```python
def compute_conv2d_flops_params(in_channels, out_channels, kernel_size,
                                input_h, input_w, stride=1, padding=0,
                                groups=1, bias=True):
    """
    计算Conv2d的FLOPs和Params

    Args:
        in_channels: 输入通道数
        out_channels: 输出通道数
        kernel_size: 卷积核大小 (假设正方形)
        input_h, input_w: 输入特征图尺寸
        stride: 步长
        padding: 填充
        groups: 分组卷积数 (1=标准卷积, in_channels=Depthwise)
        bias: 是否有偏置

    Returns:
        flops, params
    """
    # 1. 计算输出尺寸
    output_h = (input_h + 2 * padding - kernel_size) // stride + 1
    output_w = (input_w + 2 * padding - kernel_size) // stride + 1

    # 2. 计算FLOPs
    # 每个输出位置: K²×(C_in/G)次乘法 + (K²×C_in/G - 1)次加法 ≈ 2×K²×C_in/G
    # C_out个输出通道, H×W个位置
    kernel_flops = kernel_size * kernel_size * (in_channels // groups)
    flops = 2 * output_h * output_w * kernel_flops * out_channels

    if bias:
        flops += output_h * output_w * out_channels  # 偏置加法

    # 3. 计算Params
    weight_params = out_channels * (in_channels // groups) * kernel_size * kernel_size
    bias_params = out_channels if bias else 0
    params = weight_params + bias_params

    return flops, params

# 测试: 标准卷积
flops, params = compute_conv2d_flops_params(
    in_channels=256, out_channels=512, kernel_size=3,
    input_h=56, input_w=56, stride=1, padding=1
)
print(f'Standard Conv: FLOPs={flops/1e9:.2f}G, Params={params/1e6:.2f}M')
# 输出: FLOPs=14.75G, Params=1.18M

# 测试: Depthwise卷积
flops_dw, params_dw = compute_conv2d_flops_params(
    in_channels=256, out_channels=256, kernel_size=3,
    input_h=56, input_w=56, stride=1, padding=1,
    groups=256  # Depthwise: groups=in_channels
)
flops_pw, params_pw = compute_conv2d_flops_params(
    in_channels=256, out_channels=512, kernel_size=1,
    input_h=56, input_w=56, stride=1, padding=0
)
flops_total = flops_dw + flops_pw
params_total = params_dw + params_pw
print(f'Depthwise Separable: FLOPs={flops_total/1e9:.2f}G, Params={params_total/1e6:.2f}M')
# 输出: FLOPs=0.84G, Params=0.13M
# ✅ FLOPs降低17.6倍, Params降低9.1倍!
```

</details>

**Q3**: v2.1 模型比 RemDet-X 参数少 41%,但 mAP 只差 1.69%,如何解释?

<details>
<summary>点击查看答案</summary>

**可能原因**:

1. **深度信息高效利用**: RGB-D 双模态提供额外信息,用更少参数达到相近性能

   - RGB-only 需要更深网络学习深度先验
   - RGB-D 直接输入深度图,模型更轻量

2. **VisDrone 数据集规模**: 仅 6471 张训练图,过大模型容易过拟合

   - RemDet-X: 16.3M params,可能过参数化
   - v2.1: 9.6M params,参数量与数据集匹配更好

3. **架构设计差异**:

   - RemDet focus: 更深 backbone + Refine Head
   - v2.1 focus: 高效融合模块 (RGBDMidFusion)
   - 双模态融合比单纯加深网络更高效

4. **性能边际效应**: 参数量增加带来的收益递减
   ```
   5M → 10M: +3% mAP
   10M → 15M: +1.5% mAP
   15M → 20M: +0.5% mAP  ← RemDet-X可能在这个区间
   ```

**结论**: v2.1 实现了更好的效率-精度权衡,适合实际部署

</details>

---

## 八股更新记录

- **2025/10/26**: 创建八股知识手册，添加 5 个初始知识点

  - [001] 多模态融合策略
  - [002] 特征金字塔网络
  - [004] SOLR 小目标损失重加权
  - [014] Sobel 算子与边缘检测

- **2025/10/27 14:00**: Phase 2.5 完成，添加 5 个验证评估相关知识点

  - [017] VisDrone vs COCO 评估差异 (中目标定义, 密集度)
  - [018] NMS 参数对密集场景的影响 (IoU 阈值, max_det, Soft-NMS)
  - [019] 置信度阈值对 mAP 的影响 (训练/验证/推理的区别)
  - [020] 验证脚本 vs 训练脚本的核心区别 (梯度, 数据增强, 参数)
  - [021] 训练集/验证集/测试集的职责与区别 (过拟合, 数据泄露)

- **2025/10/27 16:30**: val_visdrone.py 改进，添加完整 RemDet 对齐指标
  - [022] mAP@0.5 vs mAP@0.75 vs mAP@0.5:0.95 (定位精度评估)
  - [023] FLOPs, Latency, Params 的区别与测量 (效率指标全解)

---

## 学习建议

1. **每日复习**: 每天选择 2-3 个知识点进行口述演练
2. **项目关联**: 修改代码时，主动关联对应的八股知识点
3. **模拟面试**: 定期录音自己讲解知识点，检查流畅度
4. **补充完善**: 遇到新知识点立即添加到本文档

---

## 练习计划

### 第 1 周（阶段 1 期间）

- [ ] 手撕代码：多模态融合模块
- [ ] 口述演练：Sobel 算子原理
- [ ] 对比分析：Early/Mid/Late Fusion

### 第 2 周（阶段 1-2 过渡）

- [ ] 手撕代码：SOLR 权重计算
- [ ] 口述演练：FPN 工作原理
- [ ] 对比分析：FPN vs PAN

待续...
