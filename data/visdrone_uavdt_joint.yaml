# Ultralytics YOLO üöÄ, AGPL-3.0 license
# VisDrone2019-DET RGB-D Dataset Configuration
# Created for yoloDepth project - RGB-D dual-modal object detection

# Dataset Information
# VisDrone2019-DET: Large-scale UAV object detection benchmark
# Website: http://aiskyeye.com/
# Classes: 10 object categories
# Train: 6,471 images, Val: 548 images
# Characteristics: Small objects (avg size 12√ó12 px), occlusions, varying scales

# Dataset root path (absolute path recommended)
path:
  /data2/user/2024/lzy/Datasets/ # TODO: Replace with your actual dataset path
  # Example: /data/datasets/VisDrone2019-DET
  # Example (Windows): F:/Datasets/VisDrone2019-DET

# RGB image paths (relative to 'path')
train:
  - VisDrone2019-DET-YOLO/VisDrone2YOLO/VisDrone2019-DET-train/images/rgb # VisDroneËÆ≠ÁªÉÈõÜ (6,471Âº†)
  - UAVDT_YOLO/train/images/rgb # UAVDTËÆ≠ÁªÉÈõÜ (23,829Âº†)
val:
  - VisDrone2019-DET-YOLO/VisDrone2YOLO/VisDrone2019-DET-val/images/rgb # Âè™Áî®VisDroneÈ™åËØÅÈõÜ (548Âº†ÔºåÂØπÈΩêRemDetËØÑ‰º∞)
test:
  - VisDrone2019-DET-YOLO/VisDrone2YOLO/VisDrone2019-DET-val/images/rgb # ÊµãËØïÈõÜÂêåÈ™åËØÅÈõÜ

# Depth map paths (relative to 'path') - REQUIRED FOR RGB-D TRAINING
# ‚ö†Ô∏è Ê≥®ÊÑè: Ê∑±Â∫¶ÂõæÂú®images/d/ÁõÆÂΩï (‰øùÊåÅÁé∞ÊúâÁªìÊûÑ)
train_depth:
  - VisDrone2019-DET-YOLO/VisDrone2YOLO/VisDrone2019-DET-train/images/d # VisDroneÊ∑±Â∫¶Âõæ
  - UAVDT_YOLO/train/images/d # UAVDTÊ∑±Â∫¶Âõæ (Â∞ÜÁî±generate_depths_uavdt.pyÁîüÊàê)
val_depth:
  - VisDrone2019-DET-YOLO/VisDrone2YOLO/VisDrone2019-DET-val/images/d # Âè™Áî®VisDroneÊ∑±Â∫¶Âõæ
test_depth:
  - VisDrone2019-DET-YOLO/VisDrone2YOLO/VisDrone2019-DET-val/images/d

# Number of classes
nc: 10

# Class names (VisDrone2019 categories)
names:
  0: pedestrian # Ë°å‰∫∫
  1: people # ‰∫∫Áæ§ (Â§ö‰∏™‰∫∫ËÅöÈõÜ)
  2: bicycle # Ëá™Ë°åËΩ¶
  3: car # Â∞èÊ±ΩËΩ¶
  4: van # Èù¢ÂåÖËΩ¶/ÂïÜÂä°ËΩ¶
  5: truck # Âç°ËΩ¶
  6: tricycle # ‰∏âËΩÆËΩ¶
  7: awning-tricycle # Â∏¶ÁØ∑‰∏âËΩÆËΩ¶
  8: bus # ÂÖ¨‰∫§ËΩ¶/Â§ßÂ∑¥
  9: motor # Êë©ÊâòËΩ¶/ÁîµÂä®ËΩ¶

# Class mapping to RemDet paper (for comparison)
# RemDet also uses VisDrone2019, so class definitions are identical

# ==================== Directory Structure Example ====================
#
# VisDrone2019-DET/
# ‚îú‚îÄ‚îÄ images/
# ‚îÇ   ‚îú‚îÄ‚îÄ train/
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0000001_00000_d_0000001.jpg
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0000001_00000_d_0000002.jpg
# ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
# ‚îÇ   ‚îú‚îÄ‚îÄ val/
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0000006_00159_d_0000014.jpg
# ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
# ‚îÇ   ‚îî‚îÄ‚îÄ test/  (optional)
# ‚îÇ
# ‚îú‚îÄ‚îÄ depths/  # YOU NEED TO GENERATE THESE
# ‚îÇ   ‚îú‚îÄ‚îÄ train/
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0000001_00000_d_0000001.png  # Same filename, different extension OK
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0000001_00000_d_0000002.png
# ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
# ‚îÇ   ‚îú‚îÄ‚îÄ val/
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0000006_00159_d_0000014.png
# ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
# ‚îÇ   ‚îî‚îÄ‚îÄ test/  (optional)
# ‚îÇ
# ‚îî‚îÄ‚îÄ labels/
#     ‚îú‚îÄ‚îÄ train/
#     ‚îÇ   ‚îú‚îÄ‚îÄ 0000001_00000_d_0000001.txt  # YOLO format (class x_center y_center width height)
#     ‚îÇ   ‚îî‚îÄ‚îÄ ...
#     ‚îú‚îÄ‚îÄ val/
#     ‚îÇ   ‚îî‚îÄ‚îÄ ...
#     ‚îî‚îÄ‚îÄ test/  (optional)
#
# ======================================================================

# ==================== Depth Map Generation ====================
#
# VisDrone2019 does NOT provide depth maps by default. You need to:
#
# OPTION 1: Monocular Depth Estimation (Recommended)
#   Use pre-trained models to estimate depth from RGB:
#   - DPT (Dense Prediction Transformer): https://github.com/isl-org/DPT
#   - MiDaS v3.1: https://github.com/isl-org/MiDaS
#   - ZoeDepth: https://github.com/isl-org/ZoeDepth (best for outdoor scenes)
#
#   Example command (using MiDaS):
#   ```bash
#   python run_depth_estimation.py \
#       --input_dir images/train \
#       --output_dir depths/train \
#       --model_type dpt_beit_large_512 \
#       --optimize
#   ```
#
# OPTION 2: Stereo Depth Estimation
#   If you have stereo image pairs (VisDrone doesn't), use:
#   - RAFT-Stereo: https://github.com/princeton-vl/RAFT-Stereo
#   - PSMNet: https://github.com/JiaRenChang/PSMNet
#
# OPTION 3: Use Existing RGB-D Datasets
#   Alternative UAV datasets with depth:
#   - Middlebury Stereo Datasets (outdoor scenes)
#   - DIML/CVL RGB-D Dataset (general scenes)
#   - But these may not match VisDrone's distribution
#
# IMPORTANT: Depth Quality Matters!
#   - Use high-quality depth estimation model (DPT > MiDaS > DepthAnything)
#   - Ensure depth range is consistent across dataset
#   - Apply median filtering to reduce noise (already done in YOLORGBDDataset)
#   - Validate depth quality by visualizing depth maps
#
# ==============================================================

# ==================== Data Validation Checklist ====================
#
# Before training, verify:
# [‚úì] 1. RGB images exist in images/train and images/val
# [‚úì] 2. Depth maps exist in depths/train and depths/val
# [‚úì] 3. Each RGB image has a corresponding depth map with matching filename
#        (Extensions can differ: image.jpg ‚Üî depth.png is OK)
# [‚úì] 4. Labels exist in labels/train and labels/val
# [‚úì] 5. Depth maps are single-channel grayscale images
# [‚úì] 6. Depth values are in reasonable range (0-255 for uint8, or 0-65535 for uint16)
# [‚úì] 7. Image and depth maps have same resolution (or at least same aspect ratio)
#
# Quick validation script:
# ```python
# from pathlib import Path
# import cv2
#
# rgb_dir = Path("images/train")
# depth_dir = Path("depths/train")
#
# for rgb_path in rgb_dir.glob("*.jpg"):
#     depth_path = depth_dir / rgb_path.with_suffix(".png").name
#     assert depth_path.exists(), f"Missing depth: {depth_path}"
#
#     rgb = cv2.imread(str(rgb_path))
#     depth = cv2.imread(str(depth_path), cv2.IMREAD_UNCHANGED)
#
#     assert rgb.shape[:2] == depth.shape[:2], f"Size mismatch: {rgb_path.name}"
#     print(f"‚úì {rgb_path.name}: RGB {rgb.shape}, Depth {depth.shape}")
# ```
#
# ===================================================================

# ==================== Training Configuration ====================
#
# This YAML file is loaded by train_depth.py. Example usage:
#
# Basic training:
#   python train_depth.py --data data/visdrone-rgbd.yaml --epochs 300
#
# With custom settings:
#   python train_depth.py \
#       --data data/visdrone-rgbd.yaml \
#       --model ultralytics/cfg/models/12/yolo12s-rgbd-v1.yaml \
#       --batch 16 \
#       --epochs 300 \
#       --device 0,1  # Multi-GPU training
#
# Resume from checkpoint:
#   python train_depth.py \
#       --resume runs/train/exp1/weights/last.pt \
#       --data data/visdrone-rgbd.yaml
#
# ================================================================

# ==================== Expected Performance ====================
#
# Baseline (YOLOv12s RGB-only on VisDrone2019):
#   - mAP@0.5: ~39.0%
#   - mAP@0.5:0.95: ~26.0%
#   - mAP_small: ~13.0%  (critical metric for UAV)
#   - FPS: ~70 (RTX 4090)
#
# YOLOv12s-RGBD (this project, Phase 1 target):
#   - mAP@0.5: 41-42% (+2-3 points from depth)
#   - mAP@0.5:0.95: 28-29%
#   - mAP_small: 15-16% (+2-3 points, depth helps small objects)
#   - FPS: 50-60 (RTX 4090, slightly slower due to dual-modal)
#
# RemDet-X (AAAI2025 benchmark):
#   - mAP@0.5: 45.2%
#   - mAP@0.5:0.95: 30.8%
#   - mAP_small: 21.3%  (strong small object performance)
#   - FPS: ~65 (RTX 4090)
#
# Ultimate Goal (after Phase 2-6 optimizations):
#   - mAP@0.5: 48-50% (+3-5 points over RemDet)
#   - mAP_small: 25-27% (+4-6 points over RemDet)
#   - FPS: >60 (maintain real-time performance)
#
# ==============================================================

# ==================== Dataset Statistics (Reference) ====================
#
# VisDrone2019-DET Training Set:
#   - Images: 6,471
#   - Objects: 343,288 instances
#   - Average objects per image: 53.1
#   - Object size distribution:
#       * Small (<32√ó32 px): 68.2%  ‚Üê Main challenge
#       * Medium (32-96 px): 26.5%
#       * Large (>96 px): 5.3%
#   - Occlusion: 31.7% of objects occluded
#   - Truncation: 12.8% of objects truncated
#
# VisDrone2019-DET Validation Set:
#   - Images: 548
#   - Objects: 28,447 instances
#   - Similar distribution to training set
#
# Class Distribution (Training Set):
#   - car: 135,873 (39.6%)  ‚Üê Dominant class
#   - people: 89,621 (26.1%)
#   - pedestrian: 52,464 (15.3%)
#   - motor: 31,205 (9.1%)
#   - van: 15,874 (4.6%)
#   - tricycle: 9,243 (2.7%)
#   - bus: 4,328 (1.3%)
#   - truck: 3,942 (1.1%)
#   - bicycle: 514 (0.1%)
#   - awning-tricycle: 224 (0.1%)
#
# Insight: Severe class imbalance (car 39.6% vs bicycle 0.1%)
#          ‚Üí May need class-balanced sampling or loss weighting
#
# ========================================================================

# ==================== Notes and Tips ====================
#
# 1. Depth Map Format:
#    - Preferred: 16-bit PNG (0-65535 range, better precision)
#    - Acceptable: 8-bit PNG/JPG (0-255 range, lower precision)
#    - Avoid: Float32 TIFF (slow I/O, large file size)
#
# 2. Depth Normalization:
#    - Handled automatically by YOLORGBDDataset._process_depth_channel
#    - Pipeline: median(3√ó3) ‚Üí gaussian(5√ó5) ‚Üí percentile normalization
#    - No need for manual preprocessing
#
# 3. Multi-Modal Alignment:
#    - RGB and Depth MUST have same filename (extensions can differ)
#    - Spatial alignment is critical (no misalignment tolerance)
#    - If using estimated depth, ensure model input matches RGB resolution
#
# 4. Memory Considerations:
#    - 6,471 training images √ó 4 channels = ~4GB RAM (uncompressed)
#    - Use cache=False if RAM < 16GB
#    - Use cache='disk' for balance between speed and memory
#
# 5. Training Time Estimate:
#    - RTX 4090, batch=16, 300 epochs, cache='ram': ~18-20 hours
#    - RTX 4090, batch=16, 300 epochs, cache=False: ~24-26 hours
#    - Multi-GPU (4√ó RTX 4090), batch=64: ~6-8 hours
#
# 6. Debugging Tips:
#    - Start with small epoch test (--epochs 10) to verify pipeline
#    - Monitor gate_mean in RGBDStem (should be 0.3-0.7)
#    - Visualize depth preprocessing results with val_depth.py
#    - Check for NaN/Inf in losses (indicates instability)
#
# ========================================================

# ==================== Version History ====================
# v1.0 (2025-10-26): Initial version for yoloDepth Phase 1
#                    - Basic RGB-D configuration
#                    - VisDrone2019-DET class definitions
#                    - Comprehensive documentation
# =========================================================

# ==================== TODO ====================
# [ ] Generate depth maps for VisDrone2019 using MiDaS/DPT/ZoeDepth
# [ ] Validate RGB-Depth pairing with validation script
# [ ] Run 10-epoch test training to verify pipeline
# [ ] Perform full 300-epoch training and log results to ÊîπËøõËÆ∞ÂΩï.md
# [ ] Compare with RemDet baseline on validation set
# [ ] Visualize depth quality and fusion effectiveness
# ==============================================
