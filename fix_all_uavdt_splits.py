"""
UAVDTå®Œæ•´ä¿®å¤è„šæœ¬ - ä¿®å¤æ‰€æœ‰åˆ†å‰²(train/val/test)
è§£å†³exp_joint_v16æ€§èƒ½æœªæå‡çš„é—®é¢˜

ä½¿ç”¨æ–¹æ³•:
    python fix_all_uavdt_splits.py --dataset_root /path/to/UAVDT_YOLO
"""

import os
import argparse
from pathlib import Path
from tqdm import tqdm
from collections import Counter
import shutil


def fix_labels_in_directory(label_dir, backup=True):
    """
    ä¿®å¤å•ä¸ªç›®å½•ä¸­çš„æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶
    
    Args:
        label_dir: æ ‡ç­¾ç›®å½•è·¯å¾„
        backup: æ˜¯å¦å¤‡ä»½
    
    Returns:
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    label_dir = Path(label_dir)
    
    if not label_dir.exists():
        print(f"  âš ï¸  ç›®å½•ä¸å­˜åœ¨,è·³è¿‡: {label_dir}")
        return None
    
    # æŸ¥æ‰¾æ ‡ç­¾æ–‡ä»¶
    label_files = list(label_dir.rglob("*.txt"))
    
    if not label_files:
        print(f"  âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶,è·³è¿‡: {label_dir}")
        return None
    
    print(f"  ğŸ“ æ‰¾åˆ° {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")
    
    # å¤‡ä»½
    if backup:
        backup_dir = label_dir.parent / f"{label_dir.name}_backup_{Path.cwd().stat().st_mtime}"
        if not backup_dir.exists():
            print(f"  ğŸ“¦ å¤‡ä»½åˆ°: {backup_dir}")
            shutil.copytree(label_dir, backup_dir)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_files': len(label_files),
        'modified_files': 0,
        'total_objects': 0,
        'modified_objects': 0,
        'category_before': Counter(),
        'category_after': Counter(),
        'errors': [],
    }
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for label_file in tqdm(label_files, desc="  ä¿®å¤æ ‡ç­¾", leave=False):
        try:
            # è¯»å–
            with open(label_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            modified_lines = []
            file_modified = False
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) < 5:
                    stats['errors'].append(f"{label_file}: æ ¼å¼é”™è¯¯")
                    modified_lines.append(line + '\n')
                    continue
                
                try:
                    old_category = int(parts[0])
                    stats['category_before'][old_category] += 1
                    stats['total_objects'] += 1
                    
                    # ç±»åˆ«IDå‡1
                    new_category = old_category - 1
                    
                    # éªŒè¯èŒƒå›´
                    if new_category < 0 or new_category > 9:
                        stats['errors'].append(
                            f"{label_file}: IDè¶…èŒƒå›´ {old_category}â†’{new_category}"
                        )
                        modified_lines.append(line + '\n')
                        continue
                    
                    # æ„é€ æ–°è¡Œ
                    parts[0] = str(new_category)
                    new_line = ' '.join(parts) + '\n'
                    modified_lines.append(new_line)
                    
                    stats['category_after'][new_category] += 1
                    stats['modified_objects'] += 1
                    
                    if old_category != new_category:
                        file_modified = True
                
                except ValueError:
                    stats['errors'].append(f"{label_file}: æ— æ³•è§£æç±»åˆ«ID")
                    modified_lines.append(line + '\n')
            
            # å†™å›
            if file_modified:
                with open(label_file, 'w', encoding='utf-8') as f:
                    f.writelines(modified_lines)
                stats['modified_files'] += 1
        
        except Exception as e:
            stats['errors'].append(f"{label_file}: {e}")
    
    return stats


def delete_cache_files(dataset_root):
    """åˆ é™¤æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
    dataset_root = Path(dataset_root)
    cache_files = list(dataset_root.rglob("*.cache"))
    
    if not cache_files:
        print("\n  â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")
        return 0
    
    print(f"\n  ğŸ—‘ï¸  æ‰¾åˆ° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
    
    deleted = 0
    for cache_file in cache_files:
        try:
            cache_file.unlink()
            deleted += 1
            print(f"    âœ… åˆ é™¤: {cache_file.relative_to(dataset_root)}")
        except Exception as e:
            print(f"    âŒ åˆ é™¤å¤±è´¥ {cache_file}: {e}")
    
    return deleted


def verify_labels(label_dir, expected_classes=[3, 5, 8]):
    """éªŒè¯æ ‡ç­¾ä¿®å¤ç»“æœ"""
    label_dir = Path(label_dir)
    
    if not label_dir.exists():
        return False, "ç›®å½•ä¸å­˜åœ¨"
    
    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    class_counts = Counter()
    
    for label_file in label_dir.rglob("*.txt"):
        with open(label_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    try:
                        class_id = int(parts[0])
                        class_counts[class_id] += 1
                    except ValueError:
                        pass
    
    # æ£€æŸ¥æ˜¯å¦åªåŒ…å«é¢„æœŸç±»åˆ«
    actual_classes = set(class_counts.keys())
    expected_set = set(expected_classes)
    
    if actual_classes == expected_set:
        return True, class_counts
    else:
        return False, {
            'actual': actual_classes,
            'expected': expected_set,
            'counts': class_counts
        }


def print_stats(split_name, stats):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    if stats is None:
        return
    
    print(f"\n  ğŸ“Š {split_name} ä¿®å¤ç»Ÿè®¡:")
    print(f"    æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"    ä¿®æ”¹æ–‡ä»¶æ•°: {stats['modified_files']}")
    print(f"    æ€»å¯¹è±¡æ•°: {stats['total_objects']}")
    print(f"    ä¿®æ”¹å¯¹è±¡æ•°: {stats['modified_objects']}")
    
    print(f"\n    ä¿®å¤å‰ç±»åˆ«åˆ†å¸ƒ:")
    for cat_id in sorted(stats['category_before'].keys()):
        count = stats['category_before'][cat_id]
        print(f"      ç±»åˆ« {cat_id}: {count:>8} ä¸ª")
    
    print(f"\n    ä¿®å¤åç±»åˆ«åˆ†å¸ƒ:")
    for cat_id in sorted(stats['category_after'].keys()):
        count = stats['category_after'][cat_id]
        print(f"      ç±»åˆ« {cat_id}: {count:>8} ä¸ª")
    
    if stats['errors']:
        print(f"\n    âš ï¸  é‡åˆ° {len(stats['errors'])} ä¸ªé”™è¯¯ (æ˜¾ç¤ºå‰5ä¸ª):")
        for error in stats['errors'][:5]:
            print(f"      - {error}")


def main():
    parser = argparse.ArgumentParser(
        description="ä¿®å¤UAVDTæ•°æ®é›†æ‰€æœ‰åˆ†å‰²çš„ç±»åˆ«ID (4â†’3, 6â†’5, 9â†’8)"
    )
    parser.add_argument(
        '--dataset_root',
        type=str,
        required=True,
        help='UAVDTæ•°æ®é›†æ ¹ç›®å½• (ä¾‹å¦‚: /data2/.../UAVDT_YOLO)'
    )
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='ä¸å¤‡ä»½åŸå§‹æ–‡ä»¶ (è°¨æ…ä½¿ç”¨!)'
    )
    parser.add_argument(
        '--splits',
        nargs='+',
        default=['train', 'val', 'test'],
        help='è¦ä¿®å¤çš„åˆ†å‰² (é»˜è®¤: train val test)'
    )
    parser.add_argument(
        '--no-delete-cache',
        action='store_true',
        help='ä¸åˆ é™¤ç¼“å­˜æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    dataset_root = Path(args.dataset_root)
    
    if not dataset_root.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨ {dataset_root}")
        return 1
    
    print("="*80)
    print("UAVDTæ•°æ®é›†å®Œæ•´ä¿®å¤å·¥å…·")
    print("="*80)
    print(f"æ•°æ®é›†æ ¹ç›®å½•: {dataset_root}")
    print(f"å¤‡ä»½åŸå§‹æ–‡ä»¶: {'å¦' if args.no_backup else 'æ˜¯'}")
    print(f"è¦ä¿®å¤çš„åˆ†å‰²: {', '.join(args.splits)}")
    print("="*80)
    print()
    
    # ç¡®è®¤
    confirm = input("âš ï¸  æ­¤æ“ä½œå°†ä¿®æ”¹æ ‡ç­¾æ–‡ä»¶,æ˜¯å¦ç»§ç»­? (yes/no): ")
    if confirm.lower() not in ['yes', 'y']:
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return 0
    
    print()
    
    # ä¿®å¤æ¯ä¸ªåˆ†å‰²
    all_stats = {}
    
    for split in args.splits:
        print(f"{'='*80}")
        print(f"ä¿®å¤ {split.upper()} é›†")
        print(f"{'='*80}")
        
        # RGBæ ‡ç­¾è·¯å¾„ (æ ¹æ®UAVDTç›®å½•ç»“æ„)
        label_dir = dataset_root / split / 'labels' / 'rgb'
        
        if not label_dir.exists():
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            alt_paths = [
                dataset_root / split / 'labels',
                dataset_root / split / 'rgb' / 'labels',
            ]
            
            for alt_path in alt_paths:
                if alt_path.exists():
                    label_dir = alt_path
                    break
        
        # ä¿®å¤
        stats = fix_labels_in_directory(label_dir, backup=not args.no_backup)
        
        if stats:
            all_stats[split] = stats
            print_stats(split, stats)
            
            # éªŒè¯ä¿®å¤ç»“æœ
            print(f"\n  ğŸ” éªŒè¯ä¿®å¤ç»“æœ...")
            success, result = verify_labels(label_dir)
            
            if success:
                print(f"  âœ… éªŒè¯é€šè¿‡! ç±»åˆ«åˆ†å¸ƒ:")
                for class_id, count in sorted(result.items()):
                    class_name = {3: 'car', 5: 'truck', 8: 'bus'}.get(class_id, 'unknown')
                    print(f"    ç±»åˆ« {class_id} ({class_name}): {count} ä¸ª")
            else:
                print(f"  âŒ éªŒè¯å¤±è´¥!")
                print(f"    é¢„æœŸç±»åˆ«: {result['expected']}")
                print(f"    å®é™…ç±»åˆ«: {result['actual']}")
                print(f"    åˆ†å¸ƒ: {result['counts']}")
        
        print()
    
    # åˆ é™¤ç¼“å­˜
    if not args.no_delete_cache:
        print(f"{'='*80}")
        print("åˆ é™¤ç¼“å­˜æ–‡ä»¶")
        print(f"{'='*80}")
        
        deleted = delete_cache_files(dataset_root)
        print(f"  âœ… å…±åˆ é™¤ {deleted} ä¸ªç¼“å­˜æ–‡ä»¶")
        print()
    
    # æ€»ç»“
    print("="*80)
    print("âœ… ä¿®å¤å®Œæˆ!")
    print("="*80)
    
    total_modified = sum(s['modified_objects'] for s in all_stats.values())
    print(f"\næ€»è®¡ä¿®æ”¹äº† {total_modified} ä¸ªå¯¹è±¡æ ‡ç­¾")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. éªŒè¯æ‰€æœ‰åˆ†å‰²çš„ç±»åˆ«åˆ†å¸ƒ:")
    for split in args.splits:
        print(f"     cd {dataset_root}/{split}/labels/rgb")
        print(f"     find . -name '*.txt' -exec cat {{}} \\; | awk '{{print $1}}' | sort | uniq -c")
    
    print("\n  2. åˆ é™¤VisDroneæ•°æ®é›†çš„ç¼“å­˜ (å¦‚æœä½¿ç”¨è”åˆè®­ç»ƒ):")
    print("     find /path/to/VisDrone -name '*.cache' -delete")
    
    print("\n  3. é‡æ–°è®­ç»ƒ (ç¦ç”¨ç¼“å­˜):")
    print("     CUDA_VISIBLE_DEVICES=7 python train_depth.py \\")
    print("         --model ultralytics/cfg/models/12/yolo12n-rgbd-v1.yaml \\")
    print("         --weights yolo12n.pt \\")
    print("         --data data/visdrone_uavdt_joint.yaml \\")
    print("         --cache False \\")
    print("         --epochs 300 \\")
    print("         --batch 16 \\")
    print("         --name exp_joint_v17_all_splits_fixed")
    
    print("\n  4. ç›‘æ§è®­ç»ƒæ—¥å¿—:")
    print("     tail -f runs/train/exp_joint_v17_all_splits_fixed/train.log | grep -i instance")
    
    print("\n  5. é¢„æœŸç»“æœ:")
    print("     - è®­ç»ƒå®ä¾‹æ•°: ~800k")
    print("     - Epoch 50 mAP: >30%")
    print("     - Epoch 150+ mAP: 40-45%")
    
    print("\n" + "="*80)
    
    return 0


if __name__ == "__main__":
    exit(main())
